'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/0.4/concepts/','title':"Concepts",'section':"Apache Paimon",'content':""});index.add({'id':1,'href':'/docs/0.4/how-to/creating-catalogs/','title':"Creating Catalogs",'section':"How to",'content':"Creating Catalogs #  Paimon catalogs currently support two types of metastores:\n filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive.  See CatalogOptions for detailed options when creating a catalog.\nCreating a Catalog with Filesystem Metastore #  Flink The following Flink SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs://path/to/warehouse.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs://path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; You can define any default table options with the prefix table-default. for tables created in the catalog.\nSpark3 The following shell command registers a paimon catalog named paimon. Metadata and table files are stored under hdfs://path/to/warehouse.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs://path/to/warehouse You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default;  Creating a Catalog with Hive Metastore #  By using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.\nTo use Hive catalog, Database name, Table name and Field names should be lower case.\nFlink Paimon Hive catalog in Flink relies on Flink Hive connector bundled jar. You should first download Flink Hive connector bundled jar and add it to classpath. See here for more info.\nThe following Flink SQL registers and uses a Paimon Hive catalog named my_hive. Metadata and table files are stored under hdfs://path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nIf your Hive requires security authentication such as Kerberos, LDAP, Ranger and so on. You can specify the hive-conf-dir parameter to the hive-site.xml file path.\nCREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs://path/to/warehouse\u0026#39; ); USE CATALOG my_hive; You can define any default table options with the prefix table-default. for tables created in the catalog.\nSpark3 Your Spark installation should be able to detect, or already contains Hive dependencies. See here for more information.\nThe following shell command registers a Paimon Hive catalog named paimon. Metadata and table files are stored under hdfs://path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs://path/to/warehouse \\  --conf spark.sql.catalog.paimon.metastore=hive \\  --conf spark.sql.catalog.paimon.uri=thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt; You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default;  "});index.add({'id':2,'href':'/docs/0.4/api/java-api/','title':"Java API",'section':"API",'content':"Java API #  Dependency #  Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-bundle\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.4.0-incubating\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Bundle. Paimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nCreate Catalog #  Before coming into contact with the Table, you need to create a Catalog.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.fs.Path; import org.apache.paimon.options.Options; public class CreateCatalog { public static void createFilesystemCatalog() { CatalogContext context = CatalogContext.create(new Path(\u0026#34;...\u0026#34;)); Catalog catalog = CatalogFactory.createCatalog(context); } public static void createHiveCatalog() { // Paimon Hive catalog relies on Hive jars  // You should add hive classpath or hive bundled jar.  Options options = new Options(); options.set(\u0026#34;warehouse\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;metastore\u0026#34;, \u0026#34;hive\u0026#34;); options.set(\u0026#34;uri\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;hive-conf-dir\u0026#34;, \u0026#34;...\u0026#34;); CatalogContext context = CatalogContext.create(options); Catalog catalog = CatalogFactory.createCatalog(context); } } Create Database #  You can use the catalog to create databases. The created databases are persistence in the file system.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.fs.Path; public class CreateDatabase { public static void main(String[] args) { try { catalog.createDatabase(\u0026#34;my_db\u0026#34;, false); } catch (Catalog.DatabaseAlreadyExistException e) { // do something  } } } Determine Whether Database Exists #  You can use the catalog to determine whether the database exists\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.fs.Path; public class DatabaseExists { public static void main(String[] args) { boolean exists = catalog.databaseExists(\u0026#34;my_db\u0026#34;); } } List Databases #  You can use the catalog to list databases.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.fs.Path; public class ListDatabases { public static void main(String[] args) { List\u0026lt;String\u0026gt; databases = catalog.listDatabases(); } } Drop Database #  You can use the catalog to drop databases.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.fs.Path; public class DropDatabase { public static void main(String[] args) { try { catalog.dropDatabase(\u0026#34;my_db\u0026#34;, false, true); } catch (Catalog.DatabaseNotEmptyException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Create Table #  You can use the catalog to create tables. The created tables are persistence in the file system. Next time you can directly obtain these tables.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.types.DataTypes; public class CreateTable { public static void main(String[] args) { Schema.Builder schemaBuilder = Schema.newBuilder(); schemaBuilder.primaryKey(\u0026#34;...\u0026#34;); schemaBuilder.partitionKeys(\u0026#34;...\u0026#34;); schemaBuilder.column(\u0026#34;f0\u0026#34;, DataTypes.INT()); schemaBuilder.column(\u0026#34;f1\u0026#34;, DataTypes.STRING()); Schema schema = schemaBuilder.build(); Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { catalog.createTable(identifier, schema, false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Get Table #  The Table interface provides access to the table metadata and tools to read and write table.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class GetTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Table table = catalog.getTable(identifier); } catch (Catalog.TableNotExistException e) { // do something  } } } Determine Whether Table Exists #  You can use the catalog to determine whether the table exists\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class TableExists { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); boolean exists = catalog.tableExists(identifier); } } List Tables #  You can use the catalog to list tables.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; public class ListTables { public static void main(String[] args) { try { catalog.listTables(\u0026#34;my_db\u0026#34;); } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Drop Table #  You can use the catalog to drop table.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class DropTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { catalog.dropTable(identifier, false); } catch (Catalog.TableNotExistException e) { // do something  } } } Rename Table #  You can use the catalog to rename a table.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class RenameTable { public static void main(String[] args) { Identifier fromTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Identifier toTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;test_table\u0026#34;); try { catalog.renameTable(fromTableIdentifier, toTableIdentifier, false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.TableNotExistException e) { // do something  } } } Alter Table #  You can use the catalog to alter a table, but you need to pay attention to the following points.\n Add column cannot specify NOT NULL. Cannot update partition column type in the table. Cannot change nullability of primary key. If the type of the column is nested row type, update the column type is not supported. Update column to nested row type is not supported.  import org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.SchemaChange; import org.apache.paimon.table.Table; import org.apache.paimon.types.DataField; import org.apache.paimon.types.DataTypes; import com.google.common.collect.Lists; import java.util.Arrays; public class AlterTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Map\u0026lt;String,String\u0026gt; options = new HashMap\u0026lt;\u0026gt;(); options.put(\u0026#34;bucket\u0026#34;, \u0026#34;4\u0026#34;); options.put(\u0026#34;compaction.max.file-num\u0026#34;, \u0026#34;40\u0026#34;); catalog.createTable( identifier, new Schema( Lists.newArrayList( new DataField(0, \u0026#34;col1\u0026#34;, DataTypes.STRING(), \u0026#34;field1\u0026#34;), new DataField(1, \u0026#34;col2\u0026#34;, DataTypes.STRING(), \u0026#34;field2\u0026#34;), new DataField(2, \u0026#34;col3\u0026#34;, DataTypes.STRING(), \u0026#34;field3\u0026#34;), new DataField(3, \u0026#34;col4\u0026#34;, DataTypes.BIGINT(), \u0026#34;field4\u0026#34;), new DataField( 4, \u0026#34;col5\u0026#34;, DataTypes.ROW( new DataField(5, \u0026#34;f1\u0026#34;, DataTypes.STRING(), \u0026#34;f1\u0026#34;), new DataField(6, \u0026#34;f2\u0026#34;, DataTypes.STRING(), \u0026#34;f2\u0026#34;), new DataField(7, \u0026#34;f3\u0026#34;, DataTypes.STRING(), \u0026#34;f3\u0026#34;)), \u0026#34;field5\u0026#34;), new DataField(8, \u0026#34;col6\u0026#34;, DataTypes.STRING(), \u0026#34;field6\u0026#34;)), Lists.newArrayList(\u0026#34;col1\u0026#34;), // partition keys  Lists.newArrayList(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;), //primary key  options, \u0026#34;table comment\u0026#34;), false); // add option  SchemaChange addOption = SchemaChange.setOption(\u0026#34;snapshot.time-retained\u0026#34;, \u0026#34;2h\u0026#34;); // remove option  SchemaChange removeOption = SchemaChange.removeOption(\u0026#34;compaction.max.file-num\u0026#34;); // add column  SchemaChange addColumn = SchemaChange.addColumn(\u0026#34;col1_after\u0026#34;, DataTypes.STRING()); // add a column after col1  SchemaChange.Move after = SchemaChange.Move.after(\u0026#34;col1_after\u0026#34;, \u0026#34;col1\u0026#34;); SchemaChange addColumnAfterField = SchemaChange.addColumn(\u0026#34;col7\u0026#34;, DataTypes.STRING(), \u0026#34;\u0026#34;, after); // rename column  SchemaChange renameColumn = SchemaChange.renameColumn(\u0026#34;col3\u0026#34;, \u0026#34;col3_new_name\u0026#34;); // drop column  SchemaChange dropColumn = SchemaChange.dropColumn(\u0026#34;col6\u0026#34;); // update column comment  SchemaChange updateColumnComment = SchemaChange.updateColumnComment(new String[]{\u0026#34;col4\u0026#34;}, \u0026#34;col4 field\u0026#34;); // update nested column comment  SchemaChange updateNestedColumnComment = SchemaChange.updateColumnComment(new String[]{\u0026#34;col5\u0026#34;, \u0026#34;f1\u0026#34;}, \u0026#34;col5 f1 field\u0026#34;); // update column type  SchemaChange updateColumnType = SchemaChange.updateColumnType(\u0026#34;col4\u0026#34;, DataTypes.DOUBLE()); // update column position, you need to pass in a parameter of type Move  SchemaChange updateColumnPosition = SchemaChange.updateColumnPosition(SchemaChange.Move.first(\u0026#34;col4\u0026#34;)); // update column nullability  SchemaChange updateColumnNullability = SchemaChange.updateColumnNullability(new String[]{\u0026#34;col4\u0026#34;}, false); // update nested column nullability  SchemaChange updateNestedColumnNullability = SchemaChange.updateColumnNullability(new String[]{\u0026#34;col5\u0026#34;, \u0026#34;f2\u0026#34;}, false); SchemaChange[] schemaChanges = new SchemaChange[] {addOption, removeOption, addColumn, addColumnAfterField, renameColumn, dropColumn, updateColumnComment, updateNestedColumnComment, updateColumnType, updateColumnPosition, updateColumnNullability, updateNestedColumnNullability}; try { catalog.alterTable(identifier, Arrays.asList(schemaChanges), false); } catch (Catalog.TableNotExistException e) { // do something  } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Table metadata:\n name return a name string to identify this table. rowType return the current row type of this table containing a sequence of table\u0026rsquo;s fields. partitionKeys returns the partition keys of this table. parimaryKeys returns the primary keys of this table. options returns the configuration of this table in a map of key-value. comment returns the optional comment of this table. copy return a new table by applying dynamic options to this table.  Batch Read #  For relatively small amounts of data, or for data that has undergone projection and filtering, you can directly use a standalone program to read the table data.\nBut if the data volume of the table is relatively large, you can distribute splits to different tasks for reading.\nThe reading is divided into two stages:\n Scan Plan: Generate plan splits in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;). Read Split: Read split in distributed tasks.  import org.apache.paimon.data.InternalRow; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.TableRead; import java.io.IOException; import java.util.List; public class ReadTable { public static void main(String[] args) { // 1. Create a ReadBuilder and push filter (`withFilter`)  // and projection (`withProjection`) if necessary  ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(filter); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;)  List\u0026lt;Split\u0026gt; splits = readBuilder.newScan().plan().splits(); // 3. Distribute these splits to different tasks  // 4. Read a split in task  TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(ReadTable::readRow); } } Batch Write #  The writing is divided into two stages:\n Write records: Write records in distributed tasks, generate commit messages. Commit/Abort: Collect all CommitMessages, commit them in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;, or named \u0026lsquo;Committer\u0026rsquo;). When the commit fails for certain reason, abort unsuccessful commit via CommitMessages.  import java.util.List; import org.apache.paimon.table.sink.BatchTableCommit; import org.apache.paimon.table.sink.BatchTableWrite; import org.apache.paimon.table.sink.BatchWriteBuilder; import org.apache.paimon.table.sink.CommitMessage; public class WriteTable { public static void main(String[] args) { // 1. Create a WriteBuilder (Serializable)  BatchWriteBuilder writeBuilder = table.newBatchWriteBuilder() .withOverwrite(staticPartition); // 2. Write records in distributed tasks  BatchTableWrite write = writeBuilder.newWrite(); write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit(); // 3. Collect all CommitMessages to a global node and commit  BatchTableCommit commit = writeBuilder.newCommit(); commit.commit(messages); // Abort unsuccessful commit to delete data files  // commit.abort(messages);  } } Stream Read #  The difference of Stream Read is that StreamTableScan can continuously scan and generate splits.\nStreamTableScan provides the ability to checkpoint and restore, which can let you save the correct state during stream reading.\nimport java.io.IOException; import java.util.List; import org.apache.paimon.data.InternalRow; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.StreamTableScan; import org.apache.paimon.table.source.TableRead; public class StreamReadTable { public static void main(String[] args) throws IOException { // 1. Create a ReadBuilder and push filter (`withFilter`)  // and projection (`withProjection`) if necessary  ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(filter); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;)  StreamTableScan scan = readBuilder.newStreamScan(); while (true) { List\u0026lt;Split\u0026gt; splits = scan.plan().splits(); // Distribute these splits to different tasks  Long state = scan.checkpoint(); // can be restored in scan.restore(state) after failover  } // 3. Read a split in task  TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(row -\u0026gt; System.out.println(row)); } } Stream Write #  The difference of Stream Write is that StreamTableCommit can continuously commit.\nKey points to achieve exactly-once consistency:\n CommitUser represents a user. A user can commit multiple times. In distributed processing, you are expected to use the same commitUser. Different applications need to use different commitUsers. The commitIdentifier of StreamTableWrite and StreamTableCommit needs to be consistent, and the id needs to be incremented for the next committing. When a failure occurs, if you still have uncommitted CommitMessages, please use StreamTableCommit#filterCommitted to exclude the committed messages by commitIdentifier.  import java.util.List; import org.apache.paimon.table.sink.CommitMessage; import org.apache.paimon.table.sink.StreamTableCommit; import org.apache.paimon.table.sink.StreamTableWrite; import org.apache.paimon.table.sink.StreamWriteBuilder; public class StreamWriteTable { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable)  StreamWriteBuilder writeBuilder = table.newStreamWriteBuilder(); // 2. Write records in distributed tasks  StreamTableWrite write = writeBuilder.newWrite(); // commitIdentifier like Flink checkpointId  long commitIdentifier = 0; while (true) { write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit( false, commitIdentifier); commitIdentifier++; } // 3. Collect all CommitMessages to a global node and commit  StreamTableCommit commit = writeBuilder.newCommit(); commit.commit(commitIdentifier, messages); // 4. When failover, you can use \u0026#39;filterCommitted\u0026#39; to filter committed commits.  commit.filterCommitted(committedIdentifiers); } } Data Types #     Java Paimon     boolean boolean   byte byte   short short   int int   long long   float float   double double   string org.apache.paimon.data.BinaryString   decimal org.apache.paimon.data.Decimal   timestamp org.apache.paimon.data.Timestamp   byte[] byte[]   array org.apache.paimon.data.InternalArray   map org.apache.paimon.data.InternalMap   InternalRow org.apache.paimon.data.InternalRow    "});index.add({'id':3,'href':'/docs/0.4/concepts/overview/','title':"Overview",'section':"Concepts",'content':"Overview #  Apache Paimon(incubating) is a streaming data lake platform that supports high-speed data ingestion, change data tracking and efficient real-time analytics.\nArchitecture #  As shown in the architecture above:\nRead/Write: Paimon supports a versatile way to read/write data and perform OLAP queries.\n For reads, it supports consuming data  from historical snapshots (in batch mode), from the latest offset (in streaming mode), or reading incremental snapshots in a hybrid way.   For writes, it supports streaming synchronization from the changelog of databases (CDC) or batch insert/overwrite from offline data.  Ecosystem: In addition to Apache Flink, Paimon also supports read by other computation engines like Apache Hive, Apache Spark and Trino.\nInternal: Under the hood, Paimon stores the columnar files on the filesystem/object-store and uses the LSM tree structure to support a large volume of data updates and high-performance queries.\nUnified Storage #  For streaming engines like Apache Flink, there are typically three types of connectors:\n Message queue, such as Apache Kafka, it is used in both source and intermediate stages in this pipeline, to guarantee the latency stay within seconds. OLAP system, such as Clickhouse, it receives processed data in streaming fashion and serving user’s ad-hoc queries. Batch storage, such as Apache Hive, it supports various operations of the traditional batch processing, including INSERT OVERWRITE.  Paimon provides table abstraction. It is used in a way that does not differ from the traditional database:\n In batch execution mode, it acts like a Hive table and supports various operations of Batch SQL. Query it to see the latest snapshot. In streaming execution mode, it acts like a message queue. Query it acts like querying a stream changelog from a message queue where historical data never expires.  "});index.add({'id':4,'href':'/docs/0.4/engines/overview/','title':"Overview",'section':"Engines",'content':"Overview #  Paimon not only supports Flink SQL writes and queries natively, but also provides queries from other popular engines, such as Apache Spark and Apache Hive.\nCompatibility Matrix #     Engine Version Batch Read Batch Write Create Table Streaming Write Streaming Read Batch Overwrite     Flink 1.14 - 1.17 ✅ ✅ ✅ ✅ ✅ ✅   Hive 2.1 - 3.1 ✅ ✅ ❌ ❌ ❌ ❌   Spark 3.1 - 3.4 ✅ ✅ ✅ ❌ ❌ ❌   Spark 2.4 ✅ ❌ ❌ ❌ ❌ ❌   Trino 358 - 400 ✅ ❌ ❌ ❌ ❌ ❌   Presto 0.236 - 0.279 ✅ ❌ ❌ ❌ ❌ ❌    Ongoing engines:\n Doris: Under development, Support Paimon catalog, Doris Roadmap 2023. Seatunnel: Under development, Introduce paimon connector. Starrocks: Under discussion  Download Link\n"});index.add({'id':5,'href':'/docs/0.4/filesystems/overview/','title':"Overview",'section':"Filesystems",'content':"Overview #  Apache Paimon utilizes the same pluggable file systems as Apache Flink. Users can follow the standard plugin mechanism to configure the plugin structure if using Flink as compute engine. However, for other engines like Spark or Hive, the provided opt jars (by Flink) may get conflicts and cannot be used directly. It is not convenient for users to fix class conflicts, thus Paimon provides the self-contained and engine-unified FileSystem pluggable jars for user to query tables from Spark/Hive side.\nSupported FileSystems #     FileSystem URI Scheme Pluggable Description     Local File System file:// N Built-in Support   HDFS hdfs:// N Built-in Support, ensure that the cluster is in the hadoop environment   Aliyun OSS oss:// Y    S3 s3:// Y     Dependency #  We recommend you to download the jar directly: Download Link.\nYou can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild shaded jar with the following command.\nmvn clean install -DskipTests You can find the shaded jars under ./paimon-filesystems/paimon-${fs}/target/paimon-${fs}-0.4.0-incubating.jar.\n"});index.add({'id':6,'href':'/docs/0.4/project/roadmap/','title':"Roadmap",'section':"Project",'content':"Roadmap #  Paimon\u0026rsquo;s long-term goal is to become the better data lake platform for building the Streaming Lakehouse. Paimon will invest in real-time, ecology, and data warehouse integrity for a long time.\nIf you have other requirements, please contact us.\nWhat’s Next? #  Core #   Lookup Changelog-Producer to produce changelog in real-time Enhance Flink Lookup Join from True Lookup Provide stable Java Programing API Savepoint support More Metrics, such as the busyness of compaction thread Multi table consistency for real-time materialized views  Ingestion #   Schema Evolution Synchronization from Flink CDC Entire Database Synchronization from Flink CDC Integration with Apache Seatunnel  Compute Engines #   Flink DELETE/UPDATE support More management via Flink/Spark CALL procedures. Hive Writer Spark Writer supports INSERT OVERWRITE Spark Time Traveling Presto Reader Doris Reader  "});index.add({'id':7,'href':'/docs/0.4/maintenance/write-performance/','title':"Write Performance",'section':"Maintenance",'content':"Write Performance #  Performance of Paimon writers are related with the following factors.\nParallelism #  It is recommended that the parallelism of sink should be less than or equal to the number of buckets, preferably equal. You can control the parallelism of the sink with the sink.parallelism table property.\n  Option Required Default Type Description     sink.parallelism No (none) Integer Defines the parallelism of the sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.    Write Initialize #  In the initialization of write, the writer of the bucket needs to read all historical files. If there is a bottleneck here (For example, writing a large number of partitions simultaneously), you can use write-manifest-cache to cache the read manifest data to accelerate initialization.\nCompaction #  Number of Sorted Runs to Trigger Compaction #  Paimon uses LSM tree which supports a large number of updates. LSM organizes files in several sorted runs. When querying records from an LSM tree, all sorted runs must be combined to produce a complete view of all records.\nOne can easily see that too many sorted runs will result in poor query performance. To keep the number of sorted runs in a reasonable range, Paimon writers will automatically perform compactions. The following table property determines the minimum number of sorted runs to trigger a compaction.\n  Option Required Default Type Description     num-sorted-run.compaction-trigger No 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).    Compaction will become less frequent when num-sorted-run.compaction-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance.\nNumber of Sorted Runs to Pause Writing #  When number of sorted runs is small, Paimon writers will perform compaction asynchronously in separated threads, so records can be continuously written into the table. However to avoid unbounded growth of sorted runs, writers will have to pause writing when the number of sorted runs hits the threshold. The following table property determines the threshold.\n  Option Required Default Type Description     num-sorted-run.stop-trigger No (none) Integer The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 1.    Write stalls will become less frequent when num-sorted-run.stop-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance.\nDedicated Compaction Job #  By default, Paimon writers will perform compaction as needed when writing records. This is sufficient for most use cases, but there are two downsides:\n This may result in unstable write throughput because throughput might temporarily drop when performing a compaction. Compaction will mark some data files as \u0026ldquo;deleted\u0026rdquo; (not really deleted, see expiring snapshots for more info). If multiple writers mark the same file a conflict will occur when committing the changes. Paimon will automatically resolve the conflict, but this may result in job restarts.  To avoid these downsides, users can also choose to skip compactions in writers, and run a dedicated job only for compaction. As compactions are performed only by the dedicated job, writers can continuously write records without pausing and no conflicts will ever occur.\nTo skip compactions in writers, set the following table property to true.\n  Option Required Default Type Description     write-only No false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.    To run a dedicated job for compaction, follow these instructions.\nFlink Flink SQL currently does not support statements related to compactions, so we have to submit the compaction job through flink run.\nRun the following command to submit a compaction job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  --catalog-conf is the configuration for Paimon catalog. Each configuration should be specified in the format key=value. See here for a complete list of catalog configurations.  If you submit a batch job (set execution.runtime-mode: batch in Flink\u0026rsquo;s configuration), all current table files will be compacted. If you submit a streaming job (set execution.runtime-mode: streaming in Flink\u0026rsquo;s configuration), the job will continuously monitor new changes to the table and perform compactions as needed.\nIf you only want to submit the compaction job and don\u0026rsquo;t want to wait until the job is done, you should submit in detached mode.  Example\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  compact \\  --warehouse s3:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition dt=20221126,hh=08 \\  --partition dt=20221127,hh=09 \\  --catalog-conf s3.endpoint=https://****.com \\  --catalog-conf s3.access-key=***** \\  --catalog-conf s3.secret-key=***** For more usage of the compact action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  compact --help  Memory #  There are three main places in Paimon writer that takes up memory:\n Writer\u0026rsquo;s memory buffer, shared and preempted by all writers of a single task. This memory value can be adjusted by the write-buffer-size table property. Memory consumed when merging several sorted runs for compaction. Can be adjusted by the num-sorted-run.compaction-trigger option to change the number of sorted runs to be merged. If the row is very large, reading too many lines of data at once can consume a lot of memory when making a compaction. Reducing the read.batch-size option can alleviate the impact of this case. The memory consumed by writing columnar (ORC, Parquet, etc.) file, which is not adjustable.  "});index.add({'id':8,'href':'/docs/0.4/concepts/basic-concepts/','title':"Basic Concepts",'section':"Concepts",'content':"Basic Concepts #  Snapshot #  A snapshot captures the state of a table at some point in time. Users can access the latest data of a table through the latest snapshot. By time traveling, users can also access the previous state of a table through an earlier snapshot.\nPartition #  Paimon adopts the same partitioning concept as Apache Hive to separate data.\nPartitioning is an optional way of dividing a table into related parts based on the values of particular columns like date, city, and department. Each table can have one or more partition keys to identify a particular partition.\nBy partitioning, users can efficiently operate on a slice of records in the table. See file layouts for how files are divided into multiple partitions.\nPartition keys must be a subset of primary keys if primary keys are defined.  Bucket #  Unpartitioned tables, or partitions in partitioned tables, are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying.\nThe range for a bucket is determined by the hash value of one or more columns in the records. Users can specify bucketing columns by providing the bucket-key option. If no bucket-key option is specified, the primary key (if defined) or the complete record will be used as the bucket key.\nA bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 1GB.\nSee file layouts for how files are divided into buckets. Also, see rescale bucket if you want to adjust the number of buckets after a table is created.\nConsistency Guarantees #  Paimon writers uses two-phase commit protocol to atomically commit a batch of records to the table. Each commit produces at most two snapshots at commit time.\nFor any two writers modifying a table at the same time, as long as they do not modify the same bucket, their commits are serializable. If they modify the same bucket, only snapshot isolation is guaranteed. That is, the final table state may be a mix of the two commits, but no changes are lost.\n"});index.add({'id':9,'href':'/docs/0.4/how-to/creating-tables/','title':"Creating Tables",'section':"How to",'content':"Creating Tables #  Creating Catalog Managed Tables #  Tables created in Paimon catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.\nThe following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named MyTable with five columns in the catalog\u0026rsquo;s default database, where dt, hh and user_id are the primary keys.\nFlink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; );  Inserting jobs on the table should be stopped prior to dropping tables, or table files couldn\u0026rsquo;t be deleted completely.  Partitioned Tables #  The following SQL creates a table named MyTable with five columns partitioned by dt and hh, where dt, hh and user_id are the primary keys.\nFlink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; );  Partition keys must be a subset of primary keys if primary keys are defined.  By configuring partition.expiration-time, expired partitions can be automatically deleted.  Create Table As #  Table can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;\nWe can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.\nFlink /* For streaming mode, you need to enable the checkpoint. */ CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT ); CREATE TABLE MyTableAs AS SELECT * FROM MyTable; /* partitioned table */ CREATE TABLE MyTablePartition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE MyTablePartitionAs WITH (\u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM MyTablePartition; /* change options */ CREATE TABLE MyTableOptions ( user_id BIGINT, item_id BIGINT ) WITH (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE MyTableOptionsAs WITH (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM MyTableOptions; /* primary key */ CREATE TABLE MyTablePk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) ; CREATE TABLE MyTablePkAs WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM MyTablePk; /* primary key + partition */ CREATE TABLE MyTableAll ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); CREATE TABLE MyTableAllAs WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;, \u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM MyTableAll; Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT ); CREATE TABLE MyTableAs AS SELECT * FROM MyTable; /* partitioned table*/ CREATE TABLE MyTablePartition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE MyTablePartitionAs PARTITIONED BY (dt) AS SELECT * FROM MyTablePartition; /* change TBLPROPERTIES */ CREATE TABLE MyTableOptions ( user_id BIGINT, item_id BIGINT ) TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE MyTableOptionsAs TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM MyTableOptions; /* primary key */ CREATE TABLE MyTablePk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE MyTablePkAs TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM MyTablePk; /* primary key + partition */ CREATE TABLE MyTableAll ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE MyTableAllAs PARTITIONED BY (dt) TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM MyTableAll;  Create Table Like #  Flink To create a table with the same schema, partition, and table properties as another table, use CREATE TABLE LIKE.\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) ; CREATE TABLE MyTableLike LIKE MyTable;  Table Properties #  Users can specify table properties to enable features or improve performance of Paimon. For a complete list of such properties, see configurations.\nThe following SQL creates a table named MyTable with five columns partitioned by dt and hh, where dt, hh and user_id are the primary keys. This table has two properties: 'bucket' = '2' and 'bucket-key' = 'user_id'.\nFlink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;user_id\u0026#39; ); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;user_id\u0026#39; );  Creating External Tables #  External tables are recorded but not managed by catalogs. If an external table is dropped, its table files will not be deleted.\nPaimon external tables can be used in any catalog. If you do not want to create a Paimon catalog and just want to read / write a table, you can consider external tables.\nFlink Flink SQL supports reading and writing an external table. External Paimon tables are created by specifying the connector and path table properties. The following SQL creates an external table named MyTable with five columns, where the base path of table files is hdfs://path/to/table.\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;hdfs://path/to/table\u0026#39;, \u0026#39;auto-create\u0026#39; = \u0026#39;true\u0026#39; -- this table property creates table files for an empty table if table path does not exist  -- currently only supported by Flink ); Spark3 Spark3 only supports creating external tables through Scala API. The following Scala code loads the table located at hdfs://path/to/table into a DataSet.\nval dataset = spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;hdfs://path/to/table\u0026#34;) Spark2 Spark2 only supports creating external tables through Scala API. The following Scala code loads the table located at hdfs://path/to/table into a DataSet.\nval dataset = spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;hdfs://path/to/table\u0026#34;) Hive Hive SQL only supports reading from an external table. The following SQL creates an external table named my_table, where the base path of table files is hdfs://path/to/table. As schemas are stored in table files, users do not need to write column definitions.\nCREATE EXTERNAL TABLE my_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; LOCATION \u0026#39;hdfs://path/to/table\u0026#39;;  Creating Temporary Tables #  Flink Temporary tables are only supported by Flink. Like external tables, temporary tables are just recorded but not managed by the current Flink SQL session. If the temporary table is dropped, its resources will not be deleted. Temporary tables are also dropped when Flink SQL session is closed.\nIf you want to use Paimon catalog along with other tables but do not want to store them in other catalogs, you can create a temporary table. The following Flink SQL creates a Paimon catalog and a temporary table and also illustrates how to use both tables together.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs://path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; -- Assume that there is already a table named my_table in my_catalog  CREATE TEMPORARY TABLE temp_table ( k INT, v STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;hdfs://path/to/temp_table.csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); SELECT my_table.k, my_table.v, temp_table.v FROM my_table JOIN temp_table ON my_table.k = temp_table.k;  "});index.add({'id':10,'href':'/docs/0.4/project/download/','title':"Download",'section':"Project",'content':"Download #  This documentation is a guide for downloading Paimon Jars.\nEngine Jars #     Version Jar     Flink 1.17 paimon-flink-1.17-0.4.0-incubating.jar   Flink 1.16 paimon-flink-1.16-0.4.0-incubating.jar   Flink 1.15 paimon-flink-1.15-0.4.0-incubating.jar   Flink 1.14 paimon-flink-1.14-0.4.0-incubating.jar   Flink Action paimon-flink-action-0.4.0-incubating.jar   Spark 3.3 paimon-spark-3.3-0.4.0-incubating.jar   Spark 3.2 paimon-spark-3.2-0.4.0-incubating.jar   Spark 3.1 paimon-spark-3.1-0.4.0-incubating.jar   Spark 2 paimon-spark-2-0.4.0-incubating.jar   Hive 3.1 paimon-hive-connector-3.1-0.4.0-incubating.jar   Hive 2.3 paimon-hive-connector-2.3-0.4.0-incubating.jar   Hive 2.2 paimon-hive-connector-2.2-0.4.0-incubating.jar   Hive 2.1 paimon-hive-connector-2.1-0.4.0-incubating.jar   Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-0.4.0-incubating.jar   Presto 0.236 paimon-presto-0.236-0.4.0-incubating.jar   Presto 0.268 paimon-presto-0.268-0.4.0-incubating.jar   Presto 0.273 paimon-presto-0.273-0.4.0-incubating.jar    Filesystem Jars #     Version Jar     paimon-oss paimon-oss-0.4.0-incubating.jar   paimon-s3 paimon-s3-0.4.0-incubating.jar    API Jars #     Version Jar     paimon-bundle paimon-bundle-0.4.0-incubating.jar    "});index.add({'id':11,'href':'/docs/0.4/engines/','title':"Engines",'section':"Apache Paimon",'content':""});index.add({'id':12,'href':'/docs/0.4/engines/flink/','title':"Flink",'section':"Engines",'content':"Flink #  This documentation is a guide for using Paimon in Flink.\nPreparing Paimon Jar File #  Paimon currently supports Flink 1.17, 1.16, 1.15 and 1.14. We recommend the latest Flink version for a better experience.\nDownload the jar file with corresponding version.\n   Version Jar     Flink 1.17 paimon-flink-1.17-0.4.0-incubating.jar   Flink 1.16 paimon-flink-1.16-0.4.0-incubating.jar   Flink 1.15 paimon-flink-1.15-0.4.0-incubating.jar   Flink 1.14 paimon-flink-1.14-0.4.0-incubating.jar   Flink Action paimon-flink-action-0.4.0-incubating.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\n mvn clean install -DskipTests  You can find the bundled jar in ./paimon-flink/paimon-flink-\u0026lt;flink-version\u0026gt;/target/paimon-flink-\u0026lt;flink-version\u0026gt;-0.4.0-incubating.jar.\nQuick Start #  Step 1: Download Flink\nIf you haven\u0026rsquo;t downloaded Flink, you can download Flink, then extract the archive with the following command.\ntar -xzf flink-*.tgz Step 2: Copy Paimon Bundled Jar\nCopy paimon bundled jar to the lib directory of your Flink home.\ncp paimon-flink-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 3: Copy Hadoop Bundled Jar\nIf the machine is in a hadoop environment, please ensure the value of the environment variable HADOOP_CLASSPATH include path to the common Hadoop libraries, you do not need to use the following pre-bundled Hadoop jar.  Download Pre-bundled Hadoop jar and copy the jar file to the lib directory of your Flink home.\ncp flink-shaded-hadoop-2-uber-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 4: Start a Flink Local Cluster\nIn order to run multiple Flink jobs at the same time, you need to modify the cluster configuration in \u0026lt;FLINK_HOME\u0026gt;/conf/flink-conf.yaml.\ntaskmanager.numberOfTaskSlots:2To start a local cluster, run the bash script that comes with Flink:\n\u0026lt;FLINK_HOME\u0026gt;/bin/start-cluster.sh You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.\nYou can now start Flink SQL client to execute SQL scripts.\n\u0026lt;FLINK_HOME\u0026gt;/bin/sql-client.sh Step 5: Create a Catalog and a Table\n-- if you\u0026#39;re trying out Paimon in a distributed environment, -- the warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;file:/tmp/paimon\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); Step 6: Write Data\n-- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.word.length\u0026#39; = \u0026#39;1\u0026#39; ); -- paimon requires checkpoint interval in streaming mode SET \u0026#39;execution.checkpointing.interval\u0026#39; = \u0026#39;10 s\u0026#39;; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; Step 7: OLAP Query\n-- use tableau result mode SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; -- switch to batch mode RESET \u0026#39;execution.checkpointing.interval\u0026#39;; SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- olap query the table SELECT * FROM word_count; You can execute the query multiple times and observe the changes in the results.\nStep 8: Streaming Query\n-- switch to streaming mode SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; -- track the changes of table and calculate the count interval statistics SELECT `interval`, COUNT(*) AS interval_cnt FROM (SELECT cnt / 10000 AS `interval` FROM word_count) GROUP BY `interval`; Step 9: Exit\nCancel streaming job in localhost:8081, then execute the following SQL script to exit Flink SQL client.\n-- uncomment the following line if you want to drop the dynamic table and clear the files -- DROP TABLE word_count;  -- exit sql-client EXIT; Stop the Flink local cluster.\n./bin/stop-cluster.sh Supported Flink Data Type #  See Flink Data Types.\nAll Flink data types are supported, except that\n MULTISET is not supported. MAP is not supported as primary keys.  "});index.add({'id':13,'href':'/docs/0.4/api/flink-api/','title':"Flink API",'section':"API",'content':"Flink API #  Dependency #  Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-flink-1.17\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.4.0-incubating\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.17.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Flink. Please choose your Flink version.\nPaimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nPaimon does not provide a DataStream API, but you can read or write to Paimon tables by the conversion between DataStream and Table in Flink. See DataStream API Integration.\nWrite to Table #  import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; public class WriteToTable { public static void writeTo() { // create environments of both APIs  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create a changelog DataStream  DataStream\u0026lt;Row\u0026gt; dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.ofKind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)) .returns( Types.ROW_NAMED( new String[] {\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;}, Types.STRING, Types.INT)); // interpret the DataStream as a Table  Schema schema = Schema.newBuilder() .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .column(\u0026#34;age\u0026#34;, DataTypes.INT()) .build(); Table table = tableEnv.fromChangelogStream(dataStream, schema); // create paimon catalog  tableEnv.executeSql(\u0026#34;CREATE CATALOG paimon WITH (\u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;...\u0026#39;)\u0026#34;); tableEnv.executeSql(\u0026#34;USE CATALOG paimon\u0026#34;); // register the table under a name and perform an aggregation  tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table); // insert into paimon table from your data stream table  tableEnv.executeSql(\u0026#34;INSERT INTO sink_paimon_table SELECT * FROM InputTable\u0026#34;); } } Read from Table #  import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; public class ReadFromTable { public static void readFrom() throws Exception { // create environments of both APIs  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create paimon catalog  tableEnv.executeSql(\u0026#34;CREATE CATALOG paimon WITH (\u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;...\u0026#39;)\u0026#34;); tableEnv.executeSql(\u0026#34;USE CATALOG paimon\u0026#34;); // convert to DataStream  Table table = tableEnv.sqlQuery(\u0026#34;SELECT * FROM my_paimon_table\u0026#34;); DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toChangelogStream(table); // use this datastream  dataStream.executeAndCollect().forEachRemaining(System.out::println); // prints:  // +I[Bob, 12]  // +I[Alice, 12]  // -U[Alice, 12]  // +U[Alice, 14]  } } "});index.add({'id':14,'href':'/docs/0.4/filesystems/hdfs/','title':"HDFS",'section':"Filesystems",'content':"HDFS #  You don\u0026rsquo;t need any additional dependencies to access HDFS because you have already taken care of the Hadoop dependencies.\nHDFS Configuration #  For HDFS, the most important thing is to be able to read your HDFS configuration.\nFlink/Trino/JavaAPI You may not have to do anything, if you are in a hadoop environment. Otherwise pick one of the following ways to configure your HDFS:\n Set environment variable HADOOP_HOME or HADOOP_CONF_DIR. Configure 'hadoop-conf-dir' in the paimon catalog. Configure Hadoop options through prefix 'hadoop.' in the paimon catalog.  The first approach is recommended.\nHive/Spark HDFS Configuration is available directly through the computation cluster, see cluster configuration of Hive and Spark for details. Hadoop-compatible file systems (HCFS) #  All Hadoop file systems are automatically available when the Hadoop libraries are on the classpath.\nThis way, Paimon seamlessly supports all of Hadoop file systems implementing the org.apache.hadoop.fs.FileSystem interface, and all Hadoop-compatible file systems (HCFS).\n HDFS Alluxio (see configuration specifics below) XtreemFS …  The Hadoop configuration has to have an entry for the required file system implementation in the core-site.xml file.\nFor Alluxio support add the following entry into the core-site.xml file:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.alluxio.impl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;alluxio.hadoop.FileSystem\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Kerberos #  Flink It is recommented to use Flink Kerberos Keytab.Spark It is recommented to use Spark Kerberos Keytab.Hive An intuitive approach is to configure Hive\u0026rsquo;s kerberos authentication.Trino/JavaAPI Configure the following three options in your catalog configuration:\n security.kerberos.login.keytab: Absolute path to a Kerberos keytab file that contains the user credentials. Please make sure it is copied to each machine. security.kerberos.login.principal: Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache: True or false, indicates whether to read from your Kerberos ticket cache.  For JavaAPI:\nSecurityContext.install(catalogOptions);  HDFS HA #  Ensure that hdfs-site.xml and core-site.xml contain the necessary HA configuration.\nHDFS ViewFS #  Ensure that hdfs-site.xml and core-site.xml contain the necessary ViewFs configuration.\n"});index.add({'id':15,'href':'/docs/0.4/maintenance/read-performance/','title':"Read Performance",'section':"Maintenance",'content':"Read Performance #  Full Compaction #  Configure \u0026lsquo;full-compaction.delta-commits\u0026rsquo; perform full-compaction periodically in Flink writing. And it can ensure that partitions are full compacted before writing ends.\nIt is not recommended to set a value that exceeds the snapshot expiration time (default 1 hour). For example, if your checkpoint interval is 1 minute, it is recommended to set the \u0026lsquo;full-compaction.delta-commits\u0026rsquo; to 30.\nPrimary Key Table #  For Primary Key Table, it\u0026rsquo;s a \u0026lsquo;MergeOnRead\u0026rsquo; technology. When reading data, multiple layers of LSM data are merged, and the number of parallelism will be limited by the number of buckets. Although Paimon\u0026rsquo;s merge will be efficient, it still cannot catch up with the ordinary AppendOnly table.\nIf you want to query fast enough in certain scenarios, but can only find older data, you can:\n Configure \u0026lsquo;full-compaction.delta-commits\u0026rsquo;, when writing data (currently only Flink), full compaction will be performed periodically. Configure \u0026lsquo;scan.mode\u0026rsquo; to \u0026lsquo;compacted-full\u0026rsquo;, when reading data, snapshot of full compaction is picked. Read performance is good.  You can flexibly balance query performance and data latency when reading.\nAppend Only Table #  Small files can slow reading and affect DFS stability. By default, when there are more than \u0026lsquo;compaction.max.file-num\u0026rsquo; (default 50) small files in a single bucket, a compaction is triggered. However, when there are multiple buckets, many small files will be generated.\nYou can use full-compaction to reduce small files. Full-compaction will eliminate most small files.\nFormat #  Paimon has some query optimizations to parquet reading, so parquet will be slightly faster that orc.\n"});index.add({'id':16,'href':'/docs/0.4/how-to/altering-tables/','title':"Altering Tables",'section':"How to",'content':"Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.\nFlink ALTER TABLE my_table SET ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Spark3 ALTER TABLE my_table SET TBLPROPERTIES ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; );  Rename Table Name #  The following SQL rename the table name to new name.\nFlink ALTER TABLE my_table RENAME TO my_table_new; Spark3 ALTER TABLE my_table RENAME TO my_table_new;  If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.  Removing Table Properties #  The following SQL removes write-buffer-size table property.\nFlink ALTER TABLE my_table RESET (\u0026#39;write-buffer-size\u0026#39;); Spark3 ALTER TABLE my_table UNSET TBLPROPERTIES (\u0026#39;write-buffer-size\u0026#39;);  Adding New Columns #  The following SQL adds two columns c1 and c2 to table my_table.\nSpark3 ALTER TABLE my_table ADD COLUMNS ( c1 INT, c2 STRING );  Adding Column Position #  To add a new column with specified position, use FIRST or AFTER col_name.\nSpark3 ALTER TABLE my_table ADD COLUMN c INT FIRST; ALTER TABLE my_table ADD COLUMN c INT AFTER b;  Renaming Column Name #  The following SQL renames column c0 in table my_table to c1.\nSpark3 ALTER TABLE my_table RENAME COLUMN c0 TO c1;  Dropping Columns #  The following SQL drops tow columns c1 and c2 from table my_table.\nSpark3 ALTER TABLE my_table DROP COLUMNS (c1, c2);  Changing Column Nullability #  The following SQL sets column coupon_info to be nullable.\nSpark3 ALTER TABLE my_table ALTER COLUMN coupon_info DROP NOT NULL;  Changing Column Comment #  The following SQL changes comment of column buy_count to buy count.\nSpark3 ALTER TABLE my_table ALTER COLUMN buy_count COMMENT \u0026#39;buy count\u0026#39;;  Changing Column Position #  To modify an existent column to a new position, use FIRST or AFTER col_name.\nSpark3 ALTER TABLE my_table ALTER COLUMN col_a FIRST; ALTER TABLE my_table ALTER COLUMN col_a AFTER col_b;  Changing Column Type #  The following SQL changes type of column col_a to DOUBLE.\nSpark3 ALTER TABLE my_table ALTER COLUMN col_a TYPE \u0026#39;DOUBLE\u0026#39;;  "});index.add({'id':17,'href':'/docs/0.4/project/contributing/','title':"Contributing",'section':"Project",'content':"Contributing #  Apache Paimon (incubating) is developed by an open and friendly community. Everybody is cordially welcome to join the community and contribute to Apache Paimon. There are several ways to interact with the community and contribute to Paimon including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving website, testing release candidates and writing corresponding blog etc.\nWhat do you want to do? Contributing to Apache Paimon goes beyond writing code for the project. Below, we list different opportunities to help the project:\n  Area Further information      Report Bug To report a problem with Paimon, open Paimon’s issues.  Please give detailed information about the problem you encountered and, if possible, add a description that helps to reproduce the problem.    Contribute Code Read the Code Contribution Guide    Code Reviews Read the Code Review Guide    Release Version Releasing a new Paimon version.    Support Users Reply to questions on the user mailing list, check the latest issues in Issues for tickets which are actually user questions.      Spread the Word About Paimon Organize or attend a Paimon Meetup, contribute to the Paimon blog, share your conference, meetup or blog post on the dev@paimon.apache.org mailing list.     Any other question? Reach out to the dev@paimon.apache.org mailing list to get help!     Code Contribution Guide #  Apache Paimon is maintained, improved, and extended by code contributions of volunteers. We welcome contributions to Paimon.\nPlease feel free to ask questions at any time. Either send a mail to the Dev mailing list or comment on the issue you are working on.\n .contribute-grid { margin-bottom: 10px; display: flex; flex-direction: column; margin-left: -2px; margin-right: -2px; } .contribute-grid .column { margin-top: 4px; padding: 0 2px; } @media only screen and (min-width: 480px) { .contribute-grid { flex-direction: row; flex-wrap: wrap; } .contribute-grid .column { flex: 0 0 50%; } .contribute-grid .column { margin-top: 4px; } } @media only screen and (min-width: 960px) { .contribute-grid { flex-wrap: nowrap; } .contribute-grid .column { flex: 0 0 25%; } } .contribute-grid .panel { height: 100%; margin: 0; } .contribute-grid .panel-body { padding: 10px; } .contribute-grid h2 { margin: 0 0 10px 0; padding: 0; display: flex; align-items: flex-start; } .contribute-grid .number { margin-right: 0.25em; font-size: 1.5em; line-height: 0.9; }  1Discuss Create a Issue or mailing list discussion and reach consensus\nTo request an issue, please note that it is not just a \"please assign it to me\", you need to explain your understanding of the issue, and your design, and if possible, you need to provide your POC code.\n   2Implement Create the Pull Request and the approach agreed upon in the issue.\n1.Only create the PR if you are assigned to the issue. 2.Please associate an issue (if any), e.g. fix #123. 3.Please enable the actions of your own clone project.\n   3Review Work with the reviewer.\n1.Make sure no unrelated or unnecessary reformatting changes are included. 2.Please ensure that the test passing. 3.Please don't resolve conversation.\n   4Merge A committer of Paimon checks if the contribution fulfills the requirements and merges the code to the codebase.\n    Code Review Guide #  Every review needs to check the following six aspects. We encourage to check these aspects in order, to avoid spending time on detailed code quality reviews when formal requirements are not met or there is no consensus in the community to accept the change.\n1. Is the Contribution Well-Described? #  Check whether the contribution is sufficiently well-described to support a good review. Trivial changes and fixes do not need a long description. If the implementation is exactly according to a prior discussion on issue or the development mailing list, only a short reference to that discussion is needed.\nIf the implementation is different from the agreed approach in the consensus discussion, a detailed description of the implementation is required for any further review of the contribution.\n2. Does the Contribution Need Attention from some Specific Committers? #  Some changes require attention and approval from specific committers.\nIf the pull request needs specific attention, one of the tagged committers/contributors should give the final approval.\n3. Is the Overall Code Quality Good, Meeting Standard we Want to Maintain in Paimon? #   Does the code follow the right software engineering practices? Is the code correct, robust, maintainable, testable? Are the changes performance aware, when changing a performance sensitive part? Are the changes sufficiently covered by tests? Are the tests executing fast? If dependencies have been changed, were the NOTICE files updated?  Code guidelines can be found in the Flink Java Code Style and Quality Guide.\n4. Are the documentation updated? #  If the pull request introduces a new feature, the feature should be documented.\nBecome a Committer #  How to become a committer #  There is no strict protocol for becoming a committer. Candidates for new committers are typically people that are active contributors and community members. Candidates are suggested by current committers or PPMC members, and voted upon by the PPMC.\nIf you would like to become a committer, you should engage with the community and start contributing to Apache Paimon in any of the above ways. You might also want to talk to other committers and ask for their advice and guidance.\n  Community contributions include helping to answer user questions on the mailing list, verifying release candidates, giving talks, organizing community events, and other forms of evangelism and community building. The \u0026ldquo;Apache Way\u0026rdquo; has a strong focus on the project community, and committers can be recognized for outstanding community contributions even without any code contributions.\n  Code/technology contributions include contributed pull requests (patches), design discussions, reviews, testing, and other help in identifying and fixing bugs. Especially constructive and high quality design discussions, as well as helping other contributors, are strong indicators.\n  Identify promising candidates #  While the prior points give ways to identify promising candidates, the following are \u0026ldquo;must haves\u0026rdquo; for any committer candidate:\n  Being community minded: The candidate understands the meritocratic principles of community management. They do not always optimize for as much as possible personal contribution, but will help and empower others where it makes sense.\n  We trust that a committer candidate will use their write access to the repositories responsibly, and if in doubt, conservatively. It is important that committers are aware of what they know and what they don\u0026rsquo;t know. In doubt, committers should ask for a second pair of eyes rather than commit to parts that they are not well familiar with.\n  They have shown to be respectful towards other community members and constructive in discussions.\n  "});index.add({'id':18,'href':'/docs/0.4/maintenance/expiring-snapshots/','title':"Expiring Snapshots",'section':"Maintenance",'content':"Expiring Snapshots #  Paimon writers generates one or two snapshots per commit. Each snapshot may add some new data files or mark some old data files as deleted. However, the marked data files are not truly deleted because Paimon also supports time traveling to an earlier snapshot. They are only deleted when the snapshot expires.\nCurrently, expiration is automatically performed by Paimon writers when committing new changes. By expiring old snapshots, old data files and metadata files that are no longer used can be deleted to release disk space.\nSnapshot expiration is controlled by the following table properties.\n  Option Required Default Type Description     snapshot.time-retained No 1 h Duration The maximum time of completed snapshots to retain.   snapshot.num-retained.min No 10 Integer The minimum number of completed snapshots to retain.   snapshot.num-retained.max No Integer.MAX_VALUE Integer The maximum number of completed snapshots to retain.    Please note that too short retain time or too small retain number may result in:\n Batch queries cannot find the file. For example, the table is relatively large and the batch query takes 10 minutes to read, but the snapshot from 10 minutes ago expires, at which point the batch query will read a deleted snapshot. Streaming reading jobs on table files (without the external log system) fail to restart. When the job restarts, the snapshot it recorded may have expired.  "});index.add({'id':19,'href':'/docs/0.4/concepts/file-layouts/','title':"File Layouts",'section':"Concepts",'content':"File Layouts #  All files of a table are stored under one base directory. Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.\nSnapshot Files #  All snapshot files are stored in the snapshot directory.\nA snapshot file is a JSON file containing information about this snapshot, including\n the schema file in use the manifest list containing all changes of this snapshot  Manifest Files #  All manifest lists and manifest files are stored in the manifest directory.\nA manifest list is a list of manifest file names.\nA manifest file is a file containing changes about LSM data files and changelog files. For example, which LSM data file is created and which file is deleted in the corresponding snapshot.\nData Files #  Data files are grouped by partitions and buckets. Each bucket directory contains an LSM tree and its changelog files.\nCurrently, Paimon supports using orc(default), parquet and avro as data file\u0026rsquo;s format.\nLSM Trees #  Paimon adapts the LSM tree (log-structured merge-tree) as the data structure for file storage. This documentation briefly introduces the concepts about LSM trees.\nSorted Runs #  LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nRecords within a data file are sorted by their primary keys. Within a sorted run, ranges of primary keys of data files never overlap.\nAs you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified merge engine and the timestamp of each record.\nNew records written into the LSM tree will be first buffered in memory. When the memory buffer is full, all records in memory will be sorted and flushed to disk. A new sorted run is now created.\nCompaction #  When more and more records are written into the LSM tree, the number of sorted runs will increase. Because querying an LSM tree requires all sorted runs to be combined, too many sorted runs will result in a poor query performance, or even out of memory.\nTo limit the number of sorted runs, we have to merge several sorted runs into one big sorted run once in a while. This procedure is called compaction.\nHowever, compaction is a resource intensive procedure which consumes a certain amount of CPU time and disk IO, so too frequent compaction may in turn result in slower writes. It is a trade-off between query and write performance. Paimon currently adapts a compaction strategy similar to Rocksdb\u0026rsquo;s universal compaction.\nBy default, when Paimon writers append records to the LSM tree, they\u0026rsquo;ll also perform compactions as needed. Users can also choose to perform all compactions in a dedicated compaction job. See dedicated compaction job for more info.\n"});index.add({'id':20,'href':'/docs/0.4/filesystems/','title':"Filesystems",'section':"Apache Paimon",'content':""});index.add({'id':21,'href':'/docs/0.4/filesystems/oss/','title':"OSS",'section':"Filesystems",'content':"OSS #  Download paimon-oss-0.4.0-incubating.jar. Flink If you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-oss-0.4.0-incubating.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://path/to/warehouse\u0026#39;, \u0026#39;fs.oss.endpoint\u0026#39; = \u0026#39;oss-cn-hangzhou.aliyuncs.com\u0026#39;, \u0026#39;fs.oss.accessKeyId\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;fs.oss.accessKeySecret\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured oss access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-oss-0.4.0-incubating.jar together with paimon-spark-0.4.0-incubating.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=oss://\u0026lt;bucket-name\u0026gt;/ \\  --conf spark.sql.catalog.paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com \\  --conf spark.sql.catalog.paimon.fs.oss.accessKeyId=xxx \\  --conf spark.sql.catalog.paimon.fs.oss.accessKeySecret=yyy Hive If you have already configured oss access through Hive (Via Hadoop FileSystem), here you can skip the following configuration.  NOTE: You need to ensure that Hive metastore can access oss.\nPlace paimon-oss-0.4.0-incubating.jar together with paimon-hive-connector-0.4.0-incubating.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com; SET paimon.fs.oss.accessKeyId=xxx; SET paimon.fs.oss.accessKeySecret=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino Place paimon-oss-0.4.0-incubating.jar together with paimon-trino-0.4.0-incubating.jar under plugin/paimon directory.\nAdd options in etc/catalog/paimon.properties.\nfs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com fs.oss.accessKeyId=xxx fs.oss.accessKeySecret=yyy  "});index.add({'id':22,'href':'/docs/0.4/engines/spark3/','title':"Spark3",'section':"Engines",'content':"Spark3 #  This documentation is a guide for using Paimon in Spark3.\nPreparing Paimon Jar File #  Paimon currently supports Spark 3.3, 3.2 and 3.1. We recommend the latest Spark version for a better experience.\nDownload the jar file with corresponding version.\n   Version Jar     Spark 3.3 paimon-spark-3.3-0.4.0-incubating.jar   Spark 3.2 paimon-spark-3.2-0.4.0-incubating.jar   Spark 3.1 paimon-spark-3.1-0.4.0-incubating.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests For Spark 3.3, you can find the bundled jar in ./paimon-spark/paimon-spark-3.3/target/paimon-spark-3.3-0.4.0-incubating.jar.\nQuick Start #  If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.  Step 1: Specify Paimon Jar File\nAppend path to paimon jar file to the --jars argument when starting spark-sql.\nspark-sql ... --jars /path/to/paimon-spark-3.3-0.4.0-incubating.jar Alternatively, you can copy paimon-spark-3.3-0.4.0-incubating.jar under spark/jars in your Spark installation directory.\nStep 2: Specify Paimon Catalog\nWhen starting spark-sql, use the following command to register Paimon’s Spark catalog with the name paimon. Table files of the warehouse is stored under /tmp/paimon.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=file:/tmp/paimon After spark-sql command line has started, run the following SQL to create and switch to database paimon.default.\nCREATE DATABASE paimon.default; USE paimon.default; Step 3: Create a table and Write Some Records\ncreate table my_table ( k int, v string ) tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; ); INSERT INTO my_table VALUES (1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;); Step 4: Query Table with SQL\nSELECT * FROM my_table; /* 1\tHi 2\tHello */ Step 5: Update the Records\nINSERT INTO my_table VALUES (1, \u0026#39;Hi Again\u0026#39;), (3, \u0026#39;Test\u0026#39;); SELECT * FROM my_table; /* 1\tHi Again 2\tHello 3\tTest */ Step 6: Query Table with Scala API\nIf you don\u0026rsquo;t want to use Paimon catalog, you can also run spark-shell and query the table with Scala API.\nspark-shell ... --jars /path/to/paimon-spark-3.3-0.4.0-incubating.jar val dataset = spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;file:/tmp/paimon/default.db/my_table\u0026#34;) dataset.createOrReplaceTempView(\u0026#34;my_table\u0026#34;) spark.sql(\u0026#34;SELECT * FROM my_table\u0026#34;).show() Spark Type Conversion #  This section lists all supported type conversion between Spark and Flink. All Spark\u0026rsquo;s data types are available in package org.apache.spark.sql.types.\n  Spark Data Type Flink Data Type Atomic Type     StructType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   ByteType TinyIntType true   ShortType SmallIntType true   IntegerType IntType true   LongType BigIntType true   FloatType FloatType true   DoubleType DoubleType true   StringType VarCharType, CharType true   DateType DateType true   TimestampType TimestampType, LocalZonedTimestamp true   DecimalType(precision, scale) DecimalType(precision, scale) true   BinaryType VarBinaryType, BinaryType true     Currently, Spark\u0026rsquo;s field comment cannot be described under Flink CLI. Conversion between Spark\u0026rsquo;s UserDefinedType and Flink\u0026rsquo;s UserDefinedType is not supported.   "});index.add({'id':23,'href':'/docs/0.4/concepts/file-operations/','title':"File Operations",'section':"Concepts",'content':"File Operations #  This article is specifically designed to clarify the impact that various file operations have on files.\nThis page provides concrete examples and practical tips for effectively managing them. Furthermore, through an in-depth exploration of operations such as commit and compact, we aim to offer insights into the creation and updates of files.\nPrerequisite #  Before delving further into this page, please ensure that you have read through the following sections:\n Basic Concepts, File Layouts and How to use Paimon in Flink.  Create Catalog #  Start Flink SQL client via ./sql-client.sh and execute the following statements one by one to create a Paimon catalog.\nCREATE CATALOG paimon WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;file:///tmp/paimon\u0026#39; ); USE CATALOG paimon; This will only create a directory at given path file:///tmp/paimon.\nCreate Table #  Execute the following create table statement will create a Paimon table with 3 fields:\nCREATE TABLE T ( id BIGINT, a INT, b STRING, dt STRING COMMENT \u0026#39;timestamp string in format yyyyMMdd\u0026#39;, PRIMARY KEY(id, dt) NOT ENFORCED ) PARTITIONED BY (dt); This will create Paimon table T under the path /tmp/paimon/default.db/T, with its schema stored in /tmp/paimon/default.db/T/schema/schema-0\nInsert Records Into Table #  Run the following insert statement in Flink SQL:\nINSERT INTO T VALUES (1, 10001, \u0026#39;varchar00001\u0026#39;, \u0026#39;20230501\u0026#39;); Once the Flink job is completed, the records are written to the Paimon table through a successful commit. Users can verify the visibility of these records by executing the query SELECT * FROM T which will return a single row. The commit process creates a snapshot located at the path /tmp/paimon/default.db/T/snapshot/snapshot-1. The resulting file layout at snapshot-1 is as described below:\nThe content of snapshot-1 contains metadata of the snapshot, such as manifest list and schema id:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 1, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;7d758485-981d-4b1a-a0c6-d34c3eb254bf\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;APPEND\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684155393354, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 1, \u0026#34;deltaRecordCount\u0026#34; : 1, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } Remind that a manifest list contains all changes of the snapshot, baseManifestList is the base file upon which the changes in deltaManifestList is applied. The first commit will result in 1 manifest file, and 2 manifest lists are created (the file names might differ from those in your experiment):\n./T/manifest: manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\tmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 is the manifest file (manifest-1-0 in the above graph), which stores the information about the data files in the snapshot.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 is the baseManifestList (manifest-list-1-base in the above graph), which is effectively empty.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 is the deltaManifestList (manifest-list-1-delta in the above graph), which contains a list of manifest entries that perform operations on data files, which, in this case, is manifest-1-0.\nNow let\u0026rsquo;s insert a batch of records across different partitions and see what happens. In Flink SQL, execute the following statement:\nINSERT INTO T VALUES (2, 10002, \u0026#39;varchar00002\u0026#39;, \u0026#39;20230502\u0026#39;), (3, 10003, \u0026#39;varchar00003\u0026#39;, \u0026#39;20230503\u0026#39;), (4, 10004, \u0026#39;varchar00004\u0026#39;, \u0026#39;20230504\u0026#39;), (5, 10005, \u0026#39;varchar00005\u0026#39;, \u0026#39;20230505\u0026#39;), (6, 10006, \u0026#39;varchar00006\u0026#39;, \u0026#39;20230506\u0026#39;), (7, 10007, \u0026#39;varchar00007\u0026#39;, \u0026#39;20230507\u0026#39;), (8, 10008, \u0026#39;varchar00008\u0026#39;, \u0026#39;20230508\u0026#39;), (9, 10009, \u0026#39;varchar00009\u0026#39;, \u0026#39;20230509\u0026#39;), (10, 10010, \u0026#39;varchar00010\u0026#39;, \u0026#39;20230510\u0026#39;); The second commit takes place and executing SELECT * FROM T will return 10 rows. A new snapshot, namely snapshot-2, is created and gives us the following physical file layout:\n% ls -atR . ./T: dt=20230501 dt=20230502\tdt=20230503\tdt=20230504\tdt=20230505\tdt=20230506\tdt=20230507\tdt=20230508\tdt=20230509\tdt=20230510\tsnapshot schema manifest ./T/snapshot: LATEST snapshot-2 EARLIEST snapshot-1 ./T/manifest: manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-1\t# delta manifest list for snapshot-2 manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-0 # base manifest list for snapshot-2\t manifest-f1267033-e246-4470-a54c-5c27fdbdd074-0\t# manifest file for snapshot-2 manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\t# delta manifest list for snapshot-1  manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 # base manifest list for snapshot-1 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 # manifest file for snapshot-1 ./T/dt=20230501/bucket-0: data-b75b7381-7c8b-430f-b7e5-a204cb65843c-0.orc ... # each partition has the data written to bucket-0 ... ./T/schema: schema-0 The new file layout as of snapshot-2 looks like Delete Records From Table #  Now let\u0026rsquo;s delete records that meet the condition dt\u0026gt;=20230503. In Flink SQL, execute the following statement:\nDELETE FROM T WHERE dt \u0026gt;= \u0026#39;20230503\u0026#39;; The third commit takes place and it gives us snapshot-3. Now, listing the files under the table and your will find out no partition is dropped. Instead, a new data file is created for partition 20230503 to 20230510:\n./T/dt=20230510/bucket-0: data-b93f468c-b56f-4a93-adc4-b250b3aa3462-0.orc # newer data file created by the delete statement  data-0fcacc70-a0cb-4976-8c88-73e92769a762-0.orc # older data file created by the insert statement This make sense since we insert a record in the second commit (represented by +I[10, 10010, 'varchar00010', '20230510']) and then delete the record in the third commit. Executing SELECT * FROM T will return 2 rows, namely:\n+I[1, 10001, 'varchar00001', '20230501'] +I[2, 10002, 'varchar00002', '20230502'] The new file layout as of snapshot-3 looks like Note that manifest-3-0 contains 8 manifest entries of ADD operation type, corresponding to 8 newly written data files.\nCompact Table #  As you may have noticed, the number of small files will augment over successive snapshots, which may lead to decreased read performance. Therefore, a full-compaction is needed in order to reduce the number of small files.\nLet\u0026rsquo;s trigger the full-compaction now. Make sure you have set execution mode to batch (add an entry execution.runtime-mode: batch in flink-conf.yaml) and run a dedicated compaction job through flink run:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ an example would be (suppose you\u0026rsquo;re already in Flink home)\n./bin/flink run \\  ./lib/paimon-flink-action-0.4.0-incubating.jar \\  compact \\  --path file:///tmp/paimon/default.db/T All current table files will be compacted and a new snapshot, namely snapshot-4, is made and contains the following information:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 4, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;a3d951d5-aa0e-4071-a5d4-4c72a4233d48\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;COMPACT\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684163217960, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 38, \u0026#34;deltaRecordCount\u0026#34; : 20, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } The new file layout as of snapshot-4 looks like Note that manifest-4-0 contains 20 manifest entries (18 DELETE operations and 2 ADD operations)\n For partition 20230503 to 20230510, two DELETE operations for two data files For partition 20230501 to 20230502, one DELETE operation and one ADD operation for the same data file.  Alter Table #  Execute the following statement to configure full-compaction:\nALTER TABLE T SET (\u0026#39;full-compaction.delta-commits\u0026#39; = \u0026#39;1\u0026#39;); It will create a new schema for Paimon table, namely schema-1, but no snapshot has actually used this schema yet until the next commit.\nExpire Snapshots #  Remind that the marked data files are not truly deleted until the snapshot expires and no consumer depends on the snapshot. For more information, see Expiring Snapshots.\nDuring the process of snapshot expiration, the range of snapshots is initially determined, and then data files within these snapshots are marked for deletion. A data file is marked for deletion only when there is a manifest entry of kind DELETE that references that specific data file. This marking ensures that the file will not be utilized by subsequent snapshots and can be safely removed.\nLet\u0026rsquo;s say all 4 snapshots in the above diagram are about to expire. The expire process is as follows:\n  It first deletes all marked data files, and records any changed buckets.\n  It then deletes any changelog files and associated manifests.\n  Finally, it deletes the snapshots themselves and writes the earliest hint file.\n  If any directories are left empty after the deletion process, they will be deleted as well.\nLet\u0026rsquo;s say another snapshot, snapshot-5 is created and snapshot expiration is triggered. snapshot-1 to snapshot-4 are\nto be deleted. For simplicity, we will only focus on files from previous snapshots, the final layout after snapshot expiration looks like:\nAs a result, partition 20230503 to 20230510 are physically deleted.\nFlink Stream Write #  Finally, we will examine Flink Stream Write by utilizing the example of CDC ingestion. This section will address the capturing and writing of change data into Paimon, as well as the mechanisms behind asynchronous compact and snapshot commit and expiration.\nTo begin, let\u0026rsquo;s take a closer look at the CDC ingestion workflow and the unique roles played by each component involved.\n MySQL CDC Source uniformly reads snapshot and incremental data, with SnapshotReader reading snapshot data and BinlogReader reading incremental data, respectively. Paimon Sink writes data into Paimon table in bucket level. The CompactManager within it will trigger compaction asynchronously. Committer Operator is a singleton responsible for committing and expiring snapshots.  Next, we will go over end-to-end data flow.\nMySQL Cdc Source read snapshot and incremental data and emit them to downstream after normalization.\nPaimon Sink first buffers new records in a heap-based LSM tree, and flushes them to disk when the memory buffer is full. Note that each data file written is a sorted run. At this point, no manifest file and snapshot is created. Right before Flink checkpoint takes places, Paimon Sink will flush all buffered records and send committable message to downstream, which is read and committed by Committer Operator during checkpoint.\nDuring checkpoint, Committer Operator will create a new snapshot and associate it with manifest lists so that the snapshot\ncontains information about all data files in the table.\nAt later point asynchronous compaction might take place, and the committable produced by CompactManager contains information about previous files and merged files so that Committer Operator can construct corresponding manifest entries. In this case Committer Operator might produce two snapshot during Flink checkpoint, one for data written (snapshot of kind Append) and the other for compact (snapshot of kind Compact). If no data file is written during checkpoint interval, only snapshot of kind Compact will be created. Committer Operator will check against snapshot expiration and perform physical deletion of marked data files.\n"});index.add({'id':24,'href':'/docs/0.4/how-to/','title':"How to",'section':"Apache Paimon",'content':""});index.add({'id':25,'href':'/docs/0.4/maintenance/rescale-bucket/','title':"Rescale Bucket",'section':"Maintenance",'content':"Rescale Bucket #  Since the number of total buckets dramatically influences the performance, Paimon allows users to tune bucket numbers by ALTER TABLE command and reorganize data layout by INSERT OVERWRITE without recreating the table/partition. When executing overwrite jobs, the framework will automatically scan the data with the old bucket number and hash the record according to the current bucket number.\nRescale Overwrite #  -- rescale number of total buckets ALTER TABLE table_identifier SET (\u0026#39;bucket\u0026#39; = \u0026#39;...\u0026#39;) -- reorganize data layout of table/partition INSERT OVERWRITE table_identifier [PARTITION (part_spec)] SELECT ... FROM table_identifier [WHERE part_spec] Please note that\n ALTER TABLE only modifies the table\u0026rsquo;s metadata and will NOT reorganize or reformat existing data. Reorganize existing data must be achieved by INSERT OVERWRITE. Rescale bucket number does not influence the read and running write jobs. Once the bucket number is changed, any newly scheduled INSERT INTO jobs which write to without-reorganized existing table/partition will throw a TableException with message like Try to write table/partition ... with a new bucket num ..., but the previous bucket num is ... Please switch to batch mode, and perform INSERT OVERWRITE to rescale current data layout first.  For partitioned table, it is possible to have different bucket number for different partitions. E.g. ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;4\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-01\u0026#39;) SELECT * FROM ...; ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-02\u0026#39;) SELECT * FROM ...;  During overwrite period, make sure there are no other jobs writing the same table/partition.  Note: For the table which enables log system(e.g. Kafka), please rescale the topic\u0026rsquo;s partition as well to keep consistency.  Use Case #  Rescale bucket helps to handle sudden spikes in throughput. Suppose there is a daily streaming ETL task to sync transaction data. The table\u0026rsquo;s DDL and pipeline are listed as follows.\n-- table DDL CREATE TABLE verified_orders ( trade_order_id BIGINT, item_id BIGINT, item_price DOUBLE, dt STRING, PRIMARY KEY (dt, trade_order_id, item_id) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;16\u0026#39; ); -- like from a kafka table CREATE temporary TABLE raw_orders( trade_order_id BIGINT, item_id BIGINT, item_price BIGINT, gmt_create STRING, order_status STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); -- streaming insert as bucket num = 16 INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;; The pipeline has been running well for the past few weeks. However, the data volume has grown fast recently, and the job\u0026rsquo;s latency keeps increasing. To improve the data freshness, users can\n Suspend the streaming job with a savepoint ( see Suspended State and Stopping a Job Gracefully Creating a Final Savepoint ) $ ./bin/flink stop \\  --savepointPath /tmp/flink-savepoints \\  $JOB_ID  Increase the bucket number -- scaling out ALTER TABLE verified_orders SET (\u0026#39;bucket\u0026#39; = \u0026#39;32\u0026#39;);  Switch to the batch mode and overwrite the current partition(s) to which the streaming job is writing SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- suppose today is 2022-06-22 -- case 1: there is no late event which updates the historical partitions, thus overwrite today\u0026#39;s partition is enough INSERT OVERWRITE verified_orders PARTITION (dt = \u0026#39;2022-06-22\u0026#39;) SELECT trade_order_id, item_id, item_price FROM verified_orders WHERE dt = \u0026#39;2022-06-22\u0026#39;; -- case 2: there are late events updating the historical partitions, but the range does not exceed 3 days INSERT OVERWRITE verified_orders SELECT trade_order_id, item_id, item_price, dt FROM verified_orders WHERE dt IN (\u0026#39;2022-06-20\u0026#39;, \u0026#39;2022-06-21\u0026#39;, \u0026#39;2022-06-22\u0026#39;);  After overwrite job has finished, switch back to streaming mode. And now, the parallelism can be increased alongside with bucket number to restore the streaming job from the savepoint ( see Start a SQL Job from a savepoint ) SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026lt;savepointPath\u0026gt;; INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;;   "});index.add({'id':26,'href':'/docs/0.4/filesystems/s3/','title':"S3",'section':"Filesystems",'content':"S3 #  Download paimon-s3-0.4.0-incubating.jar. Flink If you have already configured s3 access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-s3-0.4.0-incubating.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;s3://path/to/warehouse\u0026#39;, \u0026#39;s3.endpoint\u0026#39; = \u0026#39;your-endpoint-hostname\u0026#39;, \u0026#39;s3.access-key\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;s3.secret-key\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured s3 access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-s3-0.4.0-incubating.jar together with paimon-spark-0.4.0-incubating.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=s3://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt; \\  --conf spark.sql.catalog.paimon.s3.endpoint=your-endpoint-hostname \\  --conf spark.sql.catalog.paimon.s3.access-key=xxx \\  --conf spark.sql.catalog.paimon.s3.secret-key=yyy Hive If you have already configured s3 access through Hive ((Via Hadoop FileSystem)), here you can skip the following configuration.  NOTE: You need to ensure that Hive metastore can access s3.\nPlace paimon-s3-0.4.0-incubating.jar together with paimon-hive-connector-0.4.0-incubating.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.s3.endpoint=your-endpoint-hostname; SET paimon.s3.access-key=xxx; SET paimon.s3.secret-key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino Place paimon-s3-0.4.0-incubating.jar together with paimon-trino-0.4.0-incubating.jar under plugin/paimon directory.\nAdd options in etc/catalog/paimon.properties.\ns3.endpoint=your-endpoint-hostname s3.access-key=xxx s3.secret-key=yyy  S3 Complaint Object Stores #  The S3 Filesystem also support using S3 compliant object stores such as MinIO, Tencent\u0026rsquo;s COS and IBM’s Cloud Object Storage. Just configure your endpoint to the provider of the object store service.\ns3.endpoint:your-endpoint-hostnameConfigure Path Style Access #  Some S3 compliant object stores might not have virtual host style addressing enabled by default, for example when using Standalone MinIO for testing purpose. In such cases, you will have to provide the property to enable path style access.\ns3.path.style.access:trueS3A Performance #  Tune Performance for S3AFileSystem.\nIf you encounter the following exception:\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool. Try to configure this in catalog options: fs.s3a.connection.maximum=1000.\n"});index.add({'id':27,'href':'/docs/0.4/engines/spark2/','title':"Spark2",'section':"Engines",'content':"Spark2 #  This documentation is a guide for using Paimon in Spark2.\nVersion #  Paimon supports Spark 2.4+. It is highly recommended to use Spark 2.4+ version with many improvements.\nPreparing Paimon Jar File #  Download paimon-spark-2-0.4.0-incubating.jar. You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests You can find the bundled jar in ./paimon-spark/paimon-spark-2/target/paimon-spark-2-0.4.0-incubating.jar.\nQuick Start #  If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.  Step 1: Prepare Test Data\nPaimon currently only supports reading tables through Spark2. To create a Paimon table with records, please follow our Flink quick start guide.\nAfter the guide, all table files should be stored under the path /tmp/paimon, or the warehouse path you\u0026rsquo;ve specified.\nStep 2: Specify Paimon Jar File\nYou can append path to paimon jar file to the --jars argument when starting spark-shell.\nspark-shell ... --jars /path/to/paimon-spark-2-0.4.0-incubating.jar Alternatively, you can copy paimon-spark-2-0.4.0-incubating.jar under spark/jars in your Spark installation directory.\nStep 3: Query Table\nPaimon with Spark 2.4 does not support DDL. You can use the Dataset reader and register the Dataset as a temporary table. In spark shell:\nval dataset = spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;file:/tmp/paimon/default.db/word_count\u0026#34;) dataset.createOrReplaceTempView(\u0026#34;word_count\u0026#34;) spark.sql(\u0026#34;SELECT * FROM word_count\u0026#34;).show() "});index.add({'id':28,'href':'/docs/0.4/how-to/writing-tables/','title':"Writing Tables",'section':"How to",'content':"Writing Tables #  You can use the INSERT statement to inserts new rows into a table or overwrites the existing data in the table. The inserted rows can be specified by value expressions or result from a query.\nSyntax #  INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }   part_spec\nAn optional parameter that specifies a comma-separated list of key and value pairs for partitions. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.\nSyntax: PARTITION ( partition_col_name = partition_col_val [ , \u0026hellip; ] )\n  column_list\nAn optional parameter that specifies a comma-separated list of columns belonging to the table_identifier table.\nSyntax: (col_name1 [, column_name2, \u0026hellip;])\nAll specified columns should exist in the table and not be duplicated from each other. It includes all columns except the static partition columns. The size of the column list should be exactly the size of the data from VALUES clause or query.     value_expr\nSpecifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted. A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows.\nSyntax: VALUES ( { value | NULL } [ , … ] ) [ , ( … ) ]\nCurrently, Flink doesn't support use NULL directly, so the NULL should be cast to actual data type by `CAST (NULL AS data_type)`.     For more information, please check the syntax document:\nFlink INSERT Statement\nSpark INSERT Statement\n Streaming reading will ignore the commits generated by INSERT OVERWRITE by default. If you want to read the commits of OVERWRITE, you can configure streaming-read-overwrite. For partitioned table, Paimon\u0026rsquo;s default overwrite mode is dynamic partition overwrite (that means Paimon only deletes the partitions appear in the overwrite data). You can configure dynamic-partition-overwrite to change it.   Applying Records/Changes to Tables #  Flink Use INSERT INTO to apply records and changes to tables.\nINSERT INTO MyTable SELECT ... Paimon supports shuffle data by partition and bucket in sink phase.\nSpark3 Use INSERT INTO to apply records and changes to tables.\nINSERT INTO MyTable SELECT ...  Overwriting the Whole Table #  For unpartitioned tables, Paimon supports overwriting the whole table.\nFlink Use INSERT OVERWRITE to overwrite the whole unpartitioned table.\nINSERT OVERWRITE MyTable SELECT ...  Overwriting a Partition #  For partitioned tables, Paimon supports overwriting a partition.\nFlink Use INSERT OVERWRITE to overwrite a partition.\nINSERT OVERWRITE MyTable PARTITION (key1 = value1, key2 = value2, ...) SELECT ...  Purging tables #  You can use INSERT OVERWRITE to purge tables by inserting empty value. For partitioned table, make sure that the table\u0026rsquo;s overwrite.dynamic-partition = false.\nFlink INSERT OVERWRITE MyTable SELECT * FROM MyTable WHERE false  Purging Partitions #  Currently, Paimon supports two ways to purge partitions.\n  Like purging tables, you can use INSERT OVERWRITE to purge data of partitions by inserting empty value to them.\n  Method #1 does not support to drop multiple partitions. In case that you need to drop multiple partitions, you can submit the drop-partition job through flink run.\n  Flink -- Syntax INSERT OVERWRITE MyTable PARTITION (key1 = value1, key2 = value2, ...) SELECT selectSpec FROM MyTable WHERE false -- The following SQL is an example: -- table definition CREATE TABLE MyTable ( k0 INT, k1 INT, v STRING ) PARTITIONED BY (k0, k1); -- you can use INSERT OVERWRITE MyTable PARTITION (k0 = 0) SELECT k1, v FROM MyTable WHERE false -- or INSERT OVERWRITE MyTable PARTITION (k0 = 0, k1 = 0) SELECT v FROM MyTable WHERE false Flink Job Run the following command to submit a drop-partition job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  drop-partition \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; --partition \u0026lt;partition_spec\u0026gt; [--partition \u0026lt;partition_spec\u0026gt; ...] partition_spec: key1=value1,key2=value2... For more information of drop-partition, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  drop-partition --help  Deleting from table #  Currently, Paimon supports deleting records via submitting the \u0026lsquo;delete\u0026rsquo; job through flink run.\nFlink Job Run the following command to submit a \u0026lsquo;delete\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  delete \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; --where \u0026lt;filter_spec\u0026gt; filter_spec is equal to the \u0026#39;WHERE\u0026#39; clause in SQL DELETE statement. Examples: age \u0026gt;= 18 AND age \u0026lt;= 60 animal \u0026lt;\u0026gt; \u0026#39;cat\u0026#39; id \u0026gt; (SELECT count(*) FROM employee) For more information of \u0026lsquo;delete\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  delete --help  Merging into table #  Paimon supports \u0026ldquo;MERGE INTO\u0026rdquo; via submitting the \u0026lsquo;merge-into\u0026rsquo; job through flink run.\nImportant table properties setting:\n Only primary key table supports this feature. The action won\u0026rsquo;t produce UPDATE_BEFORE, so it\u0026rsquo;s not recommended to set \u0026lsquo;changelog-producer\u0026rsquo; = \u0026lsquo;input\u0026rsquo;.   The design referenced such syntax:\nMERGE INTO target-table USING source-table | source-expr AS source-alias ON merge-condition WHEN MATCHED [AND matched-condition] THEN UPDATE SET xxx WHEN MATCHED [AND matched-condition] THEN DELETE WHEN NOT MATCHED [AND not-matched-condition] THEN INSERT VALUES (xxx) WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN UPDATE SET xxx WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN DELETE The merge-into action use \u0026ldquo;upsert\u0026rdquo; semantics instead of \u0026ldquo;update\u0026rdquo;, which means if the row exists, then do update, else do insert. For example, for non-primary-key table, you can update every column, but for primary key table, if you want to update primary keys, you have to insert a new row which has different primary keys from rows in the table. In this scenario, \u0026ldquo;upsert\u0026rdquo; is useful.\nFlink Job Run the following command to submit a \u0026lsquo;merge-into\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;target-table\u0026gt; \\  [--target-as \u0026lt;target-table-alias\u0026gt;] \\  --source-table \u0026lt;source-table-name\u0026gt; \\  [--source-sql \u0026lt;sql\u0026gt; ...]\\  --on \u0026lt;merge-condition\u0026gt; \\  --merge-actions \u0026lt;matched-upsert,matched-delete,not-matched-insert,not-matched-by-source-upsert,not-matched-by-source-delete\u0026gt; \\  --matched-upsert-condition \u0026lt;matched-condition\u0026gt; \\  --matched-upsert-set \u0026lt;upsert-changes\u0026gt; \\  --matched-delete-condition \u0026lt;matched-condition\u0026gt; \\  --not-matched-insert-condition \u0026lt;not-matched-condition\u0026gt; \\  --not-matched-insert-values \u0026lt;insert-values\u0026gt; \\  --not-matched-by-source-upsert-condition \u0026lt;not-matched-by-source-condition\u0026gt; \\  --not-matched-by-source-upsert-set \u0026lt;not-matched-upsert-changes\u0026gt; \\  --not-matched-by-source-delete-condition \u0026lt;not-matched-by-source-condition\u0026gt; You can pass sqls by \u0026#39;--source-sql \u0026lt;sql\u0026gt; [, --source-sql \u0026lt;sql\u0026gt; ...]\u0026#39; to config environment and create source table at runtime. -- Examples: -- Find all orders mentioned in the source table, then mark as important if the price is above 100 -- or delete if the price is under 10. ./flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions \\  matched-upsert,matched-delete \\  --matched-upsert-condition \u0026#34;T.price \u0026gt; 100\u0026#34; \\  --matched-upsert-set \u0026#34;mark = \u0026#39;important\u0026#39;\u0026#34; \\  --matched-delete-condition \u0026#34;T.price \u0026lt; 10\u0026#34; -- For matched order rows, increase the price, and if there is no match, insert the order from the -- source table: ./flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions \\  matched-upsert,not-matched-insert \\  --matched-upsert-set \u0026#34;price = T.price + 20\u0026#34; \\  --not-matched-insert-values * -- For not matched by source order rows (which are in the target table and does not match any row in the -- source table based on the merge-condition), decrease the price or if the mark is \u0026#39;trivial\u0026#39;, delete them: ./flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions \\  not-matched-by-source-upsert,not-matched-by-source-delete \\  --not-matched-by-source-upsert-condition \u0026#34;T.mark \u0026lt;\u0026gt; \u0026#39;trivial\u0026#39;\u0026#34; \\  --not-matched-by-source-upsert-set \u0026#34;price = T.price - 20\u0026#34; \\  --not-matched-by-source-delete-condition \u0026#34;T.mark = \u0026#39;trivial\u0026#39;\u0026#34; -- A --source-sql example: -- Create a temporary view S in new catalog and use it as source table ./flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-sql \u0026#34;CREATE CATALOG test_cat WITH (...)\u0026#34; \\  --source-sql \u0026#34;CREATE TEMPORARY VIEW test_cat.`default`.S AS SELECT order_id, price, \u0026#39;important\u0026#39; FROM important_order\u0026#34; \\  --source-table test_cat.default.S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions not-matched-insert\\  --not-matched-insert-values * The term \u0026lsquo;matched\u0026rsquo; explanation:\n matched: changed rows are from target table and each can match a source table row based on merge-condition and optional matched-condition (source ∩ target). not-matched: changed rows are from source table and all rows cannot match any target table row based on merge-condition and optional not-matched-condition (source - target). not-matched-by-source: changed rows are from target table and all row cannot match any source table row based on merge-condition and optional not-matched-by-source-condition (target - source).  Parameters format:\n matched-upsert-changes:\ncol = \u0026lt;source-table\u0026gt;.col | expression [, \u0026hellip;] (Means setting \u0026lt;target-table\u0026gt;.col with given value. Do not add \u0026lsquo;\u0026lt;target-table\u0026gt;.\u0026rsquo; before \u0026lsquo;col\u0026rsquo;.)\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to set columns with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not-matched-upsert-changes is similar to matched-upsert-changes, but you cannot reference source table\u0026rsquo;s column or use \u0026lsquo;*\u0026rsquo;. insert-values:\ncol1, col2, \u0026hellip;, col_end\nMust specify values of all columns. For each column, you can reference \u0026lt;source-table\u0026gt;.col or use an expression.\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to insert with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not-matched-condition cannot use target table\u0026rsquo;s columns to construct condition expression. not-matched-by-source-condition cannot use source table\u0026rsquo;s columns to construct condition expression.   Target alias cannot be duplicated with existed table name. If the source table is not in the current catalog and current database, the source-table-name must be qualified (database.table or catalog.database.table if created a new catalog). For examples:\n(1) If source table \u0026lsquo;my_source\u0026rsquo; is in \u0026lsquo;my_db\u0026rsquo;, qualify it:\n--source-table \u0026ldquo;my_db.my_source\u0026rdquo;\n(2) Example for sqls:\nWhen sqls changed current catalog and database, it\u0026rsquo;s OK to not qualify the source table name:\n--source-sql \u0026ldquo;CREATE CATALOG my_cat WITH (\u0026hellip;)\u0026quot;\n--source-sql \u0026ldquo;USE CATALOG my_cat\u0026rdquo;\n--source-sql \u0026ldquo;CREATE DATABASE my_db\u0026rdquo;\n--source-sql \u0026ldquo;USE my_db\u0026rdquo;\n--source-sql \u0026ldquo;CREATE TABLE S \u0026hellip;\u0026quot;\n--source-table S\nbut you must qualify it in the following case:\n--source-sql \u0026ldquo;CREATE CATALOG my_cat WITH (\u0026hellip;)\u0026quot;\n--source-sql \u0026ldquo;CREATE TABLE my_cat.`default`.S \u0026hellip;\u0026quot;\n--source-table my_cat.default.S\nYou can use just \u0026lsquo;S\u0026rsquo; as source table name in following arguments. At least one merge action must be specified. If both matched-upsert and matched-delete actions are present, their conditions must both be present too (same to not-matched-by-source-upsert and not-matched-by-source-delete). Otherwise, all conditions are optional. All conditions, set changes and values should use Flink SQL syntax. To ensure the whole command runs normally in Shell, please quote them with \u0026quot;\u0026quot; to escape blank spaces and use \u0026lsquo;\\\u0026rsquo; to escape special characters in statement. For example:\n--source-sql \u0026ldquo;CREATE TABLE T (k INT) WITH (\u0026lsquo;special-key\u0026rsquo; = \u0026lsquo;123\\!')\u0026rdquo;   For more information of \u0026lsquo;merge-into\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  merge-into --help  "});index.add({'id':29,'href':'/docs/0.4/engines/hive/','title':"Hive",'section':"Engines",'content':"Hive #  This documentation is a guide for using Paimon in Hive.\nVersion #  Paimon currently supports Hive 2.1, 2.1-cdh-6.3, 2.2, 2.3 and 3.1.\nExecution Engine #  Paimon currently supports MR and Tez execution engine for Hive.\nInstallation #  Download the jar file with corresponding version.\n    Jar     Hive 3.1 paimon-hive-connector-3.1-0.4.0-incubating.jar   Hive 2.3 paimon-hive-connector-2.3-0.4.0-incubating.jar   Hive 2.2 paimon-hive-connector-2.2-0.4.0-incubating.jar   Hive 2.1 paimon-hive-connector-2.1-0.4.0-incubating.jar   Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-0.4.0-incubating.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command. mvn clean install -DskipTests\nYou can find Hive connector jar in ./paimon-hive/paimon-hive-connector-\u0026lt;hive-version\u0026gt;/target/paimon-hive-connector-\u0026lt;hive-version\u0026gt;-0.4.0-incubating.jar.\nThere are several ways to add this jar to Hive.\n You can create an auxlib folder under the root directory of Hive, and copy paimon-hive-connector-0.4.0-incubating.jar into auxlib. You can also copy this jar to a path accessible by Hive, then use add jar /path/to/paimon-hive-connector-0.4.0-incubating.jar to enable paimon support in Hive. Note that this method is not recommended. If you\u0026rsquo;re using the MR execution engine and running a join statement, you may be faced with the exception org.apache.hive.com.esotericsoftware.kryo.kryoexception: unable to find class.  NOTE: If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.\nQuick Start with Paimon Hive Catalog #  By using paimon Hive catalog, you can create, drop and insert into paimon tables from Flink. These operations directly affect the corresponding Hive metastore. Tables created in this way can also be accessed directly from Hive.\nStep 1: Prepare Flink Hive Connector Bundled Jar\nSee creating a catalog with Hive metastore.\nStep 2: Create Test Data with Flink SQL\nExecute the following Flink SQL script in Flink SQL client to define a Paimon Hive catalog and create a table.\n-- Flink SQL CLI -- Define paimon Hive catalog  CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/path/to/table/store/warehouse\u0026#39; ); -- Use paimon Hive catalog  USE CATALOG my_hive; -- Create a table in paimon Hive catalog (use \u0026#34;default\u0026#34; database by default)  CREATE TABLE test_table ( a int, b string ); -- Insert records into test table  INSERT INTO test_table VALUES (1, \u0026#39;Table\u0026#39;), (2, \u0026#39;Store\u0026#39;); -- Read records from test table  SELECT * FROM test_table; /* +---+-------+ | a | b | +---+-------+ | 1 | Table | | 2 | Store | +---+-------+ */ Step 3: Query the Table in Hive\nRun the following Hive SQL in Hive CLI to access the created table.\n-- Assume that paimon-hive-connector-\u0026lt;hive-version\u0026gt;-0.4.0-incubating.jar is already in auxlib directory. -- List tables in Hive -- (you might need to switch to \u0026#34;default\u0026#34; database if you\u0026#39;re not there by default)  SHOW TABLES; /* OK test_table */ -- Read records from test_table  SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore */ Quick Start with External Table #  To access existing paimon table, you can also register them as external tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that paimon-hive-connector-0.4.0-incubating.jar is already in auxlib directory. -- Let\u0026#39;s use the test_table created in the above section. -- To create an external table, you don\u0026#39;t need to specify any column or table properties. -- Pointing the location to the path of table is enough.  CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; LOCATION \u0026#39;/path/to/table/store/warehouse/default.db/test_table\u0026#39;; -- Read records from external_test_table  SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore */ Hive Type Conversion #  This section lists all supported type conversion between Hive and Flink. All Hive\u0026rsquo;s data types are available in package org.apache.hadoop.hive.serde2.typeinfo.\n  Hive Data Type Paimon Data Type Atomic Type     StructTypeInfo RowType false   MapTypeInfo MapType false   ListTypeInfo ArrayType false   PrimitiveTypeInfo(\"boolean\") BooleanType true   PrimitiveTypeInfo(\"tinyint\") TinyIntType true   PrimitiveTypeInfo(\"smallint\") SmallIntType true   PrimitiveTypeInfo(\"int\") IntType true   PrimitiveTypeInfo(\"bigint\") BigIntType true   PrimitiveTypeInfo(\"float\") FloatType true   PrimitiveTypeInfo(\"double\") DoubleType true   BaseCharTypeInfo(\"char(%d)\") CharType(length) true   PrimitiveTypeInfo(\"string\") VarCharType(VarCharType.MAX_LENGTH) true   BaseCharTypeInfo(\"varchar(%d)\") VarCharType(length), length is less than VarCharType.MAX_LENGTH true   PrimitiveTypeInfo(\"date\") DateType true   TimestampType TimestampType true   DecimalTypeInfo(\"decimal(%d, %d)\") DecimalType(precision, scale) true   DecimalTypeInfo(\"binary\") VarBinaryType, BinaryType true    "});index.add({'id':30,'href':'/docs/0.4/maintenance/manage-partition/','title':"Manage Partition",'section':"Maintenance",'content':"Expiring Partitions #  You can set partition.expiration-time when creating a partitioned table. Paimon will periodically check the status of partitions and delete expired partitions according to time.\nHow to determine whether a partition has expired: compare the time extracted from the partition with the current time to see if survival time has exceeded the partition.expiration-time.\nAn example:\nCREATE TABLE T (...) PARTITIONED BY (dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39; ); More options:\n  Option Default Type Description     partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.    "});index.add({'id':31,'href':'/docs/0.4/engines/presto/','title':"Presto",'section':"Engines",'content':"This documentation is a guide for using Paimon in Presto.\nVersion #  Paimon currently supports Presto 0.236 and above.\nPreparing Paimon Jar File #  Download the jar file with corresponding version.\n   Version Jar     [0.236,0.268) paimon-presto-0.236-0.4.0-incubating.jar   [0.268,0.273) paimon-presto-0.268-0.4.0-incubating.jar   [0.273,0.279] paimon-presto-0.273-0.4.0-incubating.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests You can find Presto connector jar in ./paimon-presto/paimon-presto-\u0026lt;presto-version\u0026gt;/target/paimon-presto-*.jar.\nThen, copy paimon-presto-*.jar and flink-shaded-hadoop-*-uber-*.jar to plugin/paimon.\nConfigure Paimon Catalog #  Catalogs are registered by creating a catalog properties file in the etc/catalog directory. For example, create etc/catalog/paimon.properties with the following contents to mount the paimon connector as the paimon catalog:\nconnector.name=paimon warehouse=file:/tmp/warehouse If you are using HDFS, choose one of the following ways to configure your HDFS:\n set environment variable HADOOP_HOME. set environment variable HADOOP_CONF_DIR. configure fs.hdfs.hadoopconf in the properties.  You can configure kerberos keytag file when using KERBEROS authentication in the properties.\nsecurity.kerberos.login.principal=hadoop-user security.kerberos.login.keytab=/etc/presto/hdfs.keytab Query #  SELECT * FROM paimon.default.MyTable "});index.add({'id':32,'href':'/docs/0.4/how-to/querying-tables/','title':"Querying Tables",'section':"How to",'content':"Querying Tables #  Just like all other tables, Paimon tables can be queried with SELECT statement.\nScan Mode #  By specifying the scan.mode table property, users can specify where and how Paimon sources should produce records.\n  Scan Mode Batch Source Behavior Streaming Source Behavior     default The default scan mode. Determines actual scan mode according to other table properties. If \"scan.timestamp-millis\" is set the actual scan mode will be \"from-timestamp\", and if \"scan.snapshot-id\" is set the actual startup mode will be \"from-snapshot\". Otherwise the actual scan mode will be \"latest-full\".    latest-full  Produces the latest snapshot of table.   Produces the latest snapshot on the table upon first startup, and continues to read the following changes.    compacted-full  Produces the snapshot after the latest compaction.   Produces the snapshot after the latest compaction on the table upon first startup, and continues to read the following changes.    latest Same as \"latest-full\" Continuously reads latest changes without producing a snapshot at the beginning.   from-timestamp Produces a snapshot earlier than or equals to the timestamp specified by \"scan.timestamp-millis\". Continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning.   from-snapshot Produces a snapshot specified by \"scan.snapshot-id\". Continuously reads changes starting from a snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning.   from-snapshot-full Produces a snapshot specified by \"scan.snapshot-id\". Produces from snapshot specified by \"scan.snapshot-id\" on the table upon first startup, and continuously reads changes.    Users can also adjust changelog-producer table property to specify the pattern of produced changes. See changelog producer for details.\nTime Travel #  Currently, Paimon supports time travel for Flink and Spark 3 (requires Spark 3.3+).\nFlink  you can use dynamic table options to specify scan mode and from where to start:\n-- travel to snapshot with id 1L SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; -- travel to specified timestamp with a long value in milliseconds SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; Spark3 you can use VERSION AS OF and TIMESTAMP AS OF in query to do time travel:\n-- travel to snapshot with id 1L (use snapshot id as version) SELECT * FROM t VERSION AS OF 1; -- travel to specified timestamp SELECT * FROM t TIMESTAMP AS OF \u0026#39;2023-06-01 00:00:00.123\u0026#39;; -- you can also use a long value in seconds as timestamp SELECT * FROM t TIMESTAMP AS OF 1678883047;  Consumer ID #  This is an experimental feature.  You can specify the consumer-id when streaming read table:\nSELECT * FROM t /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;) */; When stream read Paimon tables, the next snapshot id to be recorded into the file system. This has several advantages:\n When previous job is stopped, the newly started job can continue to consume from the previous progress without resuming from the state. The newly reading will start reading from next snapshot id found in consumer files. When deciding whether a snapshot has expired, Paimon looks at all the consumers of the table in the file system, and if there are consumers that still depend on this snapshot, then this snapshot will not be deleted by expiration.  NOTE: If there is a consumer that will not be used anymore, please delete it, otherwise it will affect the expiration of the snapshot. The consumer file is in ${table-path}/consumer/consumer-${consumer-id}.  System Tables #  System tables contain metadata and information about each table, such as the snapshots created and the options in use. Users can access system tables with batch queries.\nCurrently, Flink, Spark and Trino supports querying system tables.\nIn some cases, the table name needs to be enclosed with back quotes to avoid syntax parsing conflicts, for example triple access mode:\nSELECT * FROM my_catalog.my_db.`MyTable$snapshots`; Snapshots Table #  You can query the snapshot history information of the table through snapshots table, including the record count occurred in the snapshot.\nSELECT * FROM MyTable$snapshots; /* +--------------+------------+-----------------+-------------------+--------------+-------------------------+---------------------+---------------------+-------------------------+ | snapshot_id | schema_id | commit_user | commit_identifier | commit_kind | commit_time | total_record_count | delta_record_count | changelog_record_count | +--------------+------------+-----------------+-------------------+--------------+-------------------------+---------------------+---------------------+-------------------------+ | 2 | 0 | 7ca4cd28-98e... | 2 | APPEND | 2022-10-26 11:44:15.600 | 2 | 2 | 0 | | 1 | 0 | 870062aa-3e9... | 1 | APPEND | 2022-10-26 11:44:15.148 | 1 | 1 | 0 | +--------------+------------+-----------------+-------------------+--------------+-------------------------+---------------------+---------------------+-------------------------+ 2 rows in set */ By querying the snapshots table, you can know the commit and expiration information about that table and time travel through the data.\nSchemas Table #  You can query the historical schemas of the table through schemas table.\nSELECT * FROM MyTable$schemas; /* +-----------+--------------------------------+----------------+--------------+---------+---------+ | schema_id | fields | partition_keys | primary_keys | options | comment | +-----------+--------------------------------+----------------+--------------+---------+---------+ | 0 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | | 1 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | | 2 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | +-----------+--------------------------------+----------------+--------------+---------+---------+ 3 rows in set */ You can join the snapshots table and schemas table to get the fields of given snapshots.\nSELECT s.snapshot_id, t.schema_id, t.fields FROM MyTable$snapshots s JOIN MyTable$schemas t ON s.schema_id=t.schema_id where s.snapshot_id=100; Options Table #  You can query the table\u0026rsquo;s option information which is specified from the DDL through options table. The options not shown will be the default value. You can take reference to [Configuration].\nSELECT * FROM MyTable$options; /* +------------------------+--------------------+ | key | value | +------------------------+--------------------+ | snapshot.time-retained | 5 h | +------------------------+--------------------+ 1 rows in set */ Audit log Table #  If you need to audit the changelog of the table, you can use the audit_log system table. Through audit_log table, you can get the rowkind column when you get the incremental data of the table. You can use this column for filtering and other operations to complete the audit.\nThere are four values for rowkind:\n +I: Insertion operation. -U: Update operation with the previous content of the updated row. +U: Update operation with new content of the updated row. -D: Deletion operation.  SELECT * FROM MyTable$audit_log; /* +------------------+-----------------+-----------------+ | rowkind | column_0 | column_1 | +------------------+-----------------+-----------------+ | +I | ... | ... | +------------------+-----------------+-----------------+ | -U | ... | ... | +------------------+-----------------+-----------------+ | +U | ... | ... | +------------------+-----------------+-----------------+ 3 rows in set */ Files Table #  You can query the files of the table with specific snapshot.\n-- Query the files of latest snapshot SELECT * FROM MyTable$files; +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ | [3] | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} |2023-02-24T16:06:21.166| | [2] | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} |2023-02-24T16:06:21.166| | [2] | 0 | data-83aa7973-060b-40b6-8c8... | orc | 0 | 0 | 1 | 605 | [d] | [d] | {cnt=0, val=0, word=0} | {cnt=2, val=32, word=d} | {cnt=2, val=32, word=d} |2023-02-24T16:06:21.166| | [5] | 0 | data-3d304f4a-bcea-44dc-a13... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=5, val=51, word=c} | {cnt=5, val=51, word=c} |2023-02-24T16:06:21.166| | [1] | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} |2023-02-24T16:06:21.166| | [4] | 0 | data-2c9b7095-65b7-4013-a7a... | orc | 0 | 0 | 1 | 593 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=4, val=12, word=a} | {cnt=4, val=12, word=a} |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ 6 rows in set -- You can also query the files with specific snapshot SELECT * FROM MyTable$files /*+ OPTIONS('scan.snapshot-id'='1') */; +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ | [3] | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} |2023-02-24T16:06:21.166| | [2] | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} |2023-02-24T16:06:21.166| | [1] | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ 3 rows in set "});index.add({'id':33,'href':'/docs/0.4/how-to/lookup-joins/','title':"Lookup Joins",'section':"How to",'content':"Lookup Joins #  Lookup Joins are a type of join in streaming queries. It is used to enrich a table with data that is queried from Paimon. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.\nPaimon supports lookup joins on tables with primary keys in Flink. The following example illustrates this feature.\nFirst, let\u0026rsquo;s create a Paimon table and update it in real-time.\n-- Create a paimon catalog CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;hdfs://nn:8020/warehouse/path\u0026#39; -- or \u0026#39;file://tmp/foo/bar\u0026#39; ); USE CATALOG my_catalog; -- Create a table in paimon catalog CREATE TABLE customers ( id INT PRIMARY KEY NOT ENFORCED, name STRING, country STRING, zip STRING ); -- Launch a streaming job to update customers table INSERT INTO customers ... -- Create a temporary left table, like from kafka CREATE TEMPORARY TABLE Orders ( order_id INT, total INT, customer_id INT, proc_time AS PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); You can now use customers in a lookup join query.\n-- enrich each order with customer information SELECT o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; The lookup join operator will maintain a RocksDB cache locally and pull the latest updates of the table in real time. Lookup join operator will only pull the necessary data, so your filter conditions are very important for performance.\nThis feature is only suitable for tables containing at most tens of millions of records to avoid excessive use of local disks.\nIf the records of Orders (main table) join missing because the corresponding data of customers (lookup table) is not ready. You can consider using Flink\u0026rsquo;s Delayed Retry Strategy For Lookup.  RocksDB Cache Options #  The following options allow users to finely adjust RocksDB for better performance. You can either specify them in table properties or in dynamic table hints.\n-- dynamic table hints example SELECT o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN customers /*+ OPTIONS(\u0026#39;lookup.cache-rows\u0026#39;=\u0026#39;20000\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id;    Key Default Type Description     lookup.cache-rows 10000 Long The maximum number of rows to store in the cache.   rocksdb.block.blocksize 4 kb MemorySize The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'.   rocksdb.block.cache-size 8 mb MemorySize The amount of the cache for data blocks in RocksDB. The default block-cache size is '8MB'.   rocksdb.block.metadata-blocksize 4 kb MemorySize Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'.   rocksdb.bloom-filter.bits-per-key 10.0 Double Bits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0.   rocksdb.bloom-filter.block-based-mode false Boolean If true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'.   rocksdb.compaction.level.max-size-level-base 256 mb MemorySize The upper-bound of the total size of level base files in bytes. The default value is '256MB'.   rocksdb.compaction.level.target-file-size-base 64 mb MemorySize The target file size for compaction, which determines a level-1 file size. The default value is '64MB'.   rocksdb.compaction.level.use-dynamic-size false Boolean If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc.   rocksdb.compaction.style LEVEL Enum\n The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.\nPossible values:\"LEVEL\"\"UNIVERSAL\"\"FIFO\"\"NONE\"   rocksdb.compression.type LZ4_COMPRESSION Enum\n The compression type.\nPossible values:\"NO_COMPRESSION\"\"SNAPPY_COMPRESSION\"\"ZLIB_COMPRESSION\"\"BZLIB2_COMPRESSION\"\"LZ4_COMPRESSION\"\"LZ4HC_COMPRESSION\"\"XPRESS_COMPRESSION\"\"ZSTD_COMPRESSION\"\"DISABLE_COMPRESSION_OPTION\"   rocksdb.files.open -1 Integer The maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'.   rocksdb.thread.num 2 Integer The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'.   rocksdb.use-bloom-filter false Boolean If true, every newly created SST file will contain a Bloom filter. It is disabled by default.   rocksdb.writebuffer.count 2 Integer The maximum number of write buffers that are built up in memory. The default value is '2'.   rocksdb.writebuffer.number-to-merge 1 Integer The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'.   rocksdb.writebuffer.size 64 mb MemorySize The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'.    "});index.add({'id':34,'href':'/docs/0.4/maintenance/','title':"Maintenance",'section':"Apache Paimon",'content':""});index.add({'id':35,'href':'/docs/0.4/concepts/primary-key-table/','title':"Primary Key Table",'section':"Concepts",'content':"Primary Key Table #  Changelog table is the default table type when creating a table. Users can insert, update or delete records in the table.\nPrimary keys are a set of columns that are unique for each record. Paimon imposes an ordering of data, which means the system will sort the primary key within each bucket. Using this feature, users can achieve high performance by adding filter conditions on the primary key.\nBy defining primary keys on a changelog table, users can access the following features.\nMerge Engines #  When Paimon sink receives two or more records with the same primary keys, it will merge them into one record to keep primary keys unique. By specifying the merge-engine table property, users can choose how records are merged together.\nSet table.exec.sink.upsert-materialize to NONE always in Flink SQL TableConfig, sink upsert-materialize may result in strange behavior. When the input is out of order, we recommend that you use Sequence Field to correct disorder.  Deduplicate #  deduplicate merge engine is the default merge engine. Paimon will only keep the latest record and throw away other records with the same primary keys.\nSpecifically, if the latest record is a DELETE record, all records with the same primary keys will be deleted.\nPartial Update #  By specifying 'merge-engine' = 'partial-update', users can set columns of a record across multiple updates and finally get a complete record. Specifically, value fields are updated to the latest data one by one under the same primary key, but null values are not overwritten.\nFor example, let\u0026rsquo;s say Paimon receives three records:\n \u0026lt;1, 23.0, 10, NULL\u0026gt;- \u0026lt;1, NULL, NULL, 'This is a book'\u0026gt; \u0026lt;1, 25.2, NULL, NULL\u0026gt;  If the first column is the primary key. The final result will be \u0026lt;1, 25.2, 10, 'This is a book'\u0026gt;.\nFor streaming queries, partial-update merge engine must be used together with lookup or full-compaction changelog producer.  Partial cannot receive DELETE messages because the behavior cannot be defined. You can configure partial-update.ignore-delete to ignore DELETE messages.  Aggregation #  NOTE: Set table.exec.sink.upsert-materialize to NONE always in Flink SQL TableConfig.  Sometimes users only care about aggregated results. The aggregation merge engine aggregates each value field with the latest data one by one under the same primary key according to the aggregate function.\nEach field not part of the primary keys can be given an aggregate function, specified by the fields.\u0026lt;field-name\u0026gt;.aggregate-function table property, otherwise it will use last_non_null_value aggregation as default. For example, consider the following table definition.\nFlink CREATE TABLE MyTable ( product_id BIGINT, price DOUBLE, sales BIGINT, PRIMARY KEY (product_id) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.price.aggregate-function\u0026#39; = \u0026#39;max\u0026#39;, \u0026#39;fields.sales.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; );  Field price will be aggregated by the max function, and field sales will be aggregated by the sum function. Given two input records \u0026lt;1, 23.0, 15\u0026gt; and \u0026lt;1, 30.2, 20\u0026gt;, the final result will be \u0026lt;1, 30.2, 35\u0026gt;.\nCurrent supported aggregate functions and data types are:\n sum: supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT and DOUBLE. min/max: support DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP and TIMESTAMP_LTZ. last_value / last_non_null_value: support all data types. listagg: supports STRING data type. bool_and / bool_or: support BOOLEAN data type.  Only sum supports retraction (UPDATE_BEFORE and DELETE), others aggregate functions do not support retraction. If you allow some functions to ignore retraction messages, you can configure: 'fields.${field_name}.ignore-retract'='true'.\nFor streaming queries, aggregation merge engine must be used together with lookup or full-compaction changelog producer.  Changelog Producers #  Streaming queries will continuously produce latest changes. These changes can come from the underlying table files or from an external log system like Kafka. Compared to the external log system, changes from table files have lower cost but higher latency (depending on how often snapshots are created).\nBy specifying the changelog-producer table property when creating the table, users can choose the pattern of changes produced from files.\nThe changelog-producer table property only affects changelog from files. It does not affect the external log system.  None #  By default, no extra changelog producer will be applied to the writer of table. Paimon source can only see the merged changes across snapshots, like what keys are removed and what are the new values of some keys.\nHowever, these merged changes cannot form a complete changelog, because we can\u0026rsquo;t read the old values of the keys directly from them. Merged changes require the consumers to \u0026ldquo;remember\u0026rdquo; the values of each key and to rewrite the values without seeing the old ones. Some consumers, however, need the old values to ensure correctness or efficiency.\nConsider a consumer which calculates the sum on some grouping keys (might not be equal to the primary keys). If the consumer only sees a new value 5, it cannot determine what values should be added to the summing result. For example, if the old value is 4, it should add 1 to the result. But if the old value is 6, it should in turn subtract 1 from the result. Old values are important for these types of consumers.\nTo conclude, none changelog producers are best suited for consumers such as a database system. Flink also has a built-in \u0026ldquo;normalize\u0026rdquo; operator which persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided.\nInput #  By specifying 'changelog-producer' = 'input', Paimon writers rely on their inputs as a source of complete changelog. All input records will be saved in separated changelog files and will be given to the consumers by Paimon sources.\ninput changelog producer can be used when Paimon writers' inputs are complete changelog, such as from a database CDC, or generated by Flink stateful computation.\nLookup #  This is an experimental feature.  If your input can’t produce a complete changelog but you still want to get rid of the costly normalized operator, you may consider using the 'lookup' changelog producer.\nBy specifying 'changelog-producer' = 'lookup', Paimon will generate changelog through 'lookup' before committing the data writing.\nLookup will cache data on the memory and local disk, you can use the following options to tune performance:\n  Option Default Type Description     lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size unlimited MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.    Full Compaction #  If you think the resource consumption of \u0026lsquo;lookup\u0026rsquo; is too large, you can consider using \u0026lsquo;full-compaction\u0026rsquo; changelog producer, which can decouple data writing and changelog generation, and is more suitable for scenarios with high latency (For example, 10 minutes).\nBy specifying 'changelog-producer' = 'full-compaction', Paimon will compare the results between full compactions and produce the differences as changelog. The latency of changelog is affected by the frequency of full compactions.\nBy specifying full-compaction.delta-commits table property, full compaction will be constantly triggered after delta commits (checkpoints). This is set to 1 by default, so each checkpoint will have a full compression and generate a change log.\nFull compaction changelog producer can produce complete changelog for any type of source. However it is not as efficient as the input changelog producer and the latency to produce changelog might be high.  Sequence Field #  By default, the primary key table determines the merge order according to the input order (the last input record will be the last to merge). However, in distributed computing, there will be some cases that lead to data disorder. At this time, you can use a time field as sequence.field, for example:\nWhen the record is updated or deleted, the sequence.field must become larger and cannot remain unchanged. For example, you can use Mysql Binlog operation time as sequence.field.  Flink CREATE TABLE MyTable ( pk BIGINT PRIMARY KEY NOT ENFORCED, v1 DOUBLE, v2 BIGINT, dt TIMESTAMP ) WITH ( \u0026#39;sequence.field\u0026#39; = \u0026#39;dt\u0026#39; );  The record with the largest sequence.field value will be the last to merge, regardless of the input order.\n"});index.add({'id':36,'href':'/docs/0.4/engines/trino/','title':"Trino",'section':"Engines",'content':"Trino #  Because Trino\u0026rsquo;s dependency is JDK 11, it is not possible to include the trino connector in paimon.\nSee paimon-trino.\n"});index.add({'id':37,'href':'/docs/0.4/api/','title':"API",'section':"Apache Paimon",'content':""});index.add({'id':38,'href':'/docs/0.4/concepts/append-only-table/','title':"Append Only Table",'section':"Concepts",'content':"Append Only Table #  If a table does not have a primary key defined, it is an append-only table by default.\nYou can only insert a complete record into the table. No delete or update is supported and you cannot define primary keys. This type of table is suitable for use cases that do not require updates (such as log data synchronization).\nBucketing #  You can also define bucket number for Append-only table, see Bucket.\nIt is recommended that you set the bucket-key field. Otherwise, the data will be hashed according to the whole row, and the performance will be poor.\nCompaction #  By default, the sink node will automatically perform compaction to control the number of files. The following options control the strategy of compaction:\n  Key Default Type Description     write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number which satisfies sum(size(f_i)) \u0026gt;= targetFileSize to trigger a compaction for append-only table. This value avoids almost-full-file to be compacted, which is not cost-effective.   compaction.max.file-num 50 Integer For file set [f_0,...,f_N], the maximum file number to trigger a compaction for append-only table, even if sum(size(f_i)) \u0026lt; targetFileSize. This value avoids pending too much small files, which slows down the performance.   full-compaction.delta-commits (none) Integer Full compaction will be constantly triggered after delta commits.    Streaming Source #  Streaming source behavior is only supported in Flink engine at present.\nStreaming Read Order #  For streaming reads, records are produced in the following order:\n For any two records from two different partitions  If scan.plan-sort-partition is set to true, the record with a smaller partition value will be produced first. Otherwise, the record with an earlier partition creation time will be produced first.   For any two records from the same partition and the same bucket, the first written record will be produced first. For any two records from the same partition but two different buckets, different buckets are processed by different tasks, there is no order guarantee between them.  Watermark Definition #  You can define watermark for reading Paimon tables:\nCREATE TABLE T ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (...); -- launch a bounded streaming job to read paimon_table SELECT window_start, window_end, COUNT(`user`) FROM TABLE( TUMBLE(TABLE T, DESCRIPTOR(order_time), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; You can also enable Flink Watermark alignment, which will make sure no sources/splits/shards/partitions increase their watermarks too far ahead of the rest:\n  Key Default Type Description     scan.watermark.alignment.group (none) String A group of sources to align watermarks.   scan.watermark.alignment.max-drift (none) Duration Maximal drift to align watermarks, before we pause consuming from the source/task/partition.    Bounded Stream #  Streaming Source can also be bounded, you can specify \u0026lsquo;scan.bounded.watermark\u0026rsquo; to define the end condition for bounded streaming mode, stream reading will end until a larger watermark snapshot is encountered.\nWatermark in snapshot is generated by writer, for example, you can specify a kafka source and declare the definition of watermark. When using this kafka source to write to Paimon table, the snapshots of Paimon table will generate the corresponding watermark, so that you can use the feature of bounded watermark when streaming reads of this Paimon table.\nCREATE TABLE kafka_table ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (\u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;...); -- launch a streaming insert job INSERT INTO paimon_table SELECT * FROM kakfa_table; -- launch a bounded streaming job to read paimon_table SELECT * FROM paimon_table /*+ OPTIONS(\u0026#39;scan.bounded.watermark\u0026#39;=\u0026#39;...\u0026#39;) */; Example #  The following is an example of creating the Append-Only table and specifying the bucket key.\nFlink CREATE TABLE MyTable ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;product_id\u0026#39; );  "});index.add({'id':39,'href':'/docs/0.4/how-to/cdc-ingestion/','title':"CDC Ingestion",'section':"How to",'content':"CDC Ingestion #  Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.\nMySQL #  Prepare CDC Bundled Jar #  flink-sql-connector-mysql-cdc-*.jar Synchronizing Tables #  By using MySqlSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from MySQL into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  mysql-sync-table --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition-keys \u0026lt;partition-keys\u0026gt;] \\  [--primary-keys \u0026lt;primary-keys\u0026gt;] \\  [--computed-column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed-column ...]] \\  [--mysql-conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql-conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition-keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary-keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --computed-column The definitions of computed columns. The argument field is from MySQL table field name. Supported expressions are: year(date-column): Extract year from a DATE, DATETIME or TIMESTAMP. Output is an INT value represent the year.substring(column,beginInclusive): Get column.substring(beginInclusive). Output is a STRING.substring(column,beginInclusive,endExclusive): Get column.substring(beginInclusive,endExclusive). Output is a STRING.truncate(column,width): truncate column by width. Output type is same with column.If the column is a STRING, truncate(column,width) will truncate the string to width characters, namely \"value.substring(0, width)\".If the column is an INT or LONG, truncate(column,width) will truncate the number with the algorithm \"v - (((v % W) + W) % W)\". The \"redundant\" compute part is to keep the result always positive.If the column is a DECIMAL, truncate(column,width) will truncate the decimal with the algorithm: let \"scaled_W = decimal(W, scale(v))\", then return \"v - (v % scaled_W)\".   --mysql-conf The configuration for Flink CDC MySQL table sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog-conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table-conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nThis action supports a limited number of schema changes. Currently, the framework can not drop columns, so the behaviors of DROP will be ignored, RENAME will add a new column. Currently supported schema changes includes:\n  Adding columns.\n  Altering column types. More specifically,\n altering from a string type (char, varchar, text) to another string type with longer length, altering from a binary type (binary, varbinary, blob) to another binary type with longer length, altering from an integer type (tinyint, smallint, int, bigint) to another integer type with wider range, altering from a floating-point type (float, double) to another floating-point type with wider range,  are supported.\n  Example\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  mysql-sync-table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition-keys pt \\  --primary-keys pt,uid \\  --computed-columns \u0026#39;_year=year(age)\u0026#39; \\  --mysql-conf hostname=127.0.0.1 \\  --mysql-conf username=root \\  --mysql-conf password=123456 \\  --mysql-conf database-name=source_db \\  --mysql-conf table-name=\u0026#39;source_table_.*\u0026#39; \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 Synchronizing Databases #  By using MySqlSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MySQL database into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  mysql-sync-database --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--ignore-incompatible \u0026lt;true/false\u0026gt;] \\  [--table-prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table-suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including-tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\  [--excluding-tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\  [--mysql-conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql-conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --ignore-incompatible It is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.   --table-prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table-prefix ods_\".   --table-suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table-prefix\".   --including-tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including-tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding-tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including-tables\". \"--excluding-tables\" has higher priority than \"--including-tables\" if you specified both.   --mysql-conf The configuration for Flink CDC MySQL table sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog-conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table-conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Only tables with primary keys will be synchronized.\nFor each MySQL table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nThis action supports a limited number of schema changes. Currently, the framework can not drop columns, so the behaviors of DROP will be ignored, RENAME will add a new column. Currently supported schema changes includes:\n  Adding columns.\n  Altering column types. More specifically,\n altering from a string type (char, varchar, text) to another string type with longer length, altering from a binary type (binary, varbinary, blob) to another binary type with longer length, altering from an integer type (tinyint, smallint, int, bigint) to another integer type with wider range, altering from a floating-point type (float, double) to another floating-point type with wider range,  are supported.\n  Example\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.4.0-incubating.jar \\  mysql-sync-database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql-conf hostname=127.0.0.1 \\  --mysql-conf username=root \\  --mysql-conf password=123456 \\  --mysql-conf database-name=source_db \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 "});index.add({'id':40,'href':'/docs/0.4/project/','title':"Project",'section':"Apache Paimon",'content':""});index.add({'id':41,'href':'/docs/0.4/concepts/external-log-systems/','title':"External Log Systems",'section':"Concepts",'content':"External Log Systems #  Aside from underlying table files, changelog of Paimon can also be stored into or consumed from an external log system, such as Kafka. By specifying log.system table property, users can choose which external log system to use.\nIf an external log system is used, all records written into table files will also be written into the log system. Changes produced by the streaming queries will thus come from the log system instead of table files.\nConsistency Guarantees #  By default, changes in the log systems are visible to consumers only after a snapshot, just like table files. This behavior guarantees the exactly-once semantics. That is, each record is seen by the consumers exactly once.\nHowever, users can also specify the table property 'log.consistency' = 'eventual' so that changelog written into the log system can be immediately consumed by the consumers, without waiting for the next snapshot. This behavior decreases the latency of changelog, but it can only guarantee the at-least-once semantics (that is, consumers might see duplicated records) due to possible failures.\nIf 'log.consistency' = 'eventual' is set, in order to achieve correct results, Paimon source in Flink will automatically adds a \u0026ldquo;normalize\u0026rdquo; operator for deduplication. This operator persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided.\nSupported Log Systems #  Kafka #  Preparing flink-sql-connector-kafka Jar File #  Paimon currently supports Flink 1.17, 1.16, 1.15 and 1.14. We recommend the latest Flink version for a better experience.\nDownload the flink-sql-connector-kafka jar file with corresponding version.\n   Version Jar     Flink 1.17 flink-sql-connector-kafka-1.17.0.jar   Flink 1.16 flink-sql-connector-kafka-1.16.1.jar   Flink 1.15 flink-sql-connector-kafka-1.15.4.jar   Flink 1.14 flink-sql-connector-kafka_2.11-1.14.4.jar    By specifying 'log.system' = 'kafka', users can write changes into Kafka along with table files.\nFlink CREATE TABLE T (...) WITH ( \u0026#39;log.system\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;kafka.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;kafka.topic\u0026#39; = \u0026#39;...\u0026#39; );  Table Properties for Kafka are listed as follows.\n  Key Default Type Description     kafka.bootstrap.servers (none) String Required Kafka server connection string.   kafka.topic (none) String Topic of this kafka table.    "});index.add({'id':42,'href':'/docs/0.4/maintenance/configurations/','title':"Configurations",'section':"Maintenance",'content':"Configuration #  CoreOptions #  Core options for paimon.\n  Key Default Type Description     auto-create false Boolean Whether to create underlying storage when reading and writing the table.   bucket 1 Integer Bucket number for file store.   bucket-key (none) String Specify the paimon distribution policy. Data is assigned to each bucket according to the hash value of bucket-key.\nIf you specify multiple fields, delimiter is ','.\nIf not specified, the primary key will be used; if there is no primary key, the full row will be used.   changelog-producer none Enum\n Whether to double write to a changelog file. This changelog file keeps the details of data changes, it can be read directly during stream reads.\nPossible values:\"none\": No changelog file.\"input\": Double write to a changelog file when flushing memory table, the changelog is from input.\"full-compaction\": Generate changelog files with each full compaction.\"lookup\": Generate changelog files through 'lookup' before committing the data writing.   commit.force-compact false Boolean Whether to force a compaction before commit.   compaction.max-size-amplification-percent 200 Integer The size amplification is defined as the amount (in percentage) of additional storage needed to store a single byte of data in the merge tree for changelog mode table.   compaction.max-sorted-run-num 2147483647 Integer The maximum sorted run number to pick for compaction. This value avoids merging too much sorted runs at the same time during compaction, which may lead to OutOfMemoryError.   compaction.max.file-num 50 Integer For file set [f_0,...,f_N], the maximum file number to trigger a compaction for append-only table, even if sum(size(f_i)) \u0026lt; targetFileSize. This value avoids pending too much small files, which slows down the performance.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number which satisfies sum(size(f_i)) \u0026gt;= targetFileSize to trigger a compaction for append-only table. This value avoids almost-full-file to be compacted, which is not cost-effective.   compaction.size-ratio 1 Integer Percentage flexibility while comparing sorted run size for changelog mode table. If the candidate sorted run(s) size is 1% smaller than the next sorted run's size, then include next sorted run into this candidate set.   consumer-id (none) String Consumer id for recording the offset of consumption in the storage.   continuous.discovery-interval 10 s Duration The discovery interval of continuous reading.   dynamic-partition-overwrite true Boolean Whether only overwrite dynamic partition when overwriting a partitioned table with dynamic partition columns. Works only when the table has partition keys.   file.compression (none) String Default file compression format, can be overridden by file.compression.per.level   file.compression.per.level  Map Define different compression policies for different level, you can add the conf like this: 'file.compression.per.level' = '0:lz4,1:zlib', for orc file format, the compression value could be NONE, ZLIB, SNAPPY, LZO, LZ4, for parquet file format, the compression value could be UNCOMPRESSED, SNAPPY, GZIP, LZO, BROTLI, LZ4, ZSTD.   file.format orc Enum\n Specify the message format of data files, currently orc, parquet and avro are supported.\nPossible values:\"orc\": ORC file format.\"parquet\": Parquet file format.\"avro\": Avro file format.   full-compaction.delta-commits (none) Integer Full compaction will be constantly triggered after delta commits.   local-sort.max-num-file-handles 128 Integer The maximal fan-in for external merge sort. It limits the number of file handles. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading.   log.changelog-mode auto Enum\n Specify the log changelog mode for table.\nPossible values:\"auto\": Upsert for table with primary key, all for table without primary key.\"all\": The log system stores all changes including UPDATE_BEFORE.\"upsert\": The log system does not store the UPDATE_BEFORE changes, the log consumed job will automatically add the normalized node, relying on the state to generate the required update_before.   log.consistency transactional Enum\n Specify the log consistency mode for table.\nPossible values:\"transactional\": Only the data after the checkpoint can be seen by readers, the latency depends on checkpoint interval.\"eventual\": Immediate data visibility, you may see some intermediate states, but eventually the right results will be produced, only works for table with primary key.   log.format \"debezium-json\" String Specify the message format of log system.   log.key.format \"json\" String Specify the key message format of log system with primary key.   log.scan.remove-normalize false Boolean Whether to force the removal of the normalize node when streaming read. Note: This is dangerous and is likely to cause data errors if downstream is used to calculate aggregation and the input is not complete changelog.   lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size 9223372036854775807 bytes MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.   lookup.hash-load-factor 0.75 Float The index load factor for lookup.   manifest.format avro Enum\n Specify the message format of manifest files.\nPossible values:\"orc\": ORC file format.\"parquet\": Parquet file format.\"avro\": Avro file format.   manifest.merge-min-count 30 Integer To avoid frequent manifest merges, this parameter specifies the minimum number of ManifestFileMeta to merge.   manifest.target-file-size 8 mb MemorySize Suggested file size of a manifest file.   merge-engine deduplicate Enum\n Specify the merge engine for table with primary key.\nPossible values:\"deduplicate\": De-duplicate and keep the last row.\"partial-update\": Partial update non-null fields.\"aggregation\": Aggregate fields with same primary key.   num-levels (none) Integer Total level number, for example, there are 3 levels, including 0,1,2 levels.   num-sorted-run.compaction-trigger 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).   num-sorted-run.stop-trigger (none) Integer The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 1.   orc.bloom.filter.columns (none) String A comma-separated list of columns for which to create a bloom filter when writing.   orc.bloom.filter.fpp 0.05 Double Define the default false positive probability for bloom filters.   page-size 64 kb MemorySize Memory page size.   partial-update.ignore-delete false Boolean Whether to ignore delete records in partial-update mode.   partition (none) String Define partition by table options, cannot define partition on DDL and table options at the same time.   partition.default-name \"__DEFAULT_PARTITION__\" String The default partition name in case the dynamic partition column value is null/empty string.   partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.   primary-key (none) String Define primary key by table options, cannot define primary key on DDL and table options at the same time.   read.batch-size 1024 Integer Read batch size for orc and parquet.   scan.bounded.watermark (none) Long End condition \"watermark\" for bounded streaming mode. Stream reading will end when a larger watermark snapshot is encountered.   scan.mode default Enum\n Specify the scanning behavior of the source.\nPossible values:\"default\": Determines actual startup mode according to other table properties. If \"scan.timestamp-millis\" is set the actual startup mode will be \"from-timestamp\", and if \"scan.snapshot-id\" is set the actual startup mode will be \"from-snapshot\". Otherwise the actual startup mode will be \"latest-full\".\"latest-full\": For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes. For batch sources, just produce the latest snapshot but does not read new changes.\"full\": Deprecated. Same as \"latest-full\".\"latest\": For streaming sources, continuously reads latest changes without producing a snapshot at the beginning. For batch sources, behaves the same as the \"latest-full\" startup mode.\"compacted-full\": For streaming sources, produces a snapshot after the latest compaction on the table upon first startup, and continue to read the latest changes. For batch sources, just produce a snapshot after the latest compaction but does not read new changes. Snapshots of full compaction are picked when scheduled full-compaction is enabled.\"from-timestamp\": For streaming sources, continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning. For batch sources, produces a snapshot at timestamp specified by \"scan.timestamp-millis\" but does not read new changes.\"from-snapshot\": For streaming sources, continuously reads changes starting from snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" but does not read new changes.\"from-snapshot-full\": For streaming sources, produces from snapshot specified by \"scan.snapshot-id\" on the table upon first startup, and continuously reads changes. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" but does not read new changes.   scan.plan-sort-partition false Boolean Whether to sort plan files by partition fields, this allows you to read according to the partition order, even if your partition writes are out of order.\nIt is recommended that you use this for streaming read of the 'append-only' table. By default, streaming read will read the full snapshot first. In order to avoid the disorder reading for partitions, you can open this option.   scan.snapshot-id (none) Long Optional snapshot id used in case of \"from-snapshot\" or \"from-snapshot-full\" scan mode   scan.timestamp-millis (none) Long Optional timestamp used in case of \"from-timestamp\" scan mode.   sequence.field (none) String The field that generates the sequence number for primary key table, the sequence number determines which data is the most recent.   snapshot.num-retained.max 2147483647 Integer The maximum number of completed snapshots to retain.   snapshot.num-retained.min 10 Integer The minimum number of completed snapshots to retain.   snapshot.time-retained 1 h Duration The maximum time of completed snapshots to retain.   source.split.open-file-cost 4 mb MemorySize Open file cost of a source file. It is used to avoid reading too many files with a source split, which can be very slow.   source.split.target-size 128 mb MemorySize Target size of a source split when scanning a bucket.   streaming-read-mode (none) Enum\n The mode of streaming read that specifies to read the data of table file or log\nPossible values:\nfile: Reads from the data of table file store.log: Read from the data of table log store.\nPossible values:\"log\": Reads from the log store.\"file\": Reads from the file store.   streaming-read-overwrite false Boolean Whether to read the changes from overwrite in streaming mode.   target-file-size 128 mb MemorySize Target size of a file.   write-buffer-size 256 mb MemorySize Amount of data to build up in memory before converting to a sorted on-disk file.   write-buffer-spillable (none) Boolean Whether the write buffer can be spillable. Enabled by default when using object storage.   write-manifest-cache 0 bytes MemorySize Cache size for reading manifest files for write initialization.   write-mode auto Enum\n Specify the write mode for table.\nPossible values:\"auto\": The change-log for table with primary key, append-only for table without primary key.\"append-only\": The table can only accept append-only insert operations. Neither data deduplication nor any primary key constraints will be done when inserting rows into paimon.\"change-log\": The table can accept insert/delete/update operations.   write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.    CatalogOptions #  Options for paimon catalog.\n  Key Default Type Description     fs.allow-hadoop-fallback true Boolean Allow to fallback to hadoop File IO when no file io found for the scheme.   lock-acquire-timeout 8 min Duration The maximum time to wait for acquiring the lock.   lock-check-max-sleep 8 s Duration The maximum sleep time when retrying to check the lock.   lock.enabled false Boolean Enable Catalog Lock.   metastore \"filesystem\" String Metastore of paimon catalog, supports filesystem and hive.   table.type managed Enum\n Type of table.\nPossible values:\"managed\": Paimon owned table where the entire lifecycle of the table data is managed.\"external\": The table where Paimon has loose coupling with the data stored in external locations.   uri (none) String Uri of metastore server.   warehouse (none) String The warehouse root path of catalog.    HiveCatalogOptions #  Options for Hive catalog.\n  Key Default Type Description     hadoop-conf-dir (none) String File directory of the core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml. Currently, only local file system paths are supported.   hive-conf-dir (none) String File directory of the hive-site.xml , used to create HiveMetastoreClient and security authentication, such as Kerberos, LDAP, Ranger and so on    FlinkConnectorOptions #  Flink connector options for paimon.\n  Key Default Type Description     changelog-producer.lookup-wait true Boolean When changelog-producer is set to LOOKUP, commit will wait for changelog generation by lookup.   log.system \"none\" String The log system used to keep changes of the table.\nPossible values:\n\"none\": No log system, the data is written only to file store, and the streaming read will be directly read from the file store.\"kafka\": Kafka log system, the data is double written to file store and kafka, and the streaming read will be read from kafka. If streaming read from file, configures streaming-read-mode to file.   scan.infer-parallelism false Boolean If it is false, parallelism of source are set by scan.parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).   scan.parallelism (none) Integer Define a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.   scan.split-enumerator.batch-size 10 Integer How many splits should assign to subtask per batch in StaticFileStoreSplitEnumerator to avoid exceed `akka.framesize` limit.   scan.watermark.alignment.group (none) String A group of sources to align watermarks.   scan.watermark.alignment.max-drift (none) Duration Maximal drift to align watermarks, before we pause consuming from the source/task/partition.   scan.watermark.alignment.update-interval 1 s Duration How often tasks should notify coordinator about the current watermark and how often the coordinator should announce the maximal aligned watermark.   scan.watermark.emit.strategy on-event Enum\n Emit strategy for watermark generation.\nPossible values:\"on-periodic\": Emit watermark periodically, interval is controlled by Flink 'pipeline.auto-watermark-interval'.\"on-event\": Emit watermark per record.   scan.watermark.idle-timeout (none) Duration If no records flow in a partition of a stream for that amount of time, then that partition is considered \"idle\" and will not hold back the progress of watermarks in downstream operators.   sink.parallelism (none) Integer Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.   streaming-read-atomic false Boolean The option to enable return per iterator instead of per record in streaming read.This can ensure that there will be no checkpoint segmentation in iterator consumption.\nBy default, streaming source checkpoint will be performed in any time, this means 'UPDATE_BEFORE' and 'UPDATE_AFTER' can be split into two checkpoint. Downstream can see intermediate state.    "});index.add({'id':43,'href':'/docs/0.4/versions/','title':"Versions",'section':"Apache Paimon",'content':"Versions #  An appendix of hosted documentation for all versions of Apache Paimon.\n master    stable    0.3    "});})();