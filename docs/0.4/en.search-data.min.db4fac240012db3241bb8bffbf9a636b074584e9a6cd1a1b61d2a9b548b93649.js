'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/master/concepts/','title':"Concepts",'section':"Apache Paimon",'content':""});index.add({'id':1,'href':'/docs/master/how-to/creating-catalogs/','title':"Creating Catalogs",'section':"How to",'content':"Creating Catalogs #  Paimon catalogs currently support two types of metastores:\n filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive.  See CatalogOptions for detailed options when creating a catalog.\nCreating a Catalog with Filesystem Metastore #  Flink The following Flink SQL registers and uses a Paimon catalog named my_catalog. Metadata and table files are stored under hdfs:///path/to/warehouse.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; You can define any default table options with the prefix table-default. for tables created in the catalog.\nSpark3 The following shell command registers a paimon catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default;  Creating a Catalog with Hive Metastore #  By using Paimon Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.\nTo use Hive catalog, Database name, Table name and Field names should be lower case.\nFlink Paimon Hive catalog in Flink relies on Flink Hive connector bundled jar. You should first download Flink Hive connector bundled jar and add it to classpath. See here for more info.\nThe following Flink SQL registers and uses a Paimon Hive catalog named my_hive. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nIf your Hive requires security authentication such as Kerberos, LDAP, Ranger or you want the paimon table to be managed by Apache Atlas(Setting \u0026lsquo;hive.metastore.event.listeners\u0026rsquo; in hive-site.xml). You can specify the hive-conf-dir and hadoop-conf-dir parameter to the hive-site.xml file path.\nCREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_hive; You can define any default table options with the prefix table-default. for tables created in the catalog.\nAlso, you can create FlinkGenericCatalog.\nSpark3 Your Spark installation should be able to detect, or already contains Hive dependencies. See here for more information.\nThe following shell command registers a Paimon Hive catalog named paimon. Metadata and table files are stored under hdfs:///path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=hdfs:///path/to/warehouse \\  --conf spark.sql.catalog.paimon.metastore=hive \\  --conf spark.sql.catalog.paimon.uri=thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt; You can define any default table options with the prefix spark.sql.catalog.paimon.table-default. for tables created in the catalog.\nAfter spark-sql is started, you can switch to the default database of the paimon catalog with the following SQL.\nUSE paimon.default; Also, you can create SparkGenericCatalog.\n  When using hive catalog to change incompatible column types through alter table, you need to configure hive.metastore.disallow.incompatible.col.type.changes=false. see HIVE-17832.\n  If you are using Hive3, please disable Hive ACID:\nhive.strict.managed.tables=false hive.create.as.insert.only=false metastore.create.as.acid=false  Setting Location in Properties #  If you are using an object storage , and you don\u0026rsquo;t want that the location of paimon table/database is accessed by the filesystem of hive, which may lead to the error such as \u0026ldquo;No FileSystem for scheme: s3a\u0026rdquo;. You can set location in the properties of table/database by the config of location-in-properties. See setting the location of table/database in properties \nSynchronizing Partitions into Hive Metastore #  By default, Paimon does not synchronize newly created partitions into Hive metastore. Users will see an unpartitioned table in Hive. Partition push-down will be carried out by filter push-down instead.\nIf you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Also see CoreOptions.\n"});index.add({'id':2,'href':'/docs/master/api/java-api/','title':"Java API",'section':"API",'content':"Java API #  Dependency #  Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-bundle\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.5-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Bundle. Paimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nCreate Catalog #  Before coming into contact with the Table, you need to create a Catalog.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.fs.Path; import org.apache.paimon.options.Options; public class CreateCatalog { public static void createFilesystemCatalog() { CatalogContext context = CatalogContext.create(new Path(\u0026#34;...\u0026#34;)); Catalog catalog = CatalogFactory.createCatalog(context); } public static void createHiveCatalog() { // Paimon Hive catalog relies on Hive jars  // You should add hive classpath or hive bundled jar.  Options options = new Options(); options.set(\u0026#34;warehouse\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;metastore\u0026#34;, \u0026#34;hive\u0026#34;); options.set(\u0026#34;uri\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;hive-conf-dir\u0026#34;, \u0026#34;...\u0026#34;); options.set(\u0026#34;hadoop-conf-dir\u0026#34;, \u0026#34;...\u0026#34;); CatalogContext context = CatalogContext.create(options); Catalog catalog = CatalogFactory.createCatalog(context); } } Create Database #  You can use the catalog to create databases. The created databases are persistence in the file system.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.fs.Path; public class CreateDatabase { public static void main(String[] args) { try { catalog.createDatabase(\u0026#34;my_db\u0026#34;, false); } catch (Catalog.DatabaseAlreadyExistException e) { // do something  } } } Determine Whether Database Exists #  You can use the catalog to determine whether the database exists\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.fs.Path; public class DatabaseExists { public static void main(String[] args) { boolean exists = catalog.databaseExists(\u0026#34;my_db\u0026#34;); } } List Databases #  You can use the catalog to list databases.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.fs.Path; public class ListDatabases { public static void main(String[] args) { List\u0026lt;String\u0026gt; databases = catalog.listDatabases(); } } Drop Database #  You can use the catalog to drop databases.\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.fs.Path; public class DropDatabase { public static void main(String[] args) { try { catalog.dropDatabase(\u0026#34;my_db\u0026#34;, false, true); } catch (Catalog.DatabaseNotEmptyException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Create Table #  You can use the catalog to create tables. The created tables are persistence in the file system. Next time you can directly obtain these tables.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.Schema; import org.apache.paimon.types.DataTypes; public class CreateTable { public static void main(String[] args) { Schema.Builder schemaBuilder = Schema.newBuilder(); schemaBuilder.primaryKey(\u0026#34;...\u0026#34;); schemaBuilder.partitionKeys(\u0026#34;...\u0026#34;); schemaBuilder.column(\u0026#34;f0\u0026#34;, DataTypes.INT()); schemaBuilder.column(\u0026#34;f1\u0026#34;, DataTypes.STRING()); Schema schema = schemaBuilder.build(); Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { catalog.createTable(identifier, schema, false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Get Table #  The Table interface provides access to the table metadata and tools to read and write table.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class GetTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { Table table = catalog.getTable(identifier); } catch (Catalog.TableNotExistException e) { // do something  } } } Determine Whether Table Exists #  You can use the catalog to determine whether the table exists\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class TableExists { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); boolean exists = catalog.tableExists(identifier); } } List Tables #  You can use the catalog to list tables.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; public class ListTables { public static void main(String[] args) { try { catalog.listTables(\u0026#34;my_db\u0026#34;); } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Drop Table #  You can use the catalog to drop table.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class DropTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); try { catalog.dropTable(identifier, false); } catch (Catalog.TableNotExistException e) { // do something  } } } Rename Table #  You can use the catalog to rename a table.\nimport org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; public class RenameTable { public static void main(String[] args) { Identifier fromTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Identifier toTableIdentifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;test_table\u0026#34;); try { catalog.renameTable(fromTableIdentifier, toTableIdentifier, false); } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.TableNotExistException e) { // do something  } } } Alter Table #  You can use the catalog to alter a table, but you need to pay attention to the following points.\n Add column cannot specify NOT NULL. Cannot update partition column type in the table. Cannot change nullability of primary key. If the type of the column is nested row type, update the column type is not supported. Update column to nested row type is not supported.  import org.apache.paimon.fs.Path; import org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.schema.SchemaChange; import org.apache.paimon.table.Table; import org.apache.paimon.types.DataField; import org.apache.paimon.types.DataTypes; import com.google.common.collect.Lists; import java.util.Arrays; public class AlterTable { public static void main(String[] args) { Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;my_table\u0026#34;); Map\u0026lt;String,String\u0026gt; options = new HashMap\u0026lt;\u0026gt;(); options.put(\u0026#34;bucket\u0026#34;, \u0026#34;4\u0026#34;); options.put(\u0026#34;compaction.max.file-num\u0026#34;, \u0026#34;40\u0026#34;); catalog.createTable( identifier, new Schema( Lists.newArrayList( new DataField(0, \u0026#34;col1\u0026#34;, DataTypes.STRING(), \u0026#34;field1\u0026#34;), new DataField(1, \u0026#34;col2\u0026#34;, DataTypes.STRING(), \u0026#34;field2\u0026#34;), new DataField(2, \u0026#34;col3\u0026#34;, DataTypes.STRING(), \u0026#34;field3\u0026#34;), new DataField(3, \u0026#34;col4\u0026#34;, DataTypes.BIGINT(), \u0026#34;field4\u0026#34;), new DataField( 4, \u0026#34;col5\u0026#34;, DataTypes.ROW( new DataField(5, \u0026#34;f1\u0026#34;, DataTypes.STRING(), \u0026#34;f1\u0026#34;), new DataField(6, \u0026#34;f2\u0026#34;, DataTypes.STRING(), \u0026#34;f2\u0026#34;), new DataField(7, \u0026#34;f3\u0026#34;, DataTypes.STRING(), \u0026#34;f3\u0026#34;)), \u0026#34;field5\u0026#34;), new DataField(8, \u0026#34;col6\u0026#34;, DataTypes.STRING(), \u0026#34;field6\u0026#34;)), Lists.newArrayList(\u0026#34;col1\u0026#34;), // partition keys  Lists.newArrayList(\u0026#34;col1\u0026#34;, \u0026#34;col2\u0026#34;), //primary key  options, \u0026#34;table comment\u0026#34;), false); // add option  SchemaChange addOption = SchemaChange.setOption(\u0026#34;snapshot.time-retained\u0026#34;, \u0026#34;2h\u0026#34;); // remove option  SchemaChange removeOption = SchemaChange.removeOption(\u0026#34;compaction.max.file-num\u0026#34;); // add column  SchemaChange addColumn = SchemaChange.addColumn(\u0026#34;col1_after\u0026#34;, DataTypes.STRING()); // add a column after col1  SchemaChange.Move after = SchemaChange.Move.after(\u0026#34;col1_after\u0026#34;, \u0026#34;col1\u0026#34;); SchemaChange addColumnAfterField = SchemaChange.addColumn(\u0026#34;col7\u0026#34;, DataTypes.STRING(), \u0026#34;\u0026#34;, after); // rename column  SchemaChange renameColumn = SchemaChange.renameColumn(\u0026#34;col3\u0026#34;, \u0026#34;col3_new_name\u0026#34;); // drop column  SchemaChange dropColumn = SchemaChange.dropColumn(\u0026#34;col6\u0026#34;); // update column comment  SchemaChange updateColumnComment = SchemaChange.updateColumnComment(new String[]{\u0026#34;col4\u0026#34;}, \u0026#34;col4 field\u0026#34;); // update nested column comment  SchemaChange updateNestedColumnComment = SchemaChange.updateColumnComment(new String[]{\u0026#34;col5\u0026#34;, \u0026#34;f1\u0026#34;}, \u0026#34;col5 f1 field\u0026#34;); // update column type  SchemaChange updateColumnType = SchemaChange.updateColumnType(\u0026#34;col4\u0026#34;, DataTypes.DOUBLE()); // update column position, you need to pass in a parameter of type Move  SchemaChange updateColumnPosition = SchemaChange.updateColumnPosition(SchemaChange.Move.first(\u0026#34;col4\u0026#34;)); // update column nullability  SchemaChange updateColumnNullability = SchemaChange.updateColumnNullability(new String[]{\u0026#34;col4\u0026#34;}, false); // update nested column nullability  SchemaChange updateNestedColumnNullability = SchemaChange.updateColumnNullability(new String[]{\u0026#34;col5\u0026#34;, \u0026#34;f2\u0026#34;}, false); SchemaChange[] schemaChanges = new SchemaChange[] { addOption, removeOption, addColumn, addColumnAfterField, renameColumn, dropColumn, updateColumnComment, updateNestedColumnComment, updateColumnType, updateColumnPosition, updateColumnNullability, updateNestedColumnNullability}; try { catalog.alterTable(identifier, Arrays.asList(schemaChanges), false); } catch (Catalog.TableNotExistException e) { // do something  } catch (Catalog.TableAlreadyExistException e) { // do something  } catch (Catalog.DatabaseNotExistException e) { // do something  } } } Table metadata:\n name return a name string to identify this table. rowType return the current row type of this table containing a sequence of table\u0026rsquo;s fields. partitionKeys returns the partition keys of this table. parimaryKeys returns the primary keys of this table. options returns the configuration of this table in a map of key-value. comment returns the optional comment of this table. copy return a new table by applying dynamic options to this table.  Batch Read #  For relatively small amounts of data, or for data that has undergone projection and filtering, you can directly use a standalone program to read the table data.\nBut if the data volume of the table is relatively large, you can distribute splits to different tasks for reading.\nThe reading is divided into two stages:\n Scan Plan: Generate plan splits in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;). Read Split: Read split in distributed tasks.  import org.apache.paimon.data.InternalRow; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.TableRead; import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; import org.apache.paimon.types.DataTypes; import com.google.common.collect.Lists; import java.io.IOException; import java.util.List; public class ReadTable { public static void main(String[] args) { // 1. Create a ReadBuilder and push filter (`withFilter`)  // and projection (`withProjection`) if necessary  // [{\u0026#34;Alice\u0026#34;, 12},{\u0026#34;Bob\u0026#34;, 5},{\u0026#34;Emily\u0026#34;, 18}]  PredicateBuilder builder = new PredicateBuilder(RowType.of(DataTypes.STRING(),DataTypes.INT())); Predicate notNull = builder.isNotNull(0); Predicate greaterOrEqual = builder.greaterOrEqual(1, 12); ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(Lists.newArrayList(notNull, greaterOrEqual)); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;)  List\u0026lt;Split\u0026gt; splits = readBuilder.newScan().plan().splits(); // 3. Distribute these splits to different tasks  // 4. Read a split in task  TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(ReadTable::readRow); } } Batch Write #  The writing is divided into two stages:\n Write records: Write records in distributed tasks, generate commit messages. Commit/Abort: Collect all CommitMessages, commit them in a global node (\u0026lsquo;Coordinator\u0026rsquo;, or named \u0026lsquo;Driver\u0026rsquo;, or named \u0026lsquo;Committer\u0026rsquo;). When the commit fails for certain reason, abort unsuccessful commit via CommitMessages.  import java.util.List; import org.apache.paimon.table.sink.BatchTableCommit; import org.apache.paimon.table.sink.BatchTableWrite; import org.apache.paimon.table.sink.BatchWriteBuilder; import org.apache.paimon.table.sink.CommitMessage; import org.apache.paimon.data.BinaryString; import org.apache.paimon.data.GenericRow; public class WriteTable { public static void main(String[] args) { // 1. Create a WriteBuilder (Serializable)  BatchWriteBuilder writeBuilder = table.newBatchWriteBuilder() .withOverwrite(staticPartition); // 2. Write records in distributed tasks  BatchTableWrite write = writeBuilder.newWrite(); GenericRow record1 = GenericRow.of(BinaryString.fromString(\u0026#34;Alice\u0026#34;), 12); GenericRow record2 = GenericRow.of(BinaryString.fromString(\u0026#34;Bob\u0026#34;), 5); GenericRow record3 = GenericRow.of(BinaryString.fromString(\u0026#34;Emily\u0026#34;), 18); write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit(); // 3. Collect all CommitMessages to a global node and commit  BatchTableCommit commit = writeBuilder.newCommit(); commit.commit(messages); // Abort unsuccessful commit to delete data files  // commit.abort(messages);  } } Stream Read #  The difference of Stream Read is that StreamTableScan can continuously scan and generate splits.\nStreamTableScan provides the ability to checkpoint and restore, which can let you save the correct state during stream reading.\nimport java.io.IOException; import java.util.List; import org.apache.paimon.data.InternalRow; import org.apache.paimon.reader.RecordReader; import org.apache.paimon.table.source.ReadBuilder; import org.apache.paimon.table.source.Split; import org.apache.paimon.table.source.StreamTableScan; import org.apache.paimon.table.source.TableRead; public class StreamReadTable { public static void main(String[] args) throws IOException { // 1. Create a ReadBuilder and push filter (`withFilter`)  // and projection (`withProjection`) if necessary  ReadBuilder readBuilder = table.newReadBuilder() .withProjection(projection) .withFilter(filter); // 2. Plan splits in \u0026#39;Coordinator\u0026#39; (or named \u0026#39;Driver\u0026#39;)  StreamTableScan scan = readBuilder.newStreamScan(); while (true) { List\u0026lt;Split\u0026gt; splits = scan.plan().splits(); // Distribute these splits to different tasks  Long state = scan.checkpoint(); // can be restored in scan.restore(state) after failover  } // 3. Read a split in task  TableRead read = readBuilder.newRead(); RecordReader\u0026lt;InternalRow\u0026gt; reader = read.createReader(splits); reader.forEachRemaining(row -\u0026gt; System.out.println(row)); } } Stream Write #  The difference of Stream Write is that StreamTableCommit can continuously commit.\nKey points to achieve exactly-once consistency:\n CommitUser represents a user. A user can commit multiple times. In distributed processing, you are expected to use the same commitUser. Different applications need to use different commitUsers. The commitIdentifier of StreamTableWrite and StreamTableCommit needs to be consistent, and the id needs to be incremented for the next committing. When a failure occurs, if you still have uncommitted CommitMessages, please use StreamTableCommit#filterAndCommit to exclude the committed messages by commitIdentifier.  import java.util.List; import org.apache.paimon.table.sink.CommitMessage; import org.apache.paimon.table.sink.StreamTableCommit; import org.apache.paimon.table.sink.StreamTableWrite; import org.apache.paimon.table.sink.StreamWriteBuilder; public class StreamWriteTable { public static void main(String[] args) throws Exception { // 1. Create a WriteBuilder (Serializable)  StreamWriteBuilder writeBuilder = table.newStreamWriteBuilder(); // 2. Write records in distributed tasks  StreamTableWrite write = writeBuilder.newWrite(); // commitIdentifier like Flink checkpointId  long commitIdentifier = 0; while (true) { write.write(record1); write.write(record2); write.write(record3); List\u0026lt;CommitMessage\u0026gt; messages = write.prepareCommit( false, commitIdentifier); commitIdentifier++; } // 3. Collect all CommitMessages to a global node and commit  StreamTableCommit commit = writeBuilder.newCommit(); commit.commit(commitIdentifier, messages); // 4. When failure occurs and you\u0026#39;re not sure if the commit process is successful,  // you can use `filterAndCommit` to retry the commit process.  // Succeeded commits will be automatically skipped.  /* Map\u0026lt;Long, List\u0026lt;CommitMessage\u0026gt;\u0026gt; commitIdentifiersAndMessages = new HashMap\u0026lt;\u0026gt;(); commitIdentifiersAndMessages.put(commitIdentifier, messages); commit.filterAndCommit(commitIdentifiersAndMessages); */ } } Data Types #     Java Paimon     boolean boolean   byte byte   short short   int int   long long   float float   double double   string org.apache.paimon.data.BinaryString   decimal org.apache.paimon.data.Decimal   timestamp org.apache.paimon.data.Timestamp   byte[] byte[]   array org.apache.paimon.data.InternalArray   map org.apache.paimon.data.InternalMap   InternalRow org.apache.paimon.data.InternalRow    Predicate Types #     SQL Predicate Paimon Predicate     and org.apache.paimon.predicate.PredicateBuilder.And   or org.apache.paimon.predicate.PredicateBuilder.Or   is null org.apache.paimon.predicate.PredicateBuilder.IsNull   is not null org.apache.paimon.predicate.PredicateBuilder.IsNotNull   in org.apache.paimon.predicate.PredicateBuildere.In   not in org.apache.paimon.predicate.PredicateBuilder.NotIn   = org.apache.paimon.predicate.PredicateBuilder.Equal   \u0026lt;\u0026gt; org.apache.paimon.predicate.PredicateBuilder.NotEqual   \u0026lt; org.apache.paimon.predicate.PredicateBuilder.LessThan   \u0026lt;= org.apache.paimon.predicate.PredicateBuilder.LessOrEqual   \u0026gt; org.apache.paimon.predicate.PredicateBuilder.GreaterThan   \u0026gt;= org.apache.paimon.predicate.PredicateBuilder.GreaterOrEqual    "});index.add({'id':3,'href':'/docs/master/concepts/overview/','title':"Overview",'section':"Concepts",'content':"Overview #  Apache Paimon(incubating) is a streaming data lake platform that supports high-speed data ingestion, change data tracking and efficient real-time analytics.\nArchitecture #  As shown in the architecture above:\nRead/Write: Paimon supports a versatile way to read/write data and perform OLAP queries.\n For reads, it supports consuming data  from historical snapshots (in batch mode), from the latest offset (in streaming mode), or reading incremental snapshots in a hybrid way.   For writes, it supports streaming synchronization from the changelog of databases (CDC) or batch insert/overwrite from offline data.  Ecosystem: In addition to Apache Flink, Paimon also supports read by other computation engines like Apache Hive, Apache Spark and Trino.\nInternal: Under the hood, Paimon stores the columnar files on the filesystem/object-store and uses the LSM tree structure to support a large volume of data updates and high-performance queries.\nUnified Storage #  For streaming engines like Apache Flink, there are typically three types of connectors:\n Message queue, such as Apache Kafka, it is used in both source and intermediate stages in this pipeline, to guarantee the latency stay within seconds. OLAP system, such as ClickHouse, it receives processed data in streaming fashion and serving user’s ad-hoc queries. Batch storage, such as Apache Hive, it supports various operations of the traditional batch processing, including INSERT OVERWRITE.  Paimon provides table abstraction. It is used in a way that does not differ from the traditional database:\n In batch execution mode, it acts like a Hive table and supports various operations of Batch SQL. Query it to see the latest snapshot. In streaming execution mode, it acts like a message queue. Query it acts like querying a stream changelog from a message queue where historical data never expires.  "});index.add({'id':4,'href':'/docs/master/engines/overview/','title':"Overview",'section':"Engines",'content':"Overview #  Paimon not only supports Flink SQL writes and queries natively, but also provides queries from other popular engines, such as Apache Spark and Apache Hive.\nCompatibility Matrix #     Engine Version Batch Read Batch Write Create Table Alter Table Streaming Write Streaming Read Batch Overwrite     Flink 1.14 - 1.17 ✅ ✅ ✅ ✅(1.17+) ✅ ✅ ✅   Spark 3.1 - 3.4 ✅ ✅ ✅ ✅ ❌ ❌ ❌   Hive 2.1 - 3.1 ✅ ✅ ✅ ❌ ❌ ❌ ❌   Spark 2.4 ✅ ❌ ❌ ❌ ❌ ❌ ❌   Trino 358 - 422 ✅ ❌ ✅ ✅ ❌ ❌ ❌   Presto 0.236 - 0.280 ✅ ❌ ✅ ✅ ❌ ❌ ❌    Ongoing engines:\n Doris: Under development, Support Paimon catalog, Doris Roadmap 2023. Seatunnel: Under development, Introduce paimon connector. Starrocks: Under discussion  Download Link\n"});index.add({'id':5,'href':'/docs/master/filesystems/overview/','title':"Overview",'section':"Filesystems",'content':"Overview #  Apache Paimon utilizes the same pluggable file systems as Apache Flink. Users can follow the standard plugin mechanism to configure the plugin structure if using Flink as compute engine. However, for other engines like Spark or Hive, the provided opt jars (by Flink) may get conflicts and cannot be used directly. It is not convenient for users to fix class conflicts, thus Paimon provides the self-contained and engine-unified FileSystem pluggable jars for user to query tables from Spark/Hive side.\nSupported FileSystems #     FileSystem URI Scheme Pluggable Description     Local File System file:// N Built-in Support   HDFS hdfs:// N Built-in Support, ensure that the cluster is in the hadoop environment   Aliyun OSS oss:// Y    S3 s3:// Y     Dependency #  We recommend you to download the jar directly: Download Link.\nYou can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild shaded jar with the following command.\nmvn clean install -DskipTests You can find the shaded jars under ./paimon-filesystems/paimon-${fs}/target/paimon-${fs}-0.5-SNAPSHOT.jar.\n"});index.add({'id':6,'href':'/docs/master/project/roadmap/','title':"Roadmap",'section':"Project",'content':"Roadmap #  Paimon\u0026rsquo;s long-term goal is to become the better data lake platform for building the Streaming Lakehouse. Paimon will invest in real-time, ecology, and data warehouse integrity for a long time.\nIf you have other requirements, please contact us.\nWhat’s Next? #  Core #   Foreign Key Join Partial-Update Append-only table supports batch / streaming z-order sort Supports cross partition update  Compute Engines #   More management via Flink/Spark CALL procedures Flink Sink supports at-least-once / unaligned checkpoint Flink Whole database compaction Job Public Spark Schema Evolution Pipeline Spark supports Dynamic Partition overwrite  "});index.add({'id':7,'href':'/docs/master/maintenance/write-performance/','title':"Write Performance",'section':"Maintenance",'content':"Write Performance #  Paimon\u0026rsquo;s write performance is closely related to checkpoint, so if you need greater write throughput:\n Flink Configuration ('flink-conf.yaml' or SET in SQL): Increase the checkpoint interval ('execution.checkpointing.interval'), increase max concurrent checkpoints to 3 ('execution.checkpointing.max-concurrent-checkpoints'), or just use batch mode. Increase write-buffer-size. Enable write-buffer-spillable. Rescale bucket number if you are using Fixed-Bucket mode.  Option 'changelog-producer' = 'lookup' or 'full-compaction', and option 'full-compaction.delta-commits' have a large impact on write performance, if it is a snapshot / full synchronization phase you can unset these options and then enable them again in the incremental phase.\nParallelism #  It is recommended that the parallelism of sink should be less than or equal to the number of buckets, preferably equal. You can control the parallelism of the sink with the sink.parallelism table property.\n  Option Required Default Type Description     sink.parallelism No (none) Integer Defines the parallelism of the sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.    Compaction #  Asynchronous Compaction #  Compaction is inherently asynchronous, but if you want it to be completely asynchronous and not blocking writing, expect a mode to have maximum write throughput, the compaction can be done slowly and not in a hurry. You can use the following strategies for your table:\nnum-sorted-run.stop-trigger = 2147483647 sort-spill-threshold = 10 This configuration will generate more files during peak write periods and gradually merge into optimal read performance during low write periods.\nNumber of Sorted Runs to Pause Writing #  When the number of sorted runs is small, Paimon writers will perform compaction asynchronously in separated threads, so records can be continuously written into the table. However, to avoid unbounded growth of sorted runs, writers will pause writing when the number of sorted runs hits the threshold. The following table property determines the threshold.\n  Option Required Default Type Description     num-sorted-run.stop-trigger No (none) Integer The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 1.    Write stalls will become less frequent when num-sorted-run.stop-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. If you are concerned about the OOM problem, please configure the following option. Its value depends on your memory size.\n  Option Required Default Type Description     sort-spill-threshold No (none) Integer If the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.    Number of Sorted Runs to Trigger Compaction #  Paimon uses LSM tree which supports a large number of updates. LSM organizes files in several sorted runs. When querying records from an LSM tree, all sorted runs must be combined to produce a complete view of all records.\nOne can easily see that too many sorted runs will result in poor query performance. To keep the number of sorted runs in a reasonable range, Paimon writers will automatically perform compactions. The following table property determines the minimum number of sorted runs to trigger a compaction.\n  Option Required Default Type Description     num-sorted-run.compaction-trigger No 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).    Compaction will become less frequent when num-sorted-run.compaction-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance.\nLocal Merging #  If your job suffers from primary key data skew (for example, you want to count the number of views for each page in a website, and some particular pages are very popular among the users), you can set 'local-merge-buffer-size' so that input records will be buffered and merged before they\u0026rsquo;re shuffled by bucket and written into sink. This is particularly useful when the same primary key is updated frequently between snapshots.\nThe buffer will be flushed when it is full. We recommend starting with 64 mb when you are faced with data skew but don\u0026rsquo;t know where to start adjusting buffer size.\n(Currently, Local merging not works for CDC ingestion)\nFile Format #  If you want to achieve ultimate compaction performance, you can consider using row storage file format AVRO.\n The advantage is that you can achieve high write throughput and compaction performance. The disadvantage is that your analysis queries will be slow, and the biggest problem with row storage is that it does not have the query projection. For example, if the table have 100 columns but only query a few columns, the IO of row storage cannot be ignored. Additionally, compression efficiency will decrease and storage costs will increase.  This a tradeoff.\nEnable row storage through the following options:\nfile.format = avro metadata.stats-mode = none The collection of statistical information for row storage is a bit expensive, so I suggest turning off statistical information as well.\nIf you don\u0026rsquo;t want to modify all files to Avro format, at least you can consider modifying the files in the previous layers to Avro format. You can use 'file.format.per.level' = '0:avro,1:avro' to specify the files in the first two layers to be in Avro format.\nFile Compression #  By default, Paimon uses high-performance compression algorithms such as LZ4 and SNAPPY, but their compression rates are not so good. If you want to reduce the write/read performance, you can modify the compression algorithm:\n 'file.compression': Default file compression format. If you need a higher compression rate, I recommend using 'ZSTD'. 'file.compression.per.level': Define different compression policies for different level. For example '0:lz4,1:zstd'.  Stability #  If there are too few buckets or resources, full-compaction may cause the checkpoint timeout, Flink\u0026rsquo;s default checkpoint timeout is 10 minutes.\nIf you expect stability even in this case, you can turn up the checkpoint timeout, for example:\nexecution.checkpointing.timeout = 60 min Write Initialize #  In the initialization of write, the writer of the bucket needs to read all historical files. If there is a bottleneck here (For example, writing a large number of partitions simultaneously), you can use write-manifest-cache to cache the read manifest data to accelerate initialization.\nMemory #  There are three main places in Paimon writer that takes up memory:\n Writer\u0026rsquo;s memory buffer, shared and preempted by all writers of a single task. This memory value can be adjusted by the write-buffer-size table property. Memory consumed when merging several sorted runs for compaction. Can be adjusted by the num-sorted-run.compaction-trigger option to change the number of sorted runs to be merged. If the row is very large, reading too many lines of data at once will consume a lot of memory when making a compaction. Reducing the read.batch-size option can alleviate the impact of this case. The memory consumed by writing columnar (ORC, Parquet, etc.) file. Decreasing the orc.write.batch-size option can reduce the consumption of memory for ORC format. If files are automatically compaction in the write task, dictionaries for certain large columns can significantly consume memory during compaction.  To disable dictionary encoding for all fields in Parquet format, set 'parquet.enable.dictionary'= 'false'. To disable dictionary encoding for all fields in ORC format, set orc.dictionary.key.threshold='0'. Additionally,set orc.column.encoding.direct='field1,field2' to disable dictionary encoding for specific columns.    If your Flink job does not rely on state, please avoid using managed memory, which you can control with the following Flink parameter:\ntaskmanager.memory.managed.size=1m "});index.add({'id':8,'href':'/docs/master/concepts/basic-concepts/','title':"Basic Concepts",'section':"Concepts",'content':"Basic Concepts #  Snapshot #  A snapshot captures the state of a table at some point in time. Users can access the latest data of a table through the latest snapshot. By time traveling, users can also access the previous state of a table through an earlier snapshot.\nPartition #  Paimon adopts the same partitioning concept as Apache Hive to separate data.\nPartitioning is an optional way of dividing a table into related parts based on the values of particular columns like date, city, and department. Each table can have one or more partition keys to identify a particular partition.\nBy partitioning, users can efficiently operate on a slice of records in the table. See file layouts for how files are divided into multiple partitions.\nPartition keys must be a subset of primary keys if primary keys are defined.  Bucket #  Unpartitioned tables, or partitions in partitioned tables, are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying.\nThe range for a bucket is determined by the hash value of one or more columns in the records. Users can specify bucketing columns by providing the bucket-key option. If no bucket-key option is specified, the primary key (if defined) or the complete record will be used as the bucket key.\nA bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 1GB.\nSee file layouts for how files are divided into buckets. Also, see rescale bucket if you want to adjust the number of buckets after a table is created.\nConsistency Guarantees #  Paimon writers use two-phase commit protocol to atomically commit a batch of records to the table. Each commit produces at most two snapshots at commit time.\nFor any two writers modifying a table at the same time, as long as they do not modify the same bucket, their commits are serializable. If they modify the same bucket, only snapshot isolation is guaranteed. That is, the final table state may be a mix of the two commits, but no changes are lost.\n"});index.add({'id':9,'href':'/docs/master/how-to/creating-tables/','title':"Creating Tables",'section':"How to",'content':"Creating Tables #  Creating Catalog Managed Tables #  Tables created in Paimon catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.\nThe following SQL assumes that you have registered and are using a Paimon catalog. It creates a managed table named MyTable with five columns in the catalog\u0026rsquo;s default database, where dt, hh and user_id are the primary keys.\nFlink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); Hive SET hive.metastore.warehouse.dir=warehouse_path; CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); Trino CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior VARCHAR, dt VARCHAR, hh VARCHAR ) WITH ( primary_key = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;user_id\u0026#39;] ); Presto CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior VARCHAR, dt VARCHAR, hh VARCHAR ) WITH ( primary_key = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;user_id\u0026#39;] );  Inserting jobs on the table should be stopped prior to dropping tables, or table files couldn\u0026rsquo;t be deleted completely.  Partitioned Tables #  The following SQL creates a table named MyTable with five columns partitioned by dt and hh, where dt, hh and user_id are the primary keys.\nFlink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); Hive SET hive.metastore.warehouse.dir=warehouse_path; CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING ) PARTITIONED BY ( dt STRING, hh STRING ) STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); Trino CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior VARCHAR, dt VARCHAR, hh VARCHAR ) WITH ( primary_key = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;user_id\u0026#39;], partitioned_by = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;] ); Presto CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior VARCHAR, dt VARCHAR, hh VARCHAR ) WITH ( primary_key = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;user_id\u0026#39;], partitioned_by = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;] );  By configuring partition.expiration-time, expired partitions can be automatically deleted.  Pick Partition Fields #  Partition fields must be a subset of primary keys if primary keys are defined.  The following three types of fields may be defined as partition fields in the warehouse:\n Creation Time (Recommended): The creation time is generally immutable, so you can confidently treat it as a partition field and add it to the primary key. Event Time: Event time is a field in the original table. For CDC data, such as tables synchronized from MySQL CDC or Changelogs generated by Paimon, they are all complete CDC data, including UPDATE_BEFORE records, even if you declare the primary key containing partition field, you can achieve the unique effect (require 'changelog-producer'='input'). CDC op_ts: It cannot be defined as a partition field, unable to know previous record timestamp.  Specify Statistics Mode #  Paimon will automatically collect the statistics of the data file for speeding up the query process. There are four modes supported:\n full: collect the full metrics: null_count, min, max . truncate(length): length can be any positive number, the default mode is truncate(16), which means collect the null count, min/max value with truncated length of 16. This is mainly to avoid too big column which will enlarge the manifest file. counts: only collect the null count. none: disable the metadata stats collection.  The statistics collector mode can be configured by 'metadata.stats-mode', by default is 'truncate(16)'. You can configure the field level by setting 'fields.{field_name}.stats-mode'.\nField Default Value #  Paimon table currently supports setting default values for fields in table properties, note that partition fields and primary key fields can not be specified. Flink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh) with( \u0026#39;fields.item_id.default-value\u0026#39;=\u0026#39;0\u0026#39; ); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39;, \u0026#39;fields.item_id.default-value\u0026#39;=\u0026#39;0\u0026#39; ); Hive SET hive.metastore.warehouse.dir=warehouse_path; CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39;, \u0026#39;partition\u0026#39;=\u0026#39;dt,hh\u0026#39;, \u0026#39;fields.item_id.default-value\u0026#39;=\u0026#39;0\u0026#39; );  Create Table As #  Table can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;\nWe can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.\nFlink /* For streaming mode, you need to enable the checkpoint. */ CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT ); CREATE TABLE MyTableAs AS SELECT * FROM MyTable; /* partitioned table */ CREATE TABLE MyTablePartition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE MyTablePartitionAs WITH (\u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM MyTablePartition; /* change options */ CREATE TABLE MyTableOptions ( user_id BIGINT, item_id BIGINT ) WITH (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE MyTableOptionsAs WITH (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM MyTableOptions; /* primary key */ CREATE TABLE MyTablePk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE MyTablePkAs WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM MyTablePk; /* primary key + partition */ CREATE TABLE MyTableAll ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); CREATE TABLE MyTableAllAs WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;, \u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM MyTableAll; Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT ); CREATE TABLE MyTableAs AS SELECT * FROM MyTable; /* partitioned table*/ CREATE TABLE MyTablePartition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE MyTablePartitionAs PARTITIONED BY (dt) AS SELECT * FROM MyTablePartition; /* change TBLPROPERTIES */ CREATE TABLE MyTableOptions ( user_id BIGINT, item_id BIGINT ) TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE MyTableOptionsAs TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM MyTableOptions; /* primary key */ CREATE TABLE MyTablePk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE MyTablePkAs TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM MyTablePk; /* primary key + partition */ CREATE TABLE MyTableAll ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE MyTableAllAs PARTITIONED BY (dt) TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM MyTableAll;  Create Table Like #  Flink To create a table with the same schema, partition, and table properties as another table, use CREATE TABLE LIKE.\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); CREATE TABLE MyTableLike LIKE MyTable;  Table Properties #  Users can specify table properties to enable features or improve performance of Paimon. For a complete list of such properties, see configurations.\nThe following SQL creates a table named MyTable with five columns partitioned by dt and hh, where dt, hh and user_id are the primary keys. This table has two properties: 'bucket' = '2' and 'bucket-key' = 'user_id'.\nFlink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;user_id\u0026#39; ); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;user_id\u0026#39; ); Hive CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39;, \u0026#39;partition\u0026#39;=\u0026#39;dt,hh\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;user_id\u0026#39; ); Trino CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior VARCHAR, dt VARCHAR, hh VARCHAR ) WITH ( primary_key = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;user_id\u0026#39;], partitioned_by = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;], bucket = \u0026#39;2\u0026#39;, bucket_key = \u0026#39;user_id\u0026#39; ); Presto CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior VARCHAR, dt VARCHAR, hh VARCHAR ) WITH ( primary_key = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;, \u0026#39;user_id\u0026#39;], partitioned_by = ARRAY[\u0026#39;dt\u0026#39;, \u0026#39;hh\u0026#39;], bucket = \u0026#39;2\u0026#39;, bucket_key = \u0026#39;user_id\u0026#39; );  Creating External Tables #  If the table already exists, options will not be updated into the table\u0026rsquo;s metadata, just as dynamic options.  External tables are recorded but not managed by catalogs. If an external table is dropped, its table files will not be deleted.\nPaimon external tables can be used in any catalog. If you do not want to create a Paimon catalog and just want to read / write a table, you can consider external tables.\nFlink Flink SQL supports reading and writing an external table. External Paimon tables are created by specifying the connector and path table properties. The following SQL creates an external table named MyTable with five columns, where the base path of table files is hdfs:///path/to/table.\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;hdfs:///path/to/table\u0026#39;, \u0026#39;auto-create\u0026#39; = \u0026#39;true\u0026#39; -- this table property creates table files for an empty table if table path does not exist  -- currently only supported by Flink ); Spark3 Spark3 only supports creating external tables through Scala API. The following Scala code loads the table located at hdfs:///path/to/table into a DataSet.\nval dataset = spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;hdfs:///path/to/table\u0026#34;) Spark2 Spark2 only supports creating external tables through Scala API. The following Scala code loads the table located at hdfs:///path/to/table into a DataSet.\nval dataset = spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;hdfs:///path/to/table\u0026#34;) Hive To access existing paimon table, you can also register them as external tables in Hive. The following SQL creates an external table named my_table, where the base path of table files is hdfs:///path/to/table. As schemas are stored in table files, users do not need to write column definitions.\nCREATE EXTERNAL TABLE my_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; LOCATION \u0026#39;hdfs:///path/to/table\u0026#39;;  Creating Temporary Tables #  Flink Temporary tables are only supported by Flink. Like external tables, temporary tables are just recorded but not managed by the current Flink SQL session. If the temporary table is dropped, its resources will not be deleted. Temporary tables are also dropped when Flink SQL session is closed.\nIf you want to use Paimon catalog along with other tables but do not want to store them in other catalogs, you can create a temporary table. The following Flink SQL creates a Paimon catalog and a temporary table and also illustrates how to use both tables together.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; -- Assume that there is already a table named my_table in my_catalog  CREATE TEMPORARY TABLE temp_table ( k INT, v STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;hdfs:///path/to/temp_table.csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); SELECT my_table.k, my_table.v, temp_table.v FROM my_table JOIN temp_table ON my_table.k = temp_table.k;  "});index.add({'id':10,'href':'/docs/master/project/download/','title':"Download",'section':"Project",'content':"Download #  This documentation is a guide for downloading Paimon Jars.\nEngine Jars #     Version Jar     Flink 1.17 paimon-flink-1.17-0.5-SNAPSHOT.jar   Flink 1.16 paimon-flink-1.16-0.5-SNAPSHOT.jar   Flink 1.15 paimon-flink-1.15-0.5-SNAPSHOT.jar   Flink 1.14 paimon-flink-1.14-0.5-SNAPSHOT.jar   Flink Action paimon-flink-action-0.5-SNAPSHOT.jar   Spark 3.4 paimon-spark-3.4-0.5-SNAPSHOT.jar   Spark 3.3 paimon-spark-3.3-0.5-SNAPSHOT.jar   Spark 3.2 paimon-spark-3.2-0.5-SNAPSHOT.jar   Spark 3.1 paimon-spark-3.1-0.5-SNAPSHOT.jar   Spark 2 paimon-spark-2-0.5-SNAPSHOT.jar   Hive 3.1 paimon-hive-connector-3.1-0.5-SNAPSHOT.jar   Hive 2.3 paimon-hive-connector-2.3-0.5-SNAPSHOT.jar   Hive 2.2 paimon-hive-connector-2.2-0.5-SNAPSHOT.jar   Hive 2.1 paimon-hive-connector-2.1-0.5-SNAPSHOT.jar   Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-0.5-SNAPSHOT.jar   Presto 0.236 paimon-presto-0.236-0.5-SNAPSHOT.jar   Presto 0.268 paimon-presto-0.268-0.5-SNAPSHOT.jar   Presto 0.273 paimon-presto-0.273-0.5-SNAPSHOT.jar   Presto SQL 332 paimon-prestosql-332-0.5-SNAPSHOT.jar   Trino 358 paimon-trino-358-0.5-SNAPSHOT.jar   Trino 368 paimon-trino-368-0.5-SNAPSHOT.jar   Trino 369 paimon-trino-369-0.5-SNAPSHOT.jar   Trino 370 paimon-trino-370-0.5-SNAPSHOT.jar   Trino 388 paimon-trino-388-0.5-SNAPSHOT.jar   Trino 393 paimon-trino-393-0.5-SNAPSHOT.jar   Trino 422 paimon-trino-422-0.5-SNAPSHOT.jar    Filesystem Jars #     Version Jar     paimon-oss paimon-oss-0.5-SNAPSHOT.jar   paimon-s3 paimon-s3-0.5-SNAPSHOT.jar    API Jars #     Version Jar     paimon-bundle paimon-bundle-0.5-SNAPSHOT.jar    MANIFEST.MF For unstable version, you can find git commit id in jar:\nManifest-Version: 1.0 Implementation-Title: Paimon : Common Implementation-Version: 0.5-SNAPSHOT Specification-Vendor: The Apache Software Foundation Specification-Title: Paimon : Common Implementation-Vendor-Id: org.apache.paimon SCM-Branch: master Implementation-Vendor: The Apache Software Foundation SCM-Revision: c8b4772f3cb4b25b25537e1ab0775441c627bf1c Created-By: Apache Maven 3.2.5 Build-Jdk: 1.8.0_301 Specification-Version: 0.5-SNAPSHOT The SCM-Revision git commit id.\n"});index.add({'id':11,'href':'/docs/master/engines/','title':"Engines",'section':"Apache Paimon",'content':""});index.add({'id':12,'href':'/docs/master/engines/flink/','title':"Flink",'section':"Engines",'content':"Flink #  This documentation is a guide for using Paimon in Flink.\nPreparing Paimon Jar File #  Paimon currently supports Flink 1.17, 1.16, 1.15 and 1.14. We recommend the latest Flink version for a better experience.\nDownload the jar file with corresponding version.\n Currently, paimon provides two types jar: one of which(the bundled jar) is used for read/write data, and the other(action jar) for operations such as manually compaction,     Version Type Jar     Flink 1.17 Bundled Jar paimon-flink-1.17-0.5-SNAPSHOT.jar   Flink 1.16 Bundled Jar paimon-flink-1.16-0.5-SNAPSHOT.jar   Flink 1.15 Bundled Jar paimon-flink-1.15-0.5-SNAPSHOT.jar   Flink 1.14 Bundled Jar paimon-flink-1.14-0.5-SNAPSHOT.jar   Flink Action Action Jar paimon-flink-action-0.5-SNAPSHOT.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\n mvn clean install -DskipTests  You can find the bundled jar in ./paimon-flink/paimon-flink-\u0026lt;flink-version\u0026gt;/target/paimon-flink-\u0026lt;flink-version\u0026gt;-0.5-SNAPSHOT.jar, and the action jar in ./paimon-flink/paimon-flink-action/target/paimon-flink-action-0.5-SNAPSHOT.jar.\nQuick Start #  Using bundled Jar #  Step 1: Download Flink\nIf you haven\u0026rsquo;t downloaded Flink, you can download Flink, then extract the archive with the following command.\ntar -xzf flink-*.tgz Step 2: Copy Paimon Bundled Jar\nCopy paimon bundled jar to the lib directory of your Flink home.\ncp paimon-flink-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 3: Copy Hadoop Bundled Jar\nIf the machine is in a hadoop environment, please ensure the value of the environment variable HADOOP_CLASSPATH include path to the common Hadoop libraries, you do not need to use the following pre-bundled Hadoop jar.  Download Pre-bundled Hadoop jar and copy the jar file to the lib directory of your Flink home.\ncp flink-shaded-hadoop-2-uber-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 4: Start a Flink Local Cluster\nIn order to run multiple Flink jobs at the same time, you need to modify the cluster configuration in \u0026lt;FLINK_HOME\u0026gt;/conf/flink-conf.yaml.\ntaskmanager.numberOfTaskSlots:2To start a local cluster, run the bash script that comes with Flink:\n\u0026lt;FLINK_HOME\u0026gt;/bin/start-cluster.sh You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.\nYou can now start Flink SQL client to execute SQL scripts.\n\u0026lt;FLINK_HOME\u0026gt;/bin/sql-client.sh Step 5: Create a Catalog and a Table\nCatalog -- if you\u0026#39;re trying out Paimon in a distributed environment, -- the warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;file:/tmp/paimon\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); Generic-Catalog Using FlinkGenericCatalog, you need to use Hive metastore. Then, you can use all the tables from Paimon, Hive, and Flink Generic Tables (Kafka and other tables)!\nIn this mode, you should use \u0026lsquo;connector\u0026rsquo; option for creating tables.\nPaimon will use hive.metastore.warehouse.dir in your hive-site.xml, please use path with scheme. For example, hdfs://.... Otherwise, Paimon will use the local path.  CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon-generic\u0026#39;, \u0026#39;hive-conf-dir\u0026#39;=\u0026#39;...\u0026#39;, \u0026#39;hadoop-conf-dir\u0026#39;=\u0026#39;...\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ) WITH ( \u0026#39;connector\u0026#39;=\u0026#39;paimon\u0026#39; );  Step 6: Write Data\n-- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.word.length\u0026#39; = \u0026#39;1\u0026#39; ); -- paimon requires checkpoint interval in streaming mode SET \u0026#39;execution.checkpointing.interval\u0026#39; = \u0026#39;10 s\u0026#39;; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; Step 7: OLAP Query\n-- use tableau result mode SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; -- switch to batch mode RESET \u0026#39;execution.checkpointing.interval\u0026#39;; SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- olap query the table SELECT * FROM word_count; You can execute the query multiple times and observe the changes in the results.\nStep 8: Streaming Query\n-- switch to streaming mode SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; -- track the changes of table and calculate the count interval statistics SELECT `interval`, COUNT(*) AS interval_cnt FROM (SELECT cnt / 10000 AS `interval` FROM word_count) GROUP BY `interval`; Step 9: Exit\nCancel streaming job in localhost:8081, then execute the following SQL script to exit Flink SQL client.\n-- uncomment the following line if you want to drop the dynamic table and clear the files -- DROP TABLE word_count;  -- exit sql-client EXIT; Stop the Flink local cluster.\n./bin/stop-cluster.sh Using Action Jar #  After the Flink Local Cluster has been started, you can execute the action jar by using the following command\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\ \u0026lt;action\u0026gt; \u0026lt;args\u0026gt; The following command will used to compact a table\nBatch \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\ compact \\ --path \u0026lt;TABLE_PATH\u0026gt; Supported Flink Data Type #  See Flink Data Types.\nAll Flink data types are supported, except that\n MULTISET is not supported. MAP is not supported as primary keys.  Use Flink Managed Memory #  Paimon tasks can create memory pools based on executor memory which will be managed by Flink executor, such as managed memory in Flink task manager. It will improve the stability and performance of sinks by managing writer buffers for multiple tasks through executor.\nThe following properties can be set if using Flink managed memory:\n   Option Default Description     sink.use-managed-memory-allocator false If true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator, which means each task allocates and manages its own memory pool (heap memory), if there are too many tasks in one Executor, it may cause performance issues and even OOM.   sink.managed.writer-buffer-memory 256M Weight of writer buffer in managed memory, Flink will compute the memory size, for writer according to the weight, the actual memory used depends on the running environment. Now the memory size defined in this property are equals to the exact memory allocated to write buffer in runtime.    Use In SQL Users can set memory weight in SQL for Flink Managed Memory, then Flink sink operator will get the memory pool size and create allocator for Paimon writer.\nINSERT INTO paimon_table /*+ OPTIONS(\u0026#39;sink.use-managed-memory-allocator\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;sink.managed.writer-buffer-memory\u0026#39;=\u0026#39;256M\u0026#39;) */ SELECT * FROM ....; "});index.add({'id':13,'href':'/docs/master/api/flink-api/','title':"Flink API",'section':"API",'content':"Flink API #  Dependency #  Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-flink-1.17\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.5-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.17.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; Or download the jar file: Paimon Flink. Please choose your Flink version.\nPaimon relies on Hadoop environment, you should add hadoop classpath or bundled jar.\nPaimon does not provide a DataStream API, but you can read or write to Paimon tables by the conversion between DataStream and Table in Flink. See DataStream API Integration.\nWrite to Table #  import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.DataTypes; import org.apache.flink.table.api.Schema; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; import org.apache.flink.types.RowKind; public class WriteToTable { public static void writeTo() { // create environments of both APIs  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create a changelog DataStream  DataStream\u0026lt;Row\u0026gt; dataStream = env.fromElements( Row.ofKind(RowKind.INSERT, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.INSERT, \u0026#34;Bob\u0026#34;, 5), Row.ofKind(RowKind.UPDATE_BEFORE, \u0026#34;Alice\u0026#34;, 12), Row.ofKind(RowKind.UPDATE_AFTER, \u0026#34;Alice\u0026#34;, 100)) .returns( Types.ROW_NAMED( new String[] {\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;}, Types.STRING, Types.INT)); // interpret the DataStream as a Table  Schema schema = Schema.newBuilder() .column(\u0026#34;name\u0026#34;, DataTypes.STRING()) .column(\u0026#34;age\u0026#34;, DataTypes.INT()) .build(); Table table = tableEnv.fromChangelogStream(dataStream, schema); // create paimon catalog  tableEnv.executeSql(\u0026#34;CREATE CATALOG paimon WITH (\u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;...\u0026#39;)\u0026#34;); tableEnv.executeSql(\u0026#34;USE CATALOG paimon\u0026#34;); // register the table under a name and perform an aggregation  tableEnv.createTemporaryView(\u0026#34;InputTable\u0026#34;, table); // insert into paimon table from your data stream table  tableEnv.executeSql(\u0026#34;INSERT INTO sink_paimon_table SELECT * FROM InputTable\u0026#34;); } } Read from Table #  import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; public class ReadFromTable { public static void readFrom() throws Exception { // create environments of both APIs  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // create paimon catalog  tableEnv.executeSql(\u0026#34;CREATE CATALOG paimon WITH (\u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;...\u0026#39;)\u0026#34;); tableEnv.executeSql(\u0026#34;USE CATALOG paimon\u0026#34;); // convert to DataStream  Table table = tableEnv.sqlQuery(\u0026#34;SELECT * FROM my_paimon_table\u0026#34;); DataStream\u0026lt;Row\u0026gt; dataStream = tableEnv.toChangelogStream(table); // use this datastream  dataStream.executeAndCollect().forEachRemaining(System.out::println); // prints:  // +I[Bob, 12]  // +I[Alice, 12]  // -U[Alice, 12]  // +U[Alice, 14]  } } Cdc ingestion Table #  Paimon supports ingest data into Paimon tables with schema evolution.\n You can use Java API to write cdc records into Paimon Tables. You can write records to Paimon\u0026rsquo;s partial-update table with adding columns dynamically.  Here is an example to use RichCdcSinkBuilder API:\nimport org.apache.paimon.catalog.Catalog; import org.apache.paimon.catalog.CatalogContext; import org.apache.paimon.catalog.CatalogFactory; import org.apache.paimon.catalog.Identifier; import org.apache.paimon.flink.sink.cdc.RichCdcRecord; import org.apache.paimon.flink.sink.cdc.RichCdcSinkBuilder; import org.apache.paimon.fs.Path; import org.apache.paimon.options.Options; import org.apache.paimon.schema.Schema; import org.apache.paimon.table.Table; import org.apache.paimon.types.DataTypes; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import static org.apache.paimon.types.RowKind.INSERT; public class WriteCdcToTable { public static void writeTo() throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream\u0026lt;RichCdcRecord\u0026gt; dataStream = env.fromElements( RichCdcRecord.builder(INSERT) .field(\u0026#34;order_id\u0026#34;, DataTypes.BIGINT(), \u0026#34;123\u0026#34;) .field(\u0026#34;price\u0026#34;, DataTypes.DOUBLE(), \u0026#34;62.2\u0026#34;) .build(), // dt field will be added with schema evolution  RichCdcRecord.builder(INSERT) .field(\u0026#34;order_id\u0026#34;, DataTypes.BIGINT(), \u0026#34;245\u0026#34;) .field(\u0026#34;price\u0026#34;, DataTypes.DOUBLE(), \u0026#34;82.1\u0026#34;) .field(\u0026#34;dt\u0026#34;, DataTypes.TIMESTAMP(), \u0026#34;2023-06-12 20:21:12\u0026#34;) .build()); Identifier identifier = Identifier.create(\u0026#34;my_db\u0026#34;, \u0026#34;T\u0026#34;); Options catalogOptions = new Options(); catalogOptions.set(\u0026#34;warehouse\u0026#34;, \u0026#34;/path/to/warehouse\u0026#34;); Catalog.Loader catalogLoader = () -\u0026gt; FlinkCatalogFactory.createPaimonCatalog(catalogOptions); new RichCdcSinkBuilder() .withInput(dataStream) .withTable(createTableIfNotExists(identifier)) .withIdentifier(identifier) .withCatalogLoader(catalogLoader) .build(); env.execute(); } private static Table createTableIfNotExists(Identifier identifier) throws Exception { CatalogContext context = CatalogContext.create(new Path(\u0026#34;...\u0026#34;)); Catalog catalog = CatalogFactory.createCatalog(context); Schema.Builder schemaBuilder = Schema.newBuilder(); schemaBuilder.primaryKey(\u0026#34;order_id\u0026#34;); schemaBuilder.column(\u0026#34;order_id\u0026#34;, DataTypes.BIGINT()); schemaBuilder.column(\u0026#34;price\u0026#34;, DataTypes.DOUBLE()); Schema schema = schemaBuilder.build(); try { catalog.createTable(identifier, schema, false); } catch (Catalog.TableAlreadyExistException e) { // do something  } return catalog.getTable(identifier); } } "});index.add({'id':14,'href':'/docs/master/filesystems/hdfs/','title':"HDFS",'section':"Filesystems",'content':"HDFS #  You don\u0026rsquo;t need any additional dependencies to access HDFS because you have already taken care of the Hadoop dependencies.\nHDFS Configuration #  For HDFS, the most important thing is to be able to read your HDFS configuration.\nFlink/Trino/JavaAPI You may not have to do anything, if you are in a hadoop environment. Otherwise pick one of the following ways to configure your HDFS:\n Set environment variable HADOOP_HOME or HADOOP_CONF_DIR. Configure 'hadoop-conf-dir' in the paimon catalog. Configure Hadoop options through prefix 'hadoop.' in the paimon catalog.  The first approach is recommended.\nIf you do not want to include the value of the environment variable, you can configure hadoop-conf-loader to option.\nHive/Spark HDFS Configuration is available directly through the computation cluster, see cluster configuration of Hive and Spark for details. Hadoop-compatible file systems (HCFS) #  All Hadoop file systems are automatically available when the Hadoop libraries are on the classpath.\nThis way, Paimon seamlessly supports all of Hadoop file systems implementing the org.apache.hadoop.fs.FileSystem interface, and all Hadoop-compatible file systems (HCFS).\n HDFS Alluxio (see configuration specifics below) XtreemFS …  The Hadoop configuration has to have an entry for the required file system implementation in the core-site.xml file.\nFor Alluxio support add the following entry into the core-site.xml file:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.alluxio.impl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;alluxio.hadoop.FileSystem\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Kerberos #  Flink It is recommended to use Flink Kerberos Keytab.Spark It is recommended to use Spark Kerberos Keytab.Hive An intuitive approach is to configure Hive\u0026rsquo;s kerberos authentication.Trino/JavaAPI Configure the following three options in your catalog configuration:\n security.kerberos.login.keytab: Absolute path to a Kerberos keytab file that contains the user credentials. Please make sure it is copied to each machine. security.kerberos.login.principal: Kerberos principal name associated with the keytab. security.kerberos.login.use-ticket-cache: True or false, indicates whether to read from your Kerberos ticket cache.  For JavaAPI:\nSecurityContext.install(catalogOptions);  HDFS HA #  Ensure that hdfs-site.xml and core-site.xml contain the necessary HA configuration.\nHDFS ViewFS #  Ensure that hdfs-site.xml and core-site.xml contain the necessary ViewFs configuration.\n"});index.add({'id':15,'href':'/docs/master/maintenance/read-performance/','title':"Read Performance",'section':"Maintenance",'content':"Read Performance #  Full Compaction #  Configure \u0026lsquo;full-compaction.delta-commits\u0026rsquo; perform full-compaction periodically in Flink writing. And it can ensure that partitions are full compacted before writing ends.\nPaimon defaults to handling small files and providing decent read performance. Please do not configure this full-compaction option without any requirements, as it will have a significant impact on performance.  Primary Key Table #  For Primary Key Table, it\u0026rsquo;s a \u0026lsquo;MergeOnRead\u0026rsquo; technology. When reading data, multiple layers of LSM data are merged, and the number of parallelism will be limited by the number of buckets. Although Paimon\u0026rsquo;s merge performance is efficient, it still cannot catch up with the ordinary AppendOnly table.\nIf you want to query fast enough in certain scenarios, but can only find older data, you can:\n Configure \u0026lsquo;full-compaction.delta-commits\u0026rsquo;, when writing data (currently only Flink), full compaction will be performed periodically. Configure \u0026lsquo;scan.mode\u0026rsquo; to \u0026lsquo;compacted-full\u0026rsquo;, when reading data, snapshot of full compaction is picked. Read performance is good.  You can flexibly balance query performance and data latency when reading.\nAppend Only Table #  Small files will slow down reading performance and affect the stability of DFS. By default, when there are more than \u0026lsquo;compaction.max.file-num\u0026rsquo; (default 50) small files in a single bucket, a compaction task will be triggered to compact them. Furthermore, if there are multiple buckets, many small files will be generated.\nYou can use full-compaction to reduce small files. Full-compaction will eliminate most small files.\nFormat #  Paimon has some query optimizations to parquet reading, so parquet will be slightly faster that orc.\n"});index.add({'id':16,'href':'/docs/master/how-to/altering-tables/','title':"Altering Tables",'section':"How to",'content':"Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.\nFlink ALTER TABLE my_table SET ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Spark3 ALTER TABLE my_table SET TBLPROPERTIES ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Trino ALTER TABLE my_table SET PROPERTIES write_buffer_size = \u0026#39;256 MB\u0026#39;;  NOTE: Versions below Trino 368 do not support changing/adding table properties.\n  Rename Table Name #  The following SQL rename the table name to new name.\nFlink ALTER TABLE my_table RENAME TO my_table_new; Spark3 The simplest sql to call is:\nALTER TABLE my_table RENAME TO my_table_new; Note that: we can rename paimon table in spark this way:\nALTER TABLE [catalog.[database.]]test1 RENAME to [database.]test2; But we can\u0026rsquo;t put catalog name before the renamed-to table, it will throw an error if we write sql like this:\nALTER TABLE catalog.database.test1 RENAME to catalog.database.test2; Trino ALTER TABLE my_table RENAME TO my_table_new; Presto ALTER TABLE my_table RENAME TO my_table_new;  If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.  Removing Table Properties #  The following SQL removes write-buffer-size table property.\nFlink ALTER TABLE my_table RESET (\u0026#39;write-buffer-size\u0026#39;); Spark3 ALTER TABLE my_table UNSET TBLPROPERTIES (\u0026#39;write-buffer-size\u0026#39;);  Adding New Columns #  The following SQL adds two columns c1 and c2 to table my_table.\nFlink ALTER TABLE my_table ADD (c1 INT, c2 STRING); Spark3 ALTER TABLE my_table ADD COLUMNS ( c1 INT, c2 STRING ); Trino ALTER TABLE my_table ADD COLUMN c1 VARCHAR; Presto ALTER TABLE my_table ADD COLUMN c1 VARCHAR;  Renaming Column Name #  The following SQL renames column c0 in table my_table to c1.\nFlink ALTER TABLE my_table RENAME c0 TO c1; Spark3 ALTER TABLE my_table RENAME COLUMN c0 TO c1; Trino ALTER TABLE my_table RENAME COLUMN c0 TO c1; Presto ALTER TABLE my_table RENAME COLUMN c0 TO c1;  Dropping Columns #  The following SQL drops two columns c1 and c2 from table my_table. In hive catalog, you need to ensure disable hive.metastore.disallow.incompatible.col.type.changes in your hive server, otherwise this operation may fail, throws an exception like The following columns have types incompatible with the existing columns in their respective positions.\nFlink ALTER TABLE my_table DROP (c1, c2); Spark3 ALTER TABLE my_table DROP COLUMNS (c1, c2); Trino ALTER TABLE my_table DROP COLUMN c1; Presto ALTER TABLE my_table DROP COLUMN c1;  Changing Column Nullability #  The following SQL changes nullability of column coupon_info.\nFlink CREATE TABLE my_table (id INT PRIMARY KEY NOT ENFORCED, coupon_info FLOAT NOT NULL); -- Change column `coupon_info` from NOT NULL to nullable ALTER TABLE my_table MODIFY coupon_info FLOAT; -- Change column `coupon_info` from nullable to NOT NULL -- If there are NULL values already, set table option as below to drop those records silently before altering table. SET \u0026#39;table.exec.sink.not-null-enforcer\u0026#39; = \u0026#39;DROP\u0026#39;; ALTER TABLE my_table MODIFY coupon_info FLOAT NOT NULL; Spark3 ALTER TABLE my_table ALTER COLUMN coupon_info DROP NOT NULL;  Changing nullable column to NOT NULL is only supported by Flink currently.  Changing Column Comment #  The following SQL changes comment of column buy_count to buy count.\nFlink ALTER TABLE my_table MODIFY buy_count BIGINT COMMENT \u0026#39;buy count\u0026#39;; Spark3 ALTER TABLE my_table ALTER COLUMN buy_count COMMENT \u0026#39;buy count\u0026#39;;  Adding Column Position #  To add a new column with specified position, use FIRST or AFTER col_name.\nFlink ALTER TABLE my_table ADD c INT FIRST; ALTER TABLE my_table ADD c INT AFTER b; Spark3 ALTER TABLE my_table ADD COLUMN c INT FIRST; ALTER TABLE my_table ADD COLUMN c INT AFTER b;  Changing Column Position #  To modify an existent column to a new position, use FIRST or AFTER col_name.\nFlink ALTER TABLE my_table MODIFY col_a DOUBLE FIRST; ALTER TABLE my_table MODIFY col_a DOUBLE AFTER col_b; Spark3 ALTER TABLE my_table ALTER COLUMN col_a FIRST; ALTER TABLE my_table ALTER COLUMN col_a AFTER col_b;  Changing Column Type #  The following SQL changes type of column col_a to DOUBLE.\nFlink ALTER TABLE my_table MODIFY col_a DOUBLE; Spark3 ALTER TABLE my_table ALTER COLUMN col_a TYPE DOUBLE; Trino ALTER TABLE my_table ALTER COLUMN col_a SET DATA TYPE DOUBLE;  Supported Type Conversions.\n  Input Type Target Type     TINYINT SMALLINT   INT   BIGINT   FLOAT   DOUBLE   DECIMAL   BOOLEAN   CHAR   VARCHAR/STRING   SMALLINT TINYINT   INT   BIGINT   FLOAT   DOUBLE   DECIMAL   BOOLEAN   CHAR   VARCHAR/STRING   INT TINYINT   SMALLINT   BIGINT   FLOAT   DOUBLE   DECIMAL   BOOLEAN   CHAR   VARCHAR/STRING   BIGINT TINYINT   SMALLINT   INT   FLOAT   DOUBLE   DECIMAL   BOOLEAN   CHAR   VARCHAR/STRING   FLOAT TINYINT   SMALLINT   INT   BIGINT   DOUBLE   DECIMAL   CHAR   VARCHAR/STRING   DOUBLE TINYINT   SMALLINT   INT   BIGINT   FLOAT   DECIMAL   CHAR   VARCHAR/STRING   DECIMAL TINYINT   SMALLINT   INT   BIGINT   FLOAT   DOUBLE   DECIMAL   CHAR   VARCHAR/STRING   BOOLEAN TINYINT   SMALLINT   INT   BIGINT   FLOAT   DOUBLE   DECIMAL   CHAR   VARCHAR/STRING   TIMESTAMP/TIMESTAMP WITH LOCAL TIME ZONE TIMESTAMP   TIMESTAMP WITH LOCAL TIME ZONE   DATE   TIME   STRING   DATE TIMESTAMP   TIMESTAMP WITH LOCAL TIME ZONE   STRING   TIME TIMESTAMP   TIMESTAMP WITH LOCAL TIME ZONE   STRING   CHAR/VARCHAR/STRING CHAR   VARCHAR/STRING   BOOLEAN   TINYINT   SMALLINT   INT   BIGINT   FLOAT   DOUBLE   DECIMAL   TIMESTAMP   TIMESTAMP WITH LOCAL TIME ZONE   DATE   TIME   BINARY   VARBINARY   BINARY BINARY   VARBINARY   VARBINARY BINARY   VARBINARY    Adding watermark #  The following SQL adds a computed column ts from existing column log_ts, and a watermark with strategy ts - INTERVAL '1' HOUR on column ts which is marked as event time attribute of table my_table.\nFlink ALTER TABLE my_table ADD ( ts AS TO_TIMESTAMP(log_ts) AFTER log_ts, WATERMARK FOR ts AS ts - INTERVAL \u0026#39;1\u0026#39; HOUR );  Dropping watermark #  The following SQL drops the watermark of table my_table.\nFlink ALTER TABLE my_table DROP WATERMARK;  Changing watermark #  The following SQL modifies the watermark strategy to ts - INTERVAL '2' HOUR.\nFlink ALTER TABLE my_table MODIFY WATERMARK FOR ts AS ts - INTERVAL \u0026#39;2\u0026#39; HOUR  "});index.add({'id':17,'href':'/docs/master/project/contributing/','title':"Contributing",'section':"Project",'content':"Contributing #  Apache Paimon (incubating) is developed by an open and friendly community. Everybody is cordially welcome to join the community and contribute to Apache Paimon. There are several ways to interact with the community and contribute to Paimon including asking questions, filing bug reports, proposing new features, joining discussions on the mailing lists, contributing code or documentation, improving website, testing release candidates and writing corresponding blog etc.\nWhat do you want to do? Contributing to Apache Paimon goes beyond writing code for the project. Below, we list different opportunities to help the project:\n  Area Further information      Report Bug To report a problem with Paimon, open Paimon’s issues.  Please give detailed information about the problem you encountered and, if possible, add a description that helps to reproduce the problem.    Contribute Code Read the Code Contribution Guide    Code Reviews Read the Code Review Guide    Release Version Releasing a new Paimon version.    Support Users Reply to questions on the user mailing list, check the latest issues in Issues for tickets which are actually user questions.     Spread the Word About Paimon Organize or attend a Paimon Meetup, contribute to the Paimon blog, share your conference, meetup or blog post on the dev@paimon.apache.org mailing list.     Any other question? Reach out to the dev@paimon.apache.org mailing list to get help!     Code Contribution Guide #  Apache Paimon is maintained, improved, and extended by code contributions of volunteers. We welcome contributions to Paimon.\nPlease feel free to ask questions at any time. Either send a mail to the Dev mailing list or comment on the issue you are working on.\n .contribute-grid { margin-bottom: 10px; display: flex; flex-direction: column; margin-left: -2px; margin-right: -2px; } .contribute-grid .column { margin-top: 4px; padding: 0 2px; } @media only screen and (min-width: 480px) { .contribute-grid { flex-direction: row; flex-wrap: wrap; } .contribute-grid .column { flex: 0 0 50%; } .contribute-grid .column { margin-top: 4px; } } @media only screen and (min-width: 960px) { .contribute-grid { flex-wrap: nowrap; } .contribute-grid .column { flex: 0 0 25%; } } .contribute-grid .panel { height: 100%; margin: 0; } .contribute-grid .panel-body { padding: 10px; } .contribute-grid h2 { margin: 0 0 10px 0; padding: 0; display: flex; align-items: flex-start; } .contribute-grid .number { margin-right: 0.25em; font-size: 1.5em; line-height: 0.9; }  1Discuss Create an Issue or mailing list discussion and reach consensus\nTo request an issue, please note that it is not just a \"please assign it to me\", you need to explain your understanding of the issue, and your design, and if possible, you need to provide your POC code.\n   2Implement Create the Pull Request and the approach agreed upon in the issue.\n1.Only create the PR if you are assigned to the issue. 2.Please associate an issue (if any), e.g. fix #123. 3.Please enable the actions of your own clone project.\n   3Review Work with the reviewer.\n1.Make sure no unrelated or unnecessary reformatting changes are included. 2.Please ensure that the test passing. 3.Please don't resolve conversation.\n   4Merge A committer of Paimon checks if the contribution fulfills the requirements and merges the code to the codebase.\n    Code Review Guide #  Every review needs to check the following six aspects. We encourage to check these aspects in order, to avoid spending time on detailed code quality reviews when formal requirements are not met or there is no consensus in the community to accept the change.\n1. Is the Contribution Well-Described? #  Check whether the contribution is sufficiently well-described to support a good review. Trivial changes and fixes do not need a long description. If the implementation is exactly according to a prior discussion on issue or the development mailing list, only a short reference to that discussion is needed.\nIf the implementation is different from the agreed approach in the consensus discussion, a detailed description of the implementation is required for any further review of the contribution.\n2. Does the Contribution Need Attention from some Specific Committers? #  Some changes require attention and approval from specific committers.\nIf the pull request needs specific attention, one of the tagged committers/contributors should give the final approval.\n3. Is the Overall Code Quality Good, Meeting Standard we Want to Maintain in Paimon? #   Does the code follow the right software engineering practices? Is the code correct, robust, maintainable, testable? Are the changes performance aware, when changing a performance sensitive part? Are the changes sufficiently covered by tests? Are the tests executing fast? If dependencies have been changed, were the NOTICE files updated?  Code guidelines can be found in the Flink Java Code Style and Quality Guide.\n4. Are the documentation updated? #  If the pull request introduces a new feature, the feature should be documented.\nBecome a Committer #  How to become a committer #  There is no strict protocol for becoming a committer. Candidates for new committers are typically people that are active contributors and community members. Candidates are suggested by current committers or PPMC members, and voted upon by the PPMC.\nIf you would like to become a committer, you should engage with the community and start contributing to Apache Paimon in any of the above ways. You might also want to talk to other committers and ask for their advice and guidance.\n  Community contributions include helping to answer user questions on the mailing list, verifying release candidates, giving talks, organizing community events, and other forms of evangelism and community building. The \u0026ldquo;Apache Way\u0026rdquo; has a strong focus on the project community, and committers can be recognized for outstanding community contributions even without any code contributions.\n  Code/technology contributions include contributed pull requests (patches), design discussions, reviews, testing, and other help in identifying and fixing bugs. Especially constructive and high quality design discussions, as well as helping other contributors, are strong indicators.\n  Identify promising candidates #  While the prior points give ways to identify promising candidates, the following are \u0026ldquo;must haves\u0026rdquo; for any committer candidate:\n  Being community minded: The candidate understands the meritocratic principles of community management. They do not always optimize for as much as possible personal contribution, but will help and empower others where it makes sense.\n  We trust that a committer candidate will use their write access to the repositories responsibly, and if in doubt, conservatively. It is important that committers are aware of what they know and what they don\u0026rsquo;t know. In doubt, committers should ask for a second pair of eyes rather than commit to parts that they are not well familiar with.\n  They have shown to be respectful towards other community members and constructive in discussions.\n  "});index.add({'id':18,'href':'/docs/master/concepts/file-layouts/','title':"File Layouts",'section':"Concepts",'content':"File Layouts #  All files of a table are stored under one base directory. Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.\nSnapshot Files #  All snapshot files are stored in the snapshot directory.\nA snapshot file is a JSON file containing information about this snapshot, including\n the schema file in use the manifest list containing all changes of this snapshot  Manifest Files #  All manifest lists and manifest files are stored in the manifest directory.\nA manifest list is a list of manifest file names.\nA manifest file is a file containing changes about LSM data files and changelog files. For example, which LSM data file is created and which file is deleted in the corresponding snapshot.\nData Files #  Data files are grouped by partitions and buckets. Each bucket directory contains an LSM tree and its changelog files.\nCurrently, Paimon supports using orc(default), parquet and avro as data file\u0026rsquo;s format.\nLSM Trees #  Paimon adapts the LSM tree (log-structured merge-tree) as the data structure for file storage. This documentation briefly introduces the concepts about LSM trees.\nSorted Runs #  LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nRecords within a data file are sorted by their primary keys. Within a sorted run, ranges of primary keys of data files never overlap.\nAs you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified merge engine and the timestamp of each record.\nNew records written into the LSM tree will be first buffered in memory. When the memory buffer is full, all records in memory will be sorted and flushed to disk. A new sorted run is now created.\nCompaction #  When more and more records are written into the LSM tree, the number of sorted runs will increase. Because querying an LSM tree requires all sorted runs to be combined, too many sorted runs will result in a poor query performance, or even out of memory.\nTo limit the number of sorted runs, we have to merge several sorted runs into one big sorted run once in a while. This procedure is called compaction.\nHowever, compaction is a resource intensive procedure which consumes a certain amount of CPU time and disk IO, so too frequent compaction may in turn result in slower writes. It is a trade-off between query and write performance. Paimon currently adapts a compaction strategy similar to Rocksdb\u0026rsquo;s universal compaction.\nBy default, when Paimon appends records to the LSM tree, it will also perform compactions as needed. Users can also choose to perform all compactions in a dedicated compaction job. See dedicated compaction job for more info.\n"});index.add({'id':19,'href':'/docs/master/filesystems/','title':"Filesystems",'section':"Apache Paimon",'content':""});index.add({'id':20,'href':'/docs/master/maintenance/multiple-writers/','title':"Multiple Writers",'section':"Maintenance",'content':"Multiple Writers #  Paimon\u0026rsquo;s snapshot management supports writing with multiple writers.\nFor S3-like object store, its 'RENAME' does not have atomic semantic. We need to configure Hive metastore and enable 'lock.enabled' option for the catalog.  By default, Paimon supports concurrent writing to different partitions. A recommended mode is that streaming job writes records to Paimon\u0026rsquo;s latest partition; Simultaneously batch job (overwrite) writes records to the historical partition.\nSo far, everything works very well, but if you need multiple writers to write records to the same partition, it will be a bit more complicated. For example, you don\u0026rsquo;t want to use UNION ALL, you have multiple streaming jobs to write records to a 'partial-update' table. Please refer to the 'Dedicated Compaction Job' below.\nDedicated Compaction Job #  By default, Paimon writers will perform compaction as needed during writing records. This is sufficient for most use cases, but there are two downsides:\n This may result in unstable write throughput because throughput might temporarily drop when performing a compaction. Compaction will mark some data files as \u0026ldquo;deleted\u0026rdquo; (not really deleted, see expiring snapshots for more info). If multiple writers mark the same file, a conflict will occur when committing the changes. Paimon will automatically resolve the conflict, but this may result in job restarts.  To avoid these downsides, users can also choose to skip compactions in writers, and run a dedicated job only for compaction. As compactions are performed only by the dedicated job, writers can continuously write records without pausing and no conflicts will ever occur.\nTo skip compactions in writers, set the following table property to true.\n  Option Required Default Type Description     write-only No false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.    To run a dedicated job for compaction, follow these instructions.\nFlink Flink SQL currently does not support statements related to compactions, so we have to submit the compaction job through flink run.\nRun the following command to submit a compaction job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ Or run the following command to submit a compaction job for multiple database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  compact-database \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--including-tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--excluding-tables \u0026lt;paimon-table-name|name-regular-expr\u0026gt;] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]]  --database is used to specify which database is to be compacted. In compact mode, you need to specify a database name, in compact-database mode, you could specify multiple database, regular expression is supported. --including-tables is used to specify which source tables are to be compacted, you must use \u0026lsquo;|\u0026rsquo; to separate multiple tables, the format is databaseName.tableName, regular expression is supported. For example, specifying \u0026ldquo;\u0026ndash;including-tables db1.t1|db2.+\u0026rdquo; means to compact table \u0026lsquo;db1.t1\u0026rsquo; and all tables in the db2 database. --excluding-tables is used to specify which source tables are not to be compacted. The usage is same as \u0026ldquo;\u0026ndash;including-tables\u0026rdquo;. \u0026ldquo;\u0026ndash;excluding-tables\u0026rdquo; has higher priority than \u0026ldquo;\u0026ndash;including-tables\u0026rdquo; if you specified both. --catalog-conf is the configuration for Paimon catalog. Each configuration should be specified in the format key=value. See here for a complete list of catalog configurations.  If you submit a batch job (set execution.runtime-mode: batch in Flink\u0026rsquo;s configuration), all current table files will be compacted. If you submit a streaming job (set execution.runtime-mode: streaming in Flink\u0026rsquo;s configuration), the job will continuously monitor new changes to the table and perform compactions as needed.\nIf you only want to submit the compaction job and don\u0026rsquo;t want to wait until the job is done, you should submit in detached mode.  Example1: compact table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  compact \\  --warehouse s3:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition dt=20221126,hh=08 \\  --partition dt=20221127,hh=09 \\  --catalog-conf s3.endpoint=https://****.com \\  --catalog-conf s3.access-key=***** \\  --catalog-conf s3.secret-key=***** Example2: compact database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  compact-database \\  --warehouse s3:///path/to/warehouse \\  --database test_db \\  --catalog-conf s3.endpoint=https://****.com \\  --catalog-conf s3.access-key=***** \\  --catalog-conf s3.secret-key=***** For more usage of the compact action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  compact --help or\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  compact-database --help  "});index.add({'id':21,'href':'/docs/master/filesystems/oss/','title':"OSS",'section':"Filesystems",'content':"OSS #  Download paimon-oss-0.5-SNAPSHOT.jar. Flink If you have already configured oss access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-oss-0.5-SNAPSHOT.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;fs.oss.endpoint\u0026#39; = \u0026#39;oss-cn-hangzhou.aliyuncs.com\u0026#39;, \u0026#39;fs.oss.accessKeyId\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;fs.oss.accessKeySecret\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured oss access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-oss-0.5-SNAPSHOT.jar together with paimon-spark-0.5-SNAPSHOT.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=oss://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\  --conf spark.sql.catalog.paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com \\  --conf spark.sql.catalog.paimon.fs.oss.accessKeyId=xxx \\  --conf spark.sql.catalog.paimon.fs.oss.accessKeySecret=yyy Hive If you have already configured oss access through Hive (Via Hadoop FileSystem), here you can skip the following configuration.  NOTE: You need to ensure that Hive metastore can access oss.\nPlace paimon-oss-0.5-SNAPSHOT.jar together with paimon-hive-connector-0.5-SNAPSHOT.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com; SET paimon.fs.oss.accessKeyId=xxx; SET paimon.fs.oss.accessKeySecret=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino Place paimon-oss-0.5-SNAPSHOT.jar together with paimon-trino-0.5-SNAPSHOT.jar under plugin/paimon directory.\nAdd options in etc/catalog/paimon.properties.\nfs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com fs.oss.accessKeyId=xxx fs.oss.accessKeySecret=yyy  "});index.add({'id':22,'href':'/docs/master/engines/spark3/','title':"Spark3",'section':"Engines",'content':"Spark3 #  This documentation is a guide for using Paimon in Spark3.\nPreparing Paimon Jar File #  Paimon currently supports Spark 3.4, 3.3, 3.2 and 3.1. We recommend the latest Spark version for a better experience.\nDownload the jar file with corresponding version.\n   Version Jar     Spark 3.4 paimon-spark-3.4-0.5-SNAPSHOT.jar   Spark 3.3 paimon-spark-3.3-0.5-SNAPSHOT.jar   Spark 3.2 paimon-spark-3.2-0.5-SNAPSHOT.jar   Spark 3.1 paimon-spark-3.1-0.5-SNAPSHOT.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests For Spark 3.3, you can find the bundled jar in ./paimon-spark/paimon-spark-3.3/target/paimon-spark-3.3-0.5-SNAPSHOT.jar.\nQuick Start #  If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.  Step 1: Specify Paimon Jar File\nAppend path to paimon jar file to the --jars argument when starting spark-sql.\nspark-sql ... --jars /path/to/paimon-spark-3.3-0.5-SNAPSHOT.jar Alternatively, you can copy paimon-spark-3.3-0.5-SNAPSHOT.jar under spark/jars in your Spark installation directory.\nStep 2: Specify Paimon Catalog\nCatalog When starting spark-sql, use the following command to register Paimon’s Spark catalog with the name paimon. Table files of the warehouse is stored under /tmp/paimon.\nspark-sql ... \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=file:/tmp/paimon Catalogs are configured using properties under spark.sql.catalog.(catalog_name). In above case, \u0026lsquo;paimon\u0026rsquo; is the catalog name, you can change it to your own favorite catalog name.\nAfter spark-sql command line has started, run the following SQL to create and switch to database default.\nUSE paimon; USE default; After switching to the catalog ('USE paimon'), Spark\u0026rsquo;s existing tables will not be directly accessible, you can use the spark_catalog.${database_name}.${table_name} to access Spark tables.\nGeneric Catalog When starting spark-sql, use the following command to register Paimon’s Spark Generic catalog to replace Spark default catalog spark_catalog. (default warehouse is Spark spark.sql.warehouse.dir)\nCurrently, it is only recommended to use SparkGenericCatalog in the case of Hive metastore, Paimon will infer Hive conf from Spark session, you just need to configure Spark\u0026rsquo;s Hive conf.\nspark-sql ... \\  --conf spark.sql.catalog.spark_catalog=org.apache.paimon.spark.SparkGenericCatalog Using SparkGenericCatalog, you can use Paimon tables in this Catalog or non-Paimon tables such as Spark\u0026rsquo;s csv, parquet, Hive tables, etc.\n Step 3: Create a table and Write Some Records\nCatalog create table my_table ( k int, v string ) tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; ); INSERT INTO my_table VALUES (1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;); Generic Catalog create table my_table ( k int, v string ) USING paimon tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; ) ; INSERT INTO my_table VALUES (1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;);  Step 4: Query Table with SQL\nSELECT * FROM my_table; /* 1\tHi 2\tHello */ Step 5: Update the Records\nINSERT INTO my_table VALUES (1, \u0026#39;Hi Again\u0026#39;), (3, \u0026#39;Test\u0026#39;); SELECT * FROM my_table; /* 1\tHi Again 2\tHello 3\tTest */ Step 6: Query Table with Scala API\nIf you don\u0026rsquo;t want to use Paimon catalog, you can also run spark-shell and query the table with Scala API.\nspark-shell ... --jars /path/to/paimon-spark-3.3-0.5-SNAPSHOT.jar val dataset = spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;file:/tmp/paimon/default.db/my_table\u0026#34;) dataset.createOrReplaceTempView(\u0026#34;my_table\u0026#34;) spark.sql(\u0026#34;SELECT * FROM my_table\u0026#34;).show() Spark Type Conversion #  This section lists all supported type conversion between Spark and Paimon. All Spark\u0026rsquo;s data types are available in package org.apache.spark.sql.types.\n  Spark Data Type Paimon Data Type Atomic Type     StructType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   ByteType TinyIntType true   ShortType SmallIntType true   IntegerType IntType true   LongType BigIntType true   FloatType FloatType true   DoubleType DoubleType true   StringType VarCharType, CharType true   DateType DateType true   TimestampType TimestampType, LocalZonedTimestamp true   DecimalType(precision, scale) DecimalType(precision, scale) true   BinaryType VarBinaryType, BinaryType true     Currently, Spark\u0026rsquo;s field comment cannot be described under Flink CLI. Conversion between Spark\u0026rsquo;s UserDefinedType and Paimon\u0026rsquo;s UserDefinedType is not supported.   "});index.add({'id':23,'href':'/docs/master/concepts/file-operations/','title':"File Operations",'section':"Concepts",'content':"File Operations #  This article is specifically designed to clarify the impact that various file operations have on files.\nThis page provides concrete examples and practical tips for effectively managing them. Furthermore, through an in-depth exploration of operations such as commit and compact, we aim to offer insights into the creation and updates of files.\nPrerequisite #  Before delving further into this page, please ensure that you have read through the following sections:\n Basic Concepts, File Layouts and How to use Paimon in Flink.  Create Catalog #  Start Flink SQL client via ./sql-client.sh and execute the following statements one by one to create a Paimon catalog.\nCREATE CATALOG paimon WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;file:///tmp/paimon\u0026#39; ); USE CATALOG paimon; This will only create a directory at given path file:///tmp/paimon.\nCreate Table #  Execute the following create table statement will create a Paimon table with 3 fields:\nCREATE TABLE T ( id BIGINT, a INT, b STRING, dt STRING COMMENT \u0026#39;timestamp string in format yyyyMMdd\u0026#39;, PRIMARY KEY(id, dt) NOT ENFORCED ) PARTITIONED BY (dt); This will create Paimon table T under the path /tmp/paimon/default.db/T, with its schema stored in /tmp/paimon/default.db/T/schema/schema-0\nInsert Records Into Table #  Run the following insert statement in Flink SQL:\nINSERT INTO T VALUES (1, 10001, \u0026#39;varchar00001\u0026#39;, \u0026#39;20230501\u0026#39;); Once the Flink job is completed, the records are written to the Paimon table through a successful commit. Users can verify the visibility of these records by executing the query SELECT * FROM T which will return a single row. The commit process creates a snapshot located at the path /tmp/paimon/default.db/T/snapshot/snapshot-1. The resulting file layout at snapshot-1 is as described below:\nThe content of snapshot-1 contains metadata of the snapshot, such as manifest list and schema id:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 1, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;7d758485-981d-4b1a-a0c6-d34c3eb254bf\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;APPEND\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684155393354, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 1, \u0026#34;deltaRecordCount\u0026#34; : 1, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } Remind that a manifest list contains all changes of the snapshot, baseManifestList is the base file upon which the changes in deltaManifestList is applied. The first commit will result in 1 manifest file, and 2 manifest lists are created (the file names might differ from those in your experiment):\n./T/manifest: manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\tmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 is the manifest file (manifest-1-0 in the above graph), which stores the information about the data files in the snapshot.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 is the baseManifestList (manifest-list-1-base in the above graph), which is effectively empty.\nmanifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1 is the deltaManifestList (manifest-list-1-delta in the above graph), which contains a list of manifest entries that perform operations on data files, which, in this case, is manifest-1-0.\nNow let\u0026rsquo;s insert a batch of records across different partitions and see what happens. In Flink SQL, execute the following statement:\nINSERT INTO T VALUES (2, 10002, \u0026#39;varchar00002\u0026#39;, \u0026#39;20230502\u0026#39;), (3, 10003, \u0026#39;varchar00003\u0026#39;, \u0026#39;20230503\u0026#39;), (4, 10004, \u0026#39;varchar00004\u0026#39;, \u0026#39;20230504\u0026#39;), (5, 10005, \u0026#39;varchar00005\u0026#39;, \u0026#39;20230505\u0026#39;), (6, 10006, \u0026#39;varchar00006\u0026#39;, \u0026#39;20230506\u0026#39;), (7, 10007, \u0026#39;varchar00007\u0026#39;, \u0026#39;20230507\u0026#39;), (8, 10008, \u0026#39;varchar00008\u0026#39;, \u0026#39;20230508\u0026#39;), (9, 10009, \u0026#39;varchar00009\u0026#39;, \u0026#39;20230509\u0026#39;), (10, 10010, \u0026#39;varchar00010\u0026#39;, \u0026#39;20230510\u0026#39;); The second commit takes place and executing SELECT * FROM T will return 10 rows. A new snapshot, namely snapshot-2, is created and gives us the following physical file layout:\n% ls -atR . ./T: dt=20230501 dt=20230502\tdt=20230503\tdt=20230504\tdt=20230505\tdt=20230506\tdt=20230507\tdt=20230508\tdt=20230509\tdt=20230510\tsnapshot schema manifest ./T/snapshot: LATEST snapshot-2 EARLIEST snapshot-1 ./T/manifest: manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-1\t# delta manifest list for snapshot-2 manifest-list-9ac2-5e79-4978-a3bc-86c25f1a303f-0 # base manifest list for snapshot-2\t manifest-f1267033-e246-4470-a54c-5c27fdbdd074-0\t# manifest file for snapshot-2 manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-1\t# delta manifest list for snapshot-1  manifest-list-4ccc-c07f-4090-958c-cfe3ce3889e5-0 # base manifest list for snapshot-1 manifest-2b833ea4-d7dc-4de0-ae0d-ad76eced75cc-0 # manifest file for snapshot-1 ./T/dt=20230501/bucket-0: data-b75b7381-7c8b-430f-b7e5-a204cb65843c-0.orc ... # each partition has the data written to bucket-0 ... ./T/schema: schema-0 The new file layout as of snapshot-2 looks like Delete Records From Table #  Now let\u0026rsquo;s delete records that meet the condition dt\u0026gt;=20230503. In Flink SQL, execute the following statement:\nBatch DELETE FROM T WHERE dt \u0026gt;= \u0026#39;20230503\u0026#39;; The third commit takes place and it gives us snapshot-3. Now, listing the files under the table and your will find out no partition is dropped. Instead, a new data file is created for partition 20230503 to 20230510:\n./T/dt=20230510/bucket-0: data-b93f468c-b56f-4a93-adc4-b250b3aa3462-0.orc # newer data file created by the delete statement  data-0fcacc70-a0cb-4976-8c88-73e92769a762-0.orc # older data file created by the insert statement This make sense since we insert a record in the second commit (represented by +I[10, 10010, 'varchar00010', '20230510']) and then delete the record in the third commit. Executing SELECT * FROM T will return 2 rows, namely:\n+I[1, 10001, 'varchar00001', '20230501'] +I[2, 10002, 'varchar00002', '20230502'] The new file layout as of snapshot-3 looks like Note that manifest-3-0 contains 8 manifest entries of ADD operation type, corresponding to 8 newly written data files.\nCompact Table #  As you may have noticed, the number of small files will augment over successive snapshots, which may lead to decreased read performance. Therefore, a full-compaction is needed in order to reduce the number of small files.\nLet\u0026rsquo;s trigger the full-compaction now, and run a dedicated compaction job through flink run:\nBatch \u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition-name\u0026gt;] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\ an example would be (suppose you\u0026rsquo;re already in Flink home)\n./bin/flink run \\  ./lib/paimon-flink-action-0.5-SNAPSHOT.jar \\  compact \\  --path file:///tmp/paimon/default.db/T All current table files will be compacted and a new snapshot, namely snapshot-4, is made and contains the following information:\n{ \u0026#34;version\u0026#34; : 3, \u0026#34;id\u0026#34; : 4, \u0026#34;schemaId\u0026#34; : 0, \u0026#34;baseManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-0\u0026#34;, \u0026#34;deltaManifestList\u0026#34; : \u0026#34;manifest-list-9be16-82e7-4941-8b0a-7ce1c1d0fa6d-1\u0026#34;, \u0026#34;changelogManifestList\u0026#34; : null, \u0026#34;commitUser\u0026#34; : \u0026#34;a3d951d5-aa0e-4071-a5d4-4c72a4233d48\u0026#34;, \u0026#34;commitIdentifier\u0026#34; : 9223372036854775807, \u0026#34;commitKind\u0026#34; : \u0026#34;COMPACT\u0026#34;, \u0026#34;timeMillis\u0026#34; : 1684163217960, \u0026#34;logOffsets\u0026#34; : { }, \u0026#34;totalRecordCount\u0026#34; : 38, \u0026#34;deltaRecordCount\u0026#34; : 20, \u0026#34;changelogRecordCount\u0026#34; : 0, \u0026#34;watermark\u0026#34; : -9223372036854775808 } The new file layout as of snapshot-4 looks like Note that manifest-4-0 contains 20 manifest entries (18 DELETE operations and 2 ADD operations)\n For partition 20230503 to 20230510, two DELETE operations for two data files For partition 20230501 to 20230502, one DELETE operation and one ADD operation for the same data file.  Alter Table #  Execute the following statement to configure full-compaction:\nALTER TABLE T SET (\u0026#39;full-compaction.delta-commits\u0026#39; = \u0026#39;1\u0026#39;); It will create a new schema for Paimon table, namely schema-1, but no snapshot has actually used this schema yet until the next commit.\nExpire Snapshots #  Remind that the marked data files are not truly deleted until the snapshot expires and no consumer depends on the snapshot. For more information, see Expiring Snapshots.\nDuring the process of snapshot expiration, the range of snapshots is initially determined, and then data files within these snapshots are marked for deletion. A data file is marked for deletion only when there is a manifest entry of kind DELETE that references that specific data file. This marking ensures that the file will not be utilized by subsequent snapshots and can be safely removed.\nLet\u0026rsquo;s say all 4 snapshots in the above diagram are about to expire. The expire process is as follows:\n  It first deletes all marked data files, and records any changed buckets.\n  It then deletes any changelog files and associated manifests.\n  Finally, it deletes the snapshots themselves and writes the earliest hint file.\n  If any directories are left empty after the deletion process, they will be deleted as well.\nLet\u0026rsquo;s say another snapshot, snapshot-5 is created and snapshot expiration is triggered. snapshot-1 to snapshot-4 are\nto be deleted. For simplicity, we will only focus on files from previous snapshots, the final layout after snapshot expiration looks like:\nAs a result, partition 20230503 to 20230510 are physically deleted.\nFlink Stream Write #  Finally, we will examine Flink Stream Write by utilizing the example of CDC ingestion. This section will address the capturing and writing of change data into Paimon, as well as the mechanisms behind asynchronous compact and snapshot commit and expiration.\nTo begin, let\u0026rsquo;s take a closer look at the CDC ingestion workflow and the unique roles played by each component involved.\n MySQL CDC Source uniformly reads snapshot and incremental data, with SnapshotReader reading snapshot data and BinlogReader reading incremental data, respectively. Paimon Sink writes data into Paimon table in bucket level. The CompactManager within it will trigger compaction asynchronously. Committer Operator is a singleton responsible for committing and expiring snapshots.  Next, we will go over end-to-end data flow.\nMySQL Cdc Source read snapshot and incremental data and emit them to downstream after normalization.\nPaimon Sink first buffers new records in a heap-based LSM tree, and flushes them to disk when the memory buffer is full. Note that each data file written is a sorted run. At this point, no manifest file and snapshot is created. Right before Flink checkpoint takes places, Paimon Sink will flush all buffered records and send committable message to downstream, which is read and committed by Committer Operator during checkpoint.\nDuring checkpoint, Committer Operator will create a new snapshot and associate it with manifest lists so that the snapshot\ncontains information about all data files in the table.\nAt later point asynchronous compaction might take place, and the committable produced by CompactManager contains information about previous files and merged files so that Committer Operator can construct corresponding manifest entries. In this case Committer Operator might produce two snapshot during Flink checkpoint, one for data written (snapshot of kind Append) and the other for compact (snapshot of kind Compact). If no data file is written during checkpoint interval, only snapshot of kind Compact will be created. Committer Operator will check against snapshot expiration and perform physical deletion of marked data files.\n"});index.add({'id':24,'href':'/docs/master/how-to/','title':"How to",'section':"Apache Paimon",'content':""});index.add({'id':25,'href':'/docs/master/maintenance/manage-snapshots/','title':"Manage Snapshots",'section':"Maintenance",'content':"Manage Snapshots #  This section will describe the management and behavior related to snapshots.\nExpire Snapshots #  Paimon writers generate one or two snapshots per commit. Each snapshot may add some new data files or mark some old data files as deleted. However, the marked data files are not truly deleted because Paimon also supports time traveling to an earlier snapshot. They are only deleted when the snapshot expires.\nCurrently, expiration is automatically performed by Paimon writers when committing new changes. By expiring old snapshots, old data files and metadata files that are no longer used can be deleted to release disk space.\nSnapshot expiration is controlled by the following table properties.\n  Option Required Default Type Description     snapshot.time-retained No 1 h Duration The maximum time of completed snapshots to retain.   snapshot.num-retained.min No 10 Integer The minimum number of completed snapshots to retain. Should be greater than or equal to 1.   snapshot.num-retained.max No Integer.MAX_VALUE Integer The maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.    When the number of snapshots is less than snapshot.num-retained.min, no snapshots will be expired(even the condition snapshot.time-retained meet), after which snapshot.num-retained.max and snapshot.time-retained will be used to control the snapshot expiration until the remaining snapshot meets the condition.\nThe following example show more details(snapshot.num-retained.min is 2, snapshot.time-retained is 1h, snapshot.num-retained.max is 5):\n snapshot item is described using tuple (snapshotId, corresponding time)\n    New Snapshots All snapshots after expiration check explanation      (snapshots-1, 2023-07-06 10:00)   (snapshots-1, 2023-07-06 10:00)   No snapshot expired     (snapshots-2, 2023-07-06 10:20)   (snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20)   No snapshot expired     (snapshots-3, 2023-07-06 10:40)   (snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40)   No snapshot expired     (snapshots-4, 2023-07-06 11:00)   (snapshots-1, 2023-07-06 10:00) (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00)   No snapshot expired     (snapshots-5, 2023-07-06 11:20)   (snapshots-2, 2023-07-06 10:20) (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20)   snapshot-1 was expired because the condition `snapshot.time-retained` is not met     (snapshots-6, 2023-07-06 11:30)   (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30)   snapshot-2 was expired because the condition `snapshot.time-retained` is not met     (snapshots-7, 2023-07-06 11:35)   (snapshots-3, 2023-07-06 10:40) (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35)   No snapshot expired     (snapshots-8, 2023-07-06 11:36)   (snapshots-4, 2023-07-06 11:00) (snapshots-5, 2023-07-06 11:20) (snapshots-6, 2023-07-06 11:30) (snapshots-7, 2023-07-06 11:35) (snapshots-8, 2023-07-06 11:36)   snapshot-3 was expired because the condition `snapshot.num-retained.max` is not met     Please note that too short retain time or too small retain number may result in:\n Batch queries cannot find the file. For example, the table is relatively large and the batch query takes 10 minutes to read, but the snapshot from 10 minutes ago expires, at which point the batch query will read a deleted snapshot. Streaming reading jobs on table files (without the external log system) fail to restart. When the job restarts, the snapshot it recorded may have expired. (You can use Consumer Id to protect streaming reading in a small retain time of snapshot expiration).  Rollback to Snapshot #  Rollback a table to a specific snapshot ID.\nFlink Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  rollback-to \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --snapshot \u0026lt;snapshot-id\u0026gt; \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback:  // snapshot-3  // snapshot-4  // snapshot-5  // snapshot-6  // snapshot-7  table.rollbackTo(5); // after rollback:  // snapshot-3  // snapshot-4  // snapshot-5  } } Spark Run the following sql:\nCALL rollback(table =\u0026gt; \u0026#39;test.T\u0026#39;, version =\u0026gt; \u0026#39;2\u0026#39;);  "});index.add({'id':26,'href':'/docs/master/filesystems/s3/','title':"S3",'section':"Filesystems",'content':"S3 #  Download paimon-s3-0.5-SNAPSHOT.jar. Flink If you have already configured s3 access through Flink (Via Flink FileSystem), here you can skip the following configuration.  Put paimon-s3-0.5-SNAPSHOT.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;s3://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt;\u0026#39;, \u0026#39;s3.endpoint\u0026#39; = \u0026#39;your-endpoint-hostname\u0026#39;, \u0026#39;s3.access-key\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;s3.secret-key\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark If you have already configured s3 access through Spark (Via Hadoop FileSystem), here you can skip the following configuration.  Place paimon-s3-0.5-SNAPSHOT.jar together with paimon-spark-0.5-SNAPSHOT.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.paimon=org.apache.paimon.spark.SparkCatalog \\  --conf spark.sql.catalog.paimon.warehouse=s3://\u0026lt;bucket\u0026gt;/\u0026lt;path\u0026gt; \\  --conf spark.sql.catalog.paimon.s3.endpoint=your-endpoint-hostname \\  --conf spark.sql.catalog.paimon.s3.access-key=xxx \\  --conf spark.sql.catalog.paimon.s3.secret-key=yyy Hive If you have already configured s3 access through Hive ((Via Hadoop FileSystem)), here you can skip the following configuration.  NOTE: You need to ensure that Hive metastore can access s3.\nPlace paimon-s3-0.5-SNAPSHOT.jar together with paimon-hive-connector-0.5-SNAPSHOT.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET paimon.s3.endpoint=your-endpoint-hostname; SET paimon.s3.access-key=xxx; SET paimon.s3.secret-key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino Place paimon-s3-0.5-SNAPSHOT.jar together with paimon-trino-0.5-SNAPSHOT.jar under plugin/paimon directory.\nAdd options in etc/catalog/paimon.properties.\ns3.endpoint=your-endpoint-hostname s3.access-key=xxx s3.secret-key=yyy  S3 Complaint Object Stores #  The S3 Filesystem also support using S3 compliant object stores such as MinIO, Tencent\u0026rsquo;s COS and IBM’s Cloud Object Storage. Just configure your endpoint to the provider of the object store service.\ns3.endpoint:your-endpoint-hostnameConfigure Path Style Access #  Some S3 compliant object stores might not have virtual host style addressing enabled by default, for example when using Standalone MinIO for testing purpose. In such cases, you will have to provide the property to enable path style access.\ns3.path.style.access:trueS3A Performance #  Tune Performance for S3AFileSystem.\nIf you encounter the following exception:\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool. Try to configure this in catalog options: fs.s3a.connection.maximum=1000.\n"});index.add({'id':27,'href':'/docs/master/engines/spark2/','title':"Spark2",'section':"Engines",'content':"Spark2 #  This documentation is a guide for using Paimon in Spark2.\nVersion #  Paimon supports Spark 2.4+. It is highly recommended to use Spark 2.4+ version with much improvement.\nPreparing Paimon Jar File #  Download paimon-spark-2-0.5-SNAPSHOT.jar. You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests You can find the bundled jar in ./paimon-spark/paimon-spark-2/target/paimon-spark-2-0.5-SNAPSHOT.jar.\nQuick Start #  If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.  Step 1: Prepare Test Data\nPaimon currently only supports reading tables through Spark2. To create a Paimon table with records, please follow our Flink quick start guide.\nAfter the guide, all table files should be stored under the path /tmp/paimon, or the warehouse path you\u0026rsquo;ve specified.\nStep 2: Specify Paimon Jar File\nYou can append path to paimon jar file to the --jars argument when starting spark-shell.\nspark-shell ... --jars /path/to/paimon-spark-2-0.5-SNAPSHOT.jar Alternatively, you can copy paimon-spark-2-0.5-SNAPSHOT.jar under spark/jars in your Spark installation directory.\nStep 3: Query Table\nPaimon with Spark 2.4 does not support DDL. You can use the Dataset reader and register the Dataset as a temporary table. In spark shell:\nval dataset = spark.read.format(\u0026#34;paimon\u0026#34;).load(\u0026#34;file:/tmp/paimon/default.db/word_count\u0026#34;) dataset.createOrReplaceTempView(\u0026#34;word_count\u0026#34;) spark.sql(\u0026#34;SELECT * FROM word_count\u0026#34;).show() "});index.add({'id':28,'href':'/docs/master/how-to/writing-tables/','title':"Writing Tables",'section':"How to",'content':"Writing Tables #  You can use the INSERT statement to inserts new rows into a table or overwrites the existing data in the table. The inserted rows can be specified by value expressions or result from a query.\nSyntax #  INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query };   part_spec\nAn optional parameter that specifies a comma-separated list of key and value pairs for partitions. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.\nSyntax: PARTITION ( partition_col_name = partition_col_val [ , \u0026hellip; ] )\n  column_list\nAn optional parameter that specifies a comma-separated list of columns belonging to the table_identifier table.\nSyntax: (col_name1 [, column_name2, \u0026hellip;])\nAll specified columns should exist in the table and not be duplicated from each other. It includes all columns except the static partition columns. The size of the column list should be exactly the size of the data from VALUES clause or query.     value_expr\nSpecifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted. A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows.\nSyntax: VALUES ( { value | NULL } [ , … ] ) [ , ( … ) ]\nCurrently, Flink doesn't support use NULL directly, so the NULL should be cast to actual data type by `CAST (NULL AS data_type)`.     For more information, please check the syntax document:\nFlink INSERT Statement\nSpark INSERT Statement\nWrite Nullable field to Not-null field #  We cannot insert into a non-null column of one table with a nullable column of another table. Assume that, we have a column key1 in table A which is primary key, primary key cannot be null. We have a column key2 in table B, which is nullable. If we run a sql like this:\nINSERT INTO A key1 SELECT key2 FROM B We will catch an exception,\n In spark: \u0026ldquo;Cannot write nullable values to non-null column \u0026lsquo;key1\u0026rsquo;.\u0026rdquo; In flink: \u0026ldquo;Column \u0026lsquo;key1\u0026rsquo; is NOT NULL, however, a null value is being written into it. \u0026quot;  Other engines will throw respective exception to announce this. We can use function \u0026ldquo;NVL\u0026rdquo; or \u0026ldquo;COALESCE\u0026rdquo; to work around, turn a nullable column into a non-null column to escape exception:\nINSERT INTO A key1 SELECT COALESCE(key2, \u0026lt;non-null expression\u0026gt;) FROM B; Applying Records/Changes to Tables #  Flink Use INSERT INTO to apply records and changes to tables.\nINSERT INTO MyTable SELECT ... Paimon supports shuffle data by partition and bucket in sink phase.\nSpark3 Use INSERT INTO to apply records and changes to tables.\nINSERT INTO MyTable SELECT ...  Overwriting the Whole Table #  For unpartitioned tables, Paimon supports overwriting the whole table.\nUse INSERT OVERWRITE to overwrite the whole unpartitioned table.\nFlink INSERT OVERWRITE MyTable SELECT ... Spark INSERT OVERWRITE MyTable SELECT ...  Overwriting a Partition #  For partitioned tables, Paimon supports overwriting a partition.\nUse INSERT OVERWRITE to overwrite a partition.\nFlink INSERT OVERWRITE MyTable PARTITION (key1 = value1, key2 = value2, ...) SELECT ... Spark INSERT OVERWRITE MyTable PARTITION (key1 = value1, key2 = value2, ...) SELECT ...  Dynamic Overwrite #  Flink Flink\u0026rsquo;s default overwrite mode is dynamic partition overwrite (that means Paimon only deletes the partitions appear in the overwritten data). You can configure dynamic-partition-overwrite to change it to static overwritten.\n-- MyTable is a Partitioned Table  -- Dynamic overwrite INSERT OVERWRITE MyTable SELECT ... -- Static overwrite (Overwrite whole table) INSERT OVERWRITE MyTable /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39; = \u0026#39;false\u0026#39;) */ SELECT ... Spark Spark\u0026rsquo;s default overwrite mode is static partition overwrite. To enable dynamic overwritten needs these configs below:\n--conf spark.sql.catalog.spark_catalog=org.apache.paimon.spark.SparkGenericCatalog --conf spark.sql.extensions=org.apache.paimon.spark.PaimonSparkSessionExtension -- MyTable is a Partitioned Table  -- Static overwrite (Overwrite whole table) INSERT OVERWRITE MyTable SELECT ... -- Dynamic overwrite SET spark.sql.sources.partitionOverwriteMode=dynamic; INSERT OVERWRITE MyTable SELECT ...  Truncate tables #  Flink You can use INSERT OVERWRITE to purge tables by inserting empty value.\nINSERT OVERWRITE MyTable /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ SELECT * FROM MyTable WHERE false; Spark TRUNCATE TABLE MyTable;  Purging Partitions #  Currently, Paimon supports two ways to purge partitions.\n  Like purging tables, you can use INSERT OVERWRITE to purge data of partitions by inserting empty value to them.\n  Method #1 does not support to drop multiple partitions. In case that you need to drop multiple partitions, you can submit the drop-partition job through flink run.\n  Flink -- Syntax INSERT OVERWRITE MyTable /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (key1 = value1, key2 = value2, ...) SELECT selectSpec FROM MyTable WHERE false; -- The following SQL is an example: -- table definition CREATE TABLE MyTable ( k0 INT, k1 INT, v STRING ) PARTITIONED BY (k0, k1); -- you can use INSERT OVERWRITE MyTable /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (k0 = 0) SELECT k1, v FROM MyTable WHERE false; -- or INSERT OVERWRITE MyTable /*+ OPTIONS(\u0026#39;dynamic-partition-overwrite\u0026#39;=\u0026#39;false\u0026#39;) */ PARTITION (k0 = 0, k1 = 0) SELECT v FROM MyTable WHERE false; Flink Job Run the following command to submit a drop-partition job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  drop-partition \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition \u0026lt;partition_spec\u0026gt; [--partition \u0026lt;partition_spec\u0026gt; ...]] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] partition_spec: key1=value1,key2=value2... For more information of drop-partition, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  drop-partition --help  Updating tables #  Currently, Paimon supports updating records by using UPDATE in Flink 1.17 and later versions. You can perform UPDATE in Flink\u0026rsquo;s batch mode.\nImportant table properties setting:\n Only primary key table supports this feature. MergeEngine needs to be deduplicate or partial-update to support this feature.   Warning: we do not support updating primary keys.  Flink -- Syntax UPDATE table_identifier SET column1 = value1, column2 = value2, ... WHERE condition; -- The following SQL is an example: -- table definition CREATE TABLE MyTable ( a STRING, b INT, c INT, PRIMARY KEY (a) NOT ENFORCED ) WITH ( \u0026#39;write-mode\u0026#39; = \u0026#39;change-log\u0026#39;, \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use UPDATE MyTable SET b = 1, c = 2 WHERE a = \u0026#39;myTable\u0026#39;;  Deleting from table #  Flink 1.16- In Flink 1.16 and previous versions, Paimon only supports deleting records via submitting the \u0026lsquo;delete\u0026rsquo; job through flink run.\nRun the following command to submit a \u0026lsquo;delete\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  delete \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --where \u0026lt;filter_spec\u0026gt; \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] filter_spec is equal to the \u0026#39;WHERE\u0026#39; clause in SQL DELETE statement. Examples: age \u0026gt;= 18 AND age \u0026lt;= 60 animal \u0026lt;\u0026gt; \u0026#39;cat\u0026#39; id \u0026gt; (SELECT count(*) FROM employee) For more information of \u0026lsquo;delete\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  delete --help Flink 1.17\u0026#43; Important table properties setting:\n Only tables whose write-mode is set to change-log supports this feature. If the table has primary keys, MergeEngine needs to be deduplicate to support this feature.   Warning: we do not support deleting from table in streaming mode.  -- Syntax DELETE FROM table_identifier WHERE conditions; -- The following SQL is an example: -- table definition CREATE TABLE MyTable ( id BIGINT NOT NULL, currency STRING, rate BIGINT, dt String, PRIMARY KEY (id, dt) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;write-mode\u0026#39; = \u0026#39;change-log\u0026#39;, \u0026#39;merge-engine\u0026#39; = \u0026#39;deduplicate\u0026#39; ); -- you can use DELETE FROM MyTable WHERE currency = \u0026#39;UNKNOWN\u0026#39;; Spark Spark DELETE currently supports only a single point execution, for deleting small amounts of data.\nDELETE FROM MyTable WHERE currency = \u0026#39;UNKNOWN\u0026#39;;  Merging into table #  Paimon supports \u0026ldquo;MERGE INTO\u0026rdquo; via submitting the \u0026lsquo;merge-into\u0026rsquo; job through flink run.\nImportant table properties setting:\n Only primary key table supports this feature. The action won\u0026rsquo;t produce UPDATE_BEFORE, so it\u0026rsquo;s not recommended to set \u0026lsquo;changelog-producer\u0026rsquo; = \u0026lsquo;input\u0026rsquo;.   The design referenced such syntax:\nMERGE INTO target-table USING source-table | source-expr AS source-alias ON merge-condition WHEN MATCHED [AND matched-condition] THEN UPDATE SET xxx WHEN MATCHED [AND matched-condition] THEN DELETE WHEN NOT MATCHED [AND not-matched-condition] THEN INSERT VALUES (xxx) WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN UPDATE SET xxx WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN DELETE The merge-into action use \u0026ldquo;upsert\u0026rdquo; semantics instead of \u0026ldquo;update\u0026rdquo;, which means if the row exists, then do update, else do insert. For example, for non-primary-key table, you can update every column, but for primary key table, if you want to update primary keys, you have to insert a new row which has different primary keys from rows in the table. In this scenario, \u0026ldquo;upsert\u0026rdquo; is useful.\nFlink Job Run the following command to submit a \u0026lsquo;merge-into\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;target-table\u0026gt; \\  [--target-as \u0026lt;target-table-alias\u0026gt;] \\  --source-table \u0026lt;source-table-name\u0026gt; \\  [--source-sql \u0026lt;sql\u0026gt; ...]\\  --on \u0026lt;merge-condition\u0026gt; \\  --merge-actions \u0026lt;matched-upsert,matched-delete,not-matched-insert,not-matched-by-source-upsert,not-matched-by-source-delete\u0026gt; \\  --matched-upsert-condition \u0026lt;matched-condition\u0026gt; \\  --matched-upsert-set \u0026lt;upsert-changes\u0026gt; \\  --matched-delete-condition \u0026lt;matched-condition\u0026gt; \\  --not-matched-insert-condition \u0026lt;not-matched-condition\u0026gt; \\  --not-matched-insert-values \u0026lt;insert-values\u0026gt; \\  --not-matched-by-source-upsert-condition \u0026lt;not-matched-by-source-condition\u0026gt; \\  --not-matched-by-source-upsert-set \u0026lt;not-matched-upsert-changes\u0026gt; \\  --not-matched-by-source-delete-condition \u0026lt;not-matched-by-source-condition\u0026gt; \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] You can pass sqls by \u0026#39;--source-sql \u0026lt;sql\u0026gt; [, --source-sql \u0026lt;sql\u0026gt; ...]\u0026#39; to config environment and create source table at runtime. -- Examples: -- Find all orders mentioned in the source table, then mark as important if the price is above 100 -- or delete if the price is under 10. ./flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions \\  matched-upsert,matched-delete \\  --matched-upsert-condition \u0026#34;T.price \u0026gt; 100\u0026#34; \\  --matched-upsert-set \u0026#34;mark = \u0026#39;important\u0026#39;\u0026#34; \\  --matched-delete-condition \u0026#34;T.price \u0026lt; 10\u0026#34; -- For matched order rows, increase the price, and if there is no match, insert the order from the -- source table: ./flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions \\  matched-upsert,not-matched-insert \\  --matched-upsert-set \u0026#34;price = T.price + 20\u0026#34; \\  --not-matched-insert-values * -- For not matched by source order rows (which are in the target table and does not match any row in the -- source table based on the merge-condition), decrease the price or if the mark is \u0026#39;trivial\u0026#39;, delete them: ./flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions \\  not-matched-by-source-upsert,not-matched-by-source-delete \\  --not-matched-by-source-upsert-condition \u0026#34;T.mark \u0026lt;\u0026gt; \u0026#39;trivial\u0026#39;\u0026#34; \\  --not-matched-by-source-upsert-set \u0026#34;price = T.price - 20\u0026#34; \\  --not-matched-by-source-delete-condition \u0026#34;T.mark = \u0026#39;trivial\u0026#39;\u0026#34; -- A --source-sql example: -- Create a temporary view S in new catalog and use it as source table ./flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-sql \u0026#34;CREATE CATALOG test_cat WITH (...)\u0026#34; \\  --source-sql \u0026#34;CREATE TEMPORARY VIEW test_cat.`default`.S AS SELECT order_id, price, \u0026#39;important\u0026#39; FROM important_order\u0026#34; \\  --source-table test_cat.default.S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions not-matched-insert\\  --not-matched-insert-values * The term \u0026lsquo;matched\u0026rsquo; explanation:\n matched: changed rows are from target table and each can match a source table row based on merge-condition and optional matched-condition (source ∩ target). not-matched: changed rows are from source table and all rows cannot match any target table row based on merge-condition and optional not-matched-condition (source - target). not-matched-by-source: changed rows are from target table and all row cannot match any source table row based on merge-condition and optional not-matched-by-source-condition (target - source).  Parameters format:\n matched-upsert-changes:\ncol = \u0026lt;source-table\u0026gt;.col | expression [, \u0026hellip;] (Means setting \u0026lt;target-table\u0026gt;.col with given value. Do not add \u0026lsquo;\u0026lt;target-table\u0026gt;.\u0026rsquo; before \u0026lsquo;col\u0026rsquo;.)\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to set columns with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not-matched-upsert-changes is similar to matched-upsert-changes, but you cannot reference source table\u0026rsquo;s column or use \u0026lsquo;*\u0026rsquo;. insert-values:\ncol1, col2, \u0026hellip;, col_end\nMust specify values of all columns. For each column, you can reference \u0026lt;source-table\u0026gt;.col or use an expression.\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to insert with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not-matched-condition cannot use target table\u0026rsquo;s columns to construct condition expression. not-matched-by-source-condition cannot use source table\u0026rsquo;s columns to construct condition expression.   Target alias cannot be duplicated with existed table name. If the source table is not in the current catalog and current database, the source-table-name must be qualified (database.table or catalog.database.table if created a new catalog). For examples:\n(1) If source table \u0026lsquo;my_source\u0026rsquo; is in \u0026lsquo;my_db\u0026rsquo;, qualify it:\n--source-table \u0026ldquo;my_db.my_source\u0026rdquo;\n(2) Example for sqls:\nWhen sqls changed current catalog and database, it\u0026rsquo;s OK to not qualify the source table name:\n--source-sql \u0026ldquo;CREATE CATALOG my_cat WITH (\u0026hellip;)\u0026quot;\n--source-sql \u0026ldquo;USE CATALOG my_cat\u0026rdquo;\n--source-sql \u0026ldquo;CREATE DATABASE my_db\u0026rdquo;\n--source-sql \u0026ldquo;USE my_db\u0026rdquo;\n--source-sql \u0026ldquo;CREATE TABLE S \u0026hellip;\u0026quot;\n--source-table S\nbut you must qualify it in the following case:\n--source-sql \u0026ldquo;CREATE CATALOG my_cat WITH (\u0026hellip;)\u0026quot;\n--source-sql \u0026ldquo;CREATE TABLE my_cat.`default`.S \u0026hellip;\u0026quot;\n--source-table my_cat.default.S\nYou can use just \u0026lsquo;S\u0026rsquo; as source table name in following arguments. At least one merge action must be specified. If both matched-upsert and matched-delete actions are present, their conditions must both be present too (same to not-matched-by-source-upsert and not-matched-by-source-delete). Otherwise, all conditions are optional. All conditions, set changes and values should use Flink SQL syntax. To ensure the whole command runs normally in Shell, please quote them with \u0026quot;\u0026quot; to escape blank spaces and use \u0026lsquo;\\\u0026rsquo; to escape special characters in statement. For example:\n--source-sql \u0026ldquo;CREATE TABLE T (k INT) WITH (\u0026lsquo;special-key\u0026rsquo; = \u0026lsquo;123\\!')\u0026rdquo;   For more information of \u0026lsquo;merge-into\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  merge-into --help  "});index.add({'id':29,'href':'/docs/master/engines/hive/','title':"Hive",'section':"Engines",'content':"Hive #  This documentation is a guide for using Paimon in Hive.\nVersion #  Paimon currently supports Hive 3.1, 2.3, 2.2, 2.1 and 2.1-cdh-6.3.\nExecution Engine #  Paimon currently supports MR and Tez execution engine for Hive Read, and MR execution engine for Hive Write. Note If you use beeline, please restart the hive cluster.\nInstallation #  Download the jar file with corresponding version.\n    Jar     Hive 3.1 paimon-hive-connector-3.1-0.5-SNAPSHOT.jar   Hive 2.3 paimon-hive-connector-2.3-0.5-SNAPSHOT.jar   Hive 2.2 paimon-hive-connector-2.2-0.5-SNAPSHOT.jar   Hive 2.1 paimon-hive-connector-2.1-0.5-SNAPSHOT.jar   Hive 2.1-cdh-6.3 paimon-hive-connector-2.1-cdh-6.3-0.5-SNAPSHOT.jar    You can also manually build bundled jar from the source code.\nTo build from source code, clone the git repository.\nBuild bundled jar with the following command. mvn clean install -DskipTests\nYou can find Hive connector jar in ./paimon-hive/paimon-hive-connector-\u0026lt;hive-version\u0026gt;/target/paimon-hive-connector-\u0026lt;hive-version\u0026gt;-0.5-SNAPSHOT.jar.\nThere are several ways to add this jar to Hive.\n You can create an auxlib folder under the root directory of Hive, and copy paimon-hive-connector-0.5-SNAPSHOT.jar into auxlib. You can also copy this jar to a path accessible by Hive, then use add jar /path/to/paimon-hive-connector-0.5-SNAPSHOT.jar to enable paimon support in Hive. Note that this method is not recommended. If you\u0026rsquo;re using the MR execution engine and running a join statement, you may be faced with the exception org.apache.hive.com.esotericsoftware.kryo.kryoexception: unable to find class.  NOTE: If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.\nFlink SQL: with Paimon Hive Catalog #  By using paimon Hive catalog, you can create, drop, select and insert into paimon tables from Flink. These operations directly affect the corresponding Hive metastore. Tables created in this way can also be accessed directly from Hive.\nStep 1: Prepare Flink Hive Connector Bundled Jar\nSee creating a catalog with Hive metastore.\nStep 2: Create Test Data with Flink SQL\nExecute the following Flink SQL script in Flink SQL client to define a Paimon Hive catalog and create a table.\n-- Flink SQL CLI -- Define paimon Hive catalog  CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;paimon\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, -- \u0026#39;hive-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  -- \u0026#39;hadoop-conf-dir\u0026#39; = \u0026#39;...\u0026#39;, this is recommended in the kerberos environment  \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs:///path/to/table/store/warehouse\u0026#39; ); -- Use paimon Hive catalog  USE CATALOG my_hive; -- Create a table in paimon Hive catalog (use \u0026#34;default\u0026#34; database by default)  CREATE TABLE test_table ( a int, b string ); -- Insert records into test table  INSERT INTO test_table VALUES (1, \u0026#39;Table\u0026#39;), (2, \u0026#39;Store\u0026#39;); -- Read records from test table  SELECT * FROM test_table; /* +---+-------+ | a | b | +---+-------+ | 1 | Table | | 2 | Store | +---+-------+ */ Hive SQL: access Paimon Tables already in Hive metastore #  Run the following Hive SQL in Hive CLI to access the created table.\n-- Assume that paimon-hive-connector-\u0026lt;hive-version\u0026gt;-0.5-SNAPSHOT.jar is already in auxlib directory. -- List tables in Hive -- (you might need to switch to \u0026#34;default\u0026#34; database if you\u0026#39;re not there by default)  SHOW TABLES; /* OK test_table */ -- Read records from test_table  SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore */ -- Insert records into test table -- Note tez engine does not support hive write, only the hive engine is supported.  INSERT INTO test_table VALUES (3, \u0026#39;Paimon\u0026#39;); SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ -- time travel  SET paimon.scan.snapshot-id=1; SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ SET paimon.scan.snapshot-id=null; Hive SQL: create new Paimon Tables #  You can create new paimon tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that paimon-hive-connector-0.5-SNAPSHOT.jar is already in auxlib directory. -- Let\u0026#39;s create a new paimon table.  SET hive.metastore.warehouse.dir=warehouse_path; CREATE TABLE hive_test_table( a INT COMMENT \u0026#39;The a field\u0026#39;, b STRING COMMENT \u0026#39;The b field\u0026#39; ) STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39;; Hive SQL: access Paimon Tables by External Table #  To access existing paimon table, you can also register them as external tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that paimon-hive-connector-0.5-SNAPSHOT.jar is already in auxlib directory. -- Let\u0026#39;s use the test_table created in the above section. -- To create an external table, you don\u0026#39;t need to specify any column or table properties. -- Pointing the location to the path of table is enough.  CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; LOCATION \u0026#39;/path/to/table/store/warehouse/default.db/test_table\u0026#39;; -- In addition to the way setting location above, you can also place the location setting in TBProperties -- to avoid Hive accessing Paimon\u0026#39;s location through its own file system when creating tables. -- This method is effective in scenarios using Object storage,such as s3.  CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.paimon.hive.PaimonStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#39;paimon_location\u0026#39; =\u0026#39;s3://xxxxx/path/to/table/store/warehouse/default.db/test_table\u0026#39; ); -- Read records from external_test_table  SELECT a, b FROM external_test_table ORDER BY a; /* OK 1\tTable 2\tStore */ -- Insert records into test table  INSERT INTO external_test_table VALUES (3, \u0026#39;Paimon\u0026#39;); SELECT a, b FROM external_test_table ORDER BY a; /* OK 1\tTable 2\tStore 3\tPaimon */ Hive Type Conversion #  This section lists all supported type conversion between Hive and Paimon. All Hive\u0026rsquo;s data types are available in package org.apache.hadoop.hive.serde2.typeinfo.\n  Hive Data Type Paimon Data Type Atomic Type     StructTypeInfo RowType false   MapTypeInfo MapType false   ListTypeInfo ArrayType false   PrimitiveTypeInfo(\"boolean\") BooleanType true   PrimitiveTypeInfo(\"tinyint\") TinyIntType true   PrimitiveTypeInfo(\"smallint\") SmallIntType true   PrimitiveTypeInfo(\"int\") IntType true   PrimitiveTypeInfo(\"bigint\") BigIntType true   PrimitiveTypeInfo(\"float\") FloatType true   PrimitiveTypeInfo(\"double\") DoubleType true   BaseCharTypeInfo(\"char(%d)\") CharType(length) true   PrimitiveTypeInfo(\"string\") VarCharType(VarCharType.MAX_LENGTH) true   BaseCharTypeInfo(\"varchar(%d)\") VarCharType(length), length is less than VarCharType.MAX_LENGTH true   PrimitiveTypeInfo(\"date\") DateType true   TimestampType TimestampType true   DecimalTypeInfo(\"decimal(%d, %d)\") DecimalType(precision, scale) true   DecimalTypeInfo(\"binary\") VarBinaryType, BinaryType true    "});index.add({'id':30,'href':'/docs/master/maintenance/manage-partition/','title':"Manage Partition",'section':"Maintenance",'content':"Expiring Partitions #  You can set partition.expiration-time when creating a partitioned table. Paimon will periodically check the status of partitions and delete expired partitions according to time.\nHow to determine whether a partition has expired: compare the time extracted from the partition with the current time to see if survival time has exceeded the partition.expiration-time.\nNote: After the partition expires, it is logically deleted and the latest snapshot cannot query its data. But the files in the file system are not immediately physically deleted, it depends on when the corresponding snapshot expires. See Expire Snapshots.  An example for single partition field:\nCREATE TABLE T (...) PARTITIONED BY (dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39; ); An example for multiple partition fields:\nCREATE TABLE T (...) PARTITIONED BY (other_key, dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39;, \u0026#39;partition.timestamp-pattern\u0026#39; = \u0026#39;$dt\u0026#39; ); More options:\n  Option Default Type Description     partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.    "});index.add({'id':31,'href':'/docs/master/engines/presto/','title':"Presto",'section':"Engines",'content':"Presto #  This documentation is a guide for using Paimon in Presto.\nVersion #  Paimon currently supports Presto 0.236 and above.\nPreparing Paimon Jar File #     Version Jar     [0.236, 0.268) paimon-presto-0.236-0.5-SNAPSHOT.jar   [0.268, 0.273) paimon-presto-0.268-0.5-SNAPSHOT.jar   [0.273, latest] paimon-presto-0.273-0.5-SNAPSHOT.jar    You can also manually build a bundled jar from the source code.\nTo build from the source code, clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests You can find Presto connector jar in ./paimon-presto-\u0026lt;presto-version\u0026gt;/target/paimon-presto-*.jar.\nThen, copy paimon-presto-*.jar and flink-shaded-hadoop-*-uber-*.jar to plugin/paimon.\nTmp Dir #  Paimon will unzip some jars to the tmp directory for codegen. By default, Presto will use '/tmp' as the temporary directory, but '/tmp' may be periodically deleted.\nYou can configure this environment variable when Presto starts:\n-Djava.io.tmpdir=/path/to/other/tmpdir Let Paimon use a secure temporary directory.\nConfigure Paimon Catalog #  Catalogs are registered by creating a catalog properties file in the etc/catalog directory. For example, create etc/catalog/paimon.properties with the following contents to mount the paimon connector as the paimon catalog:\nconnector.name=paimon warehouse=file:/tmp/warehouse If you are using HDFS, choose one of the following ways to configure your HDFS:\n set environment variable HADOOP_HOME. set environment variable HADOOP_CONF_DIR. configure fs.hdfs.hadoopconf in the properties.  Kerberos #  You can configure kerberos keytab file when using KERBEROS authentication in the properties.\nsecurity.kerberos.login.principal=hadoop-user security.kerberos.login.keytab=/etc/presto/hdfs.keytab Keytab files must be distributed to every node in the cluster that runs Presto.\nCreate Schema #  CREATE SCHEMA paimon.test_db; Create Table #  CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = 'ORC', primary_key = ARRAY['order_key','order_date'], partitioned_by = ARRAY['order_date'], bucket = '2', bucket_key = 'order_key', changelog_producer = 'input' ) Add Column #  CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = 'ORC', primary_key = ARRAY['order_key','order_date'], partitioned_by = ARRAY['order_date'], bucket = '2', bucket_key = 'order_key', changelog_producer = 'input' ) ALTER TABLE paimon.test_db.orders ADD COLUMN \u0026quot;shipping_address varchar; Query #  SELECT * FROM paimon.default.MyTable Presto to Paimon type mapping #  This section lists all supported type conversion between Presto and Paimon. All Presto\u0026rsquo;s data types are available in package  com.facebook.presto.common.type.\n  Presto Data Type Paimon Data Type Atomic Type     RowType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   TinyintType TinyIntType true   SmallintType SmallIntType true   IntegerType IntType true   BigintType BigIntType true   RealType FloatType true   DoubleType DoubleType true   CharType(length) CharType(length) true   VarCharType(VarCharType.MAX_LENGTH) VarCharType(VarCharType.MAX_LENGTH) true   VarCharType(length) VarCharType(length), length is less than VarCharType.MAX_LENGTH true   DateType DateType true   TimestampType TimestampType true   DecimalType(precision, scale) DecimalType(precision, scale) true   VarBinaryType(length) VarBinaryType(length) true   TimestampWithTimeZoneType LocalZonedTimestampType true   "});index.add({'id':32,'href':'/docs/master/how-to/querying-tables/','title':"Querying Tables",'section':"How to",'content':"Querying Tables #  Just like all other tables, Paimon tables can be queried with SELECT statement.\nBatch Query #  Paimon\u0026rsquo;s batch read returns all the data in a snapshot of the table. By default, batch reads return the latest snapshot.\n-- Flink SQL SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; Batch Time Travel #  Paimon batch reads with time travel can specify a snapshot or a tag and read the corresponding data.\nFlink -- read the snapshot with id 1L SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; -- read the snapshot from specified timestamp in unix milliseconds SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.tag-name\u0026#39; = \u0026#39;my-tag\u0026#39;) */; Spark3 Requires Spark 3.3+.\nyou can use VERSION AS OF and TIMESTAMP AS OF in query to do time travel:\n-- read the snapshot with id 1L (use snapshot id as version) SELECT * FROM t VERSION AS OF 1; -- read the snapshot from specified timestamp SELECT * FROM t TIMESTAMP AS OF \u0026#39;2023-06-01 00:00:00.123\u0026#39;; -- read the snapshot from specified timestamp in unix seconds SELECT * FROM t TIMESTAMP AS OF 1678883047; -- read tag \u0026#39;my-tag\u0026#39; SELECT * FROM t VERSION AS OF \u0026#39;my-tag\u0026#39;; Spark3-DF // read the snapshot from specified timestamp in unix seconds spark.read .option(\u0026#34;scan.timestamp-millis\u0026#34;, \u0026#34;1678883047000\u0026#34;) .format(\u0026#34;paimon\u0026#34;) .load(\u0026#34;path/to/table\u0026#34;) // read the snapshot with id 1L (use snapshot id as version) spark.read .option(\u0026#34;scan.snapshot-id\u0026#34;, 1) .format(\u0026#34;paimon\u0026#34;) .load(\u0026#34;path/to/table\u0026#34;) // read tag \u0026#39;my-tag\u0026#39; spark.read .option(\u0026#34;scan.tag-name\u0026#34;, \u0026#34;my-tag\u0026#34;) .format(\u0026#34;paimon\u0026#34;) .load(\u0026#34;path/to/table\u0026#34;) Trino -- read the snapshot from specified timestamp with a long value in unix milliseconds SET SESSION paimon.scan_timestamp_millis=1679486589444; SELECT * FROM t; Hive Hive requires adding the following configuration parameters to the hive-site.xml file:\n\u0026lt;!--This parameter is used to configure the whitelist of permissible configuration items allowed for use in SQL standard authorization mode.--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.authorization.sqlstd.confwhitelist\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapred.*|hive.*|mapreduce.*|spark.*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--This parameter is an additional configuration for hive.security.authorization.sqlstd.confwhitelist. It allows you to add other permissible configuration items to the existing whitelist.--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.authorization.sqlstd.confwhitelist.append\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapred.*|hive.*|mapreduce.*|spark.*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; SET paimon.scan.timestamp-millis=1679486589444; SELECT * FROM t; SET paimon.scan.timestamp-millis=null; -- read tag \u0026#39;my-tag\u0026#39; set paimon.scan.tag-name=my-tag; SELECT * FROM t; set paimon.scan.tag-name=null;  Batch Incremental #  Read incremental changes between start snapshot (exclusive) and end snapshot.\nFor example:\n \u0026lsquo;5,10\u0026rsquo; means changes between snapshot 5 and snapshot 10. \u0026lsquo;TAG1,TAG3\u0026rsquo; means changes between TAG1 and TAG3.  Flink -- incremental between snapshot ids SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between\u0026#39; = \u0026#39;12,20\u0026#39;) */; -- incremental between snapshot time mills SELECT * FROM t /*+ OPTIONS(\u0026#39;incremental-between-timestamp\u0026#39; = \u0026#39;1692169000000,1692169900000\u0026#39;) */; Spark3 Requires Spark 3.2+.\nPaimon supports that use Spark SQL to do the incremental query that implemented by Spark Table Valued Function. To enable this needs these configs below:\n--conf spark.sql.catalog.spark_catalog=org.apache.paimon.spark.SparkGenericCatalog --conf spark.sql.extensions=org.apache.paimon.spark.PaimonSparkSessionExtension you can use paimon_incremental_query in query to extract the incremental data:\n-- read the incremental data between snapshot id 12 and snapshot id 20. SELECT * FROM paimon_incremental_query(\u0026#39;tableName\u0026#39;, 12, 20); Spark-DF // incremental between snapshot ids spark.read() .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;incremental-between\u0026#34;, \u0026#34;12,20\u0026#34;) .load(\u0026#34;path/to/table\u0026#34;) // incremental between snapshot time mills spark.read() .format(\u0026#34;paimon\u0026#34;) .option(\u0026#34;incremental-between-timestamp\u0026#34;, \u0026#34;1692169000000,1692169900000\u0026#34;) .load(\u0026#34;path/to/table\u0026#34;) Hive -- incremental between snapshot ids SET paimon.incremental-between=\u0026#39;12,20\u0026#39;; SELECT * FROM t; SET paimon.incremental-between=null; -- incremental between snapshot time mills SET paimon.incremental-between-timestamp=\u0026#39;1692169000000,1692169900000\u0026#39;; SELECT * FROM t; SET paimon.incremental-between-timestamp=null;  In Batch SQL, the DELETE records are not allowed to be returned, so records of -D will be dropped. If you want see DELETE records, you can use audit_log table:\nFlink SELECT * FROM t$audit_log /*+ OPTIONS(\u0026#39;incremental-between\u0026#39; = \u0026#39;12,20\u0026#39;) */;  Streaming Query #  By default, Streaming read produces the latest snapshot on the table upon first startup, and continue to read the latest changes.\nPaimon by default ensures that your startup is properly processed with the full amount included.\n-- Flink SQL SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; You can also do streaming read without the snapshot data, you can use latest scan mode:\nFlink -- Continuously reads latest changes without producing a snapshot at the beginning. SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.mode\u0026#39; = \u0026#39;latest\u0026#39;) */;  Streaming Time Travel #  If you only want to process data for today and beyond, you can do so with partitioned filters:\nSELECT * FROM t WHERE dt \u0026gt; \u0026#39;2023-06-26\u0026#39;; If it\u0026rsquo;s not a partitioned table, or you can\u0026rsquo;t filter by partition, you can use Time travel\u0026rsquo;s stream read.\nFlink -- read changes from snapshot id 1L SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */; -- read changes from snapshot specified timestamp SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.timestamp-millis\u0026#39; = \u0026#39;1678883047356\u0026#39;) */; -- read snapshot id 1L upon first startup, and continue to read the changes SELECT * FROM t /*+ OPTIONS(\u0026#39;scan.mode\u0026#39;=\u0026#39;from-snapshot-full\u0026#39;,\u0026#39;scan.snapshot-id\u0026#39; = \u0026#39;1\u0026#39;) */;  Consumer ID #  You can specify the consumer-id when streaming read table:\nSELECT * FROM t /*+ OPTIONS(\u0026#39;consumer-id\u0026#39; = \u0026#39;myid\u0026#39;) */; When stream read Paimon tables, the next snapshot id to be recorded into the file system. This has several advantages:\n When previous job is stopped, the newly started job can continue to consume from the previous progress without resuming from the state. The newly reading will start reading from next snapshot id found in consumer files. When deciding whether a snapshot has expired, Paimon looks at all the consumers of the table in the file system, and if there are consumers that still depend on this snapshot, then this snapshot will not be deleted by expiration. When there is no watermark definition, the Paimon table will pass the watermark in the snapshot to the downstream Paimon table, which means you can track the progress of the watermark for the entire pipeline.  NOTE: The consumer will prevent expiration of the snapshot. You can specify \u0026lsquo;consumer.expiration-time\u0026rsquo; to manage the lifetime of consumers.  You can reset a consumer with a given consumer ID and next snapshot ID.\nFirst, you need to stop the streaming task using this consumer ID, and then execute the reset consumer action job.  Flink Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  reset-consumer \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --consumer-id \u0026lt;consumer-id\u0026gt; \\  --next-snapshot \u0026lt;next-snapshot-id\u0026gt; \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]]  Read Overwrite #  Streaming reading will ignore the commits generated by INSERT OVERWRITE by default. If you want to read the commits of OVERWRITE, you can configure streaming-read-overwrite.\nRead Parallelism #  Flink By default, the parallelism of batch reads is the same as the number of splits, while the parallelism of stream reads is the same as the number of buckets.\nDisable scan.infer-parallelism, global parallelism will be used for reads.\nYou can also manually specify the parallelism from scan.parallelism.\n  Key Default Type Description     scan.infer-parallelism true Boolean If it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).   scan.parallelism (none) Integer Define a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.     Query Optimization #   Batch Streaming It is highly recommended to specify partition and primary key filters along with the query, which will speed up the data skipping of the query.\nThe filter functions that can accelerate data skipping are:\n = \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= IN (...) LIKE 'abc%' IS NULL  Paimon will sort the data by primary key, which speeds up the point queries and range queries. When using a composite primary key, it is best for the query filters to form a leftmost prefix of the primary key for good acceleration.\nSuppose that a table has the following specification:\nCREATE TABLE orders ( catalog_id BIGINT, order_id BIGINT, ....., PRIMARY KEY (catalog_id, order_id) NOT ENFORCED -- composite primary key ); The query obtains a good acceleration by specifying a range filter for the leftmost prefix of the primary key.\nSELECT * FROM orders WHERE catalog_id=1025; SELECT * FROM orders WHERE catalog_id=1025 AND order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 AND order_id\u0026gt;2035 AND order_id\u0026lt;6000; However, the following filter cannot accelerate the query well.\nSELECT * FROM orders WHERE order_id=29495; SELECT * FROM orders WHERE catalog_id=1025 OR order_id=29495; "});index.add({'id':33,'href':'/docs/master/maintenance/','title':"Maintenance",'section':"Apache Paimon",'content':""});index.add({'id':34,'href':'/docs/master/maintenance/manage-files/','title':"Manage Files",'section':"Maintenance",'content':"Manage Small Files #  Many users are concerned about small files, which can lead to:\n Stability issue: Too many small files in HDFS, NameNode will be overstressed. Cost issue: A small file in HDFS will temporarily use the size of a minimum of one Block, for example 128 MB. Query efficiency: The efficiency of querying too many small files will be affected.  Understand Checkpoints #  Assuming you are using Flink Writer, each checkpoint generates 1-2 snapshots, and the checkpoint forces the files to be generated on DFS, so the smaller the checkpoint interval the more small files will be generated.\n So first thing is increase checkpoint interval.  By default, not only checkpoint will cause the file to be generated, but writer\u0026rsquo;s memory (write-buffer-size) exhaustion will also flush data to DFS and generate the corresponding file. You can enable write-buffer-spillable to generate spilled files in writer to generate bigger files in DFS.\nSo second thing is increase write-buffer-size or enable write-buffer-spillable.  Understand Snapshots #  Before delving further into this section, please ensure that you have read File Operations.\nPaimon maintains multiple versions of files, compaction and deletion of files are logical and do not actually delete files. Files are only really deleted when Snapshot is expired, so the first way to reduce files is to reduce the time it takes for snapshot to be expired. Flink writer will automatically expire snapshots.\nSee Expire Snapshots.\nUnderstand Partitions and Buckets #  Paimon files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Paimon readers can recursively access all records from the table.\nFor example, the following table:\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;10\u0026#39; ); The table data will be physically sliced into different partitions, and different buckets inside, so if the overall data volume is too small, there is at least one file in a single bucket, I suggest you configure a smaller number of buckets, otherwise there will be quite a few small files as well.\nUnderstand LSM for Primary Table #  LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nBy default, sorted runs number depends on num-sorted-run.compaction-trigger, see Compaction for Primary Key Table, this means that there are at least 5 files in a bucket. If you want to reduce this number, you can keep fewer files, but write performance may suffer.\nUnderstand Files for Append-Only Table #  By default, Append-Only also does automatic compaction to reduce the number of small files.\nHowever, for Bucket\u0026rsquo;s Append-only table, it will only compact the files within the Bucket for sequential purposes, which may keep more small files. See Compaction for Append-Only Table.\nUnderstand Full-Compaction #  Maybe you think the 5 files for the primary key table are actually okay, but the Append-Only table (bucket) may have 50 small files in a single bucket, which is very difficult to accept. Worse still, partitions that are no longer active also keep so many small files.\nIt is recommended that you configure Full-Compaction, configure ‘full-compaction.delta-commits’ perform full-compaction periodically in Flink writing. And it can ensure that partitions are full compacted before writing ends.\n"});index.add({'id':35,'href':'/docs/master/concepts/primary-key-table/','title':"Primary Key Table",'section':"Concepts",'content':"Primary Key Table #  Changelog table is the default table type when creating a table. Users can insert, update or delete records in the table.\nPrimary keys consist of a set of columns that contain unique values for each record. Paimon enforces data ordering by sorting the primary key within each bucket, allowing users to achieve high performance by applying filtering conditions on the primary key.\nBy defining primary keys on a changelog table, users can access the following features.\nBucket #  A bucket is the smallest storage unit for reads and writes, each bucket directory contains an LSM tree.\nFixed Bucket #  Configure a bucket greater than 0, rescaling buckets can only be done through offline processes, see Rescale Bucket. A too large number of buckets leads to too many small files, and a too small number of buckets leads to poor write performance.\nDynamic Bucket #  Configure 'bucket' = '-1', Paimon dynamically maintains the index, automatic expansion of the number of buckets.\n Option1: 'dynamic-bucket.target-row-num': controls the target row number for one bucket. Option2: 'dynamic-bucket.assigner-parallelism': Parallelism of assigner operator, controls the number of initialized bucket.  Dynamic Bucket only support single write job. Please do not start multiple jobs to write to the same partition.  Normal Dynamic Bucket Mode #  When your updates do not cross partitions (no partitions, or primary keys contain all partition fields), Dynamic Bucket mode uses HASH index to maintain mapping from key to bucket, it requires more memory than fixed bucket mode.\nPerformance:\n Generally speaking, there is no performance loss, but there will be some additional memory consumption, 100 million entries in a partition takes up 1 GB more memory, partitions that are no longer active do not take up memory. For tables with low update rates, this mode is recommended to significantly improve performance.  Cross Partitions Update Dynamic Bucket Mode #  This is an experimental feature.  When you need cross partition updates (primary keys not contain all partition fields), Dynamic Bucket mode directly maintains the mapping of keys to partition and bucket, uses local disks, and initializes indexes by reading all existing keys in the table when starting stream write job. Different merge engines have different behaviors:\n Deduplicate: Delete data from the old partition and insert new data into the new partition. PartialUpdate \u0026amp; Aggregation: Insert new data into the old partition. FirstRow: Ignore new data if there is old value.  Performance: For tables with a large amount of data, there will be a significant loss in performance. Moreover, initialization takes a long time.\nMerge Engines #  When Paimon sink receives two or more records with the same primary keys, it will merge them into one record to keep primary keys unique. By specifying the merge-engine table property, users can choose how records are merged together.\nAlways set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig, sink upsert-materialize may result in strange behavior. When the input is out of order, we recommend that you use Sequence Field to correct disorder.  Deduplicate #  deduplicate merge engine is the default merge engine. Paimon will only keep the latest record and throw away other records with the same primary keys.\nSpecifically, if the latest record is a DELETE record, all records with the same primary keys will be deleted.\nPartial Update #  By specifying 'merge-engine' = 'partial-update', Users have the ability to update columns of a record through multiple updates until the record is complete. This is achieved by updating the value fields one by one, using the latest data under the same primary key. However, null values are not overwritten in the process.\nFor example, suppose Paimon receives three records:\n \u0026lt;1, 23.0, 10, NULL\u0026gt;- \u0026lt;1, NULL, NULL, 'This is a book'\u0026gt; \u0026lt;1, 25.2, NULL, NULL\u0026gt;  Assuming that the first column is the primary key, the final result would be \u0026lt;1, 25.2, 10, 'This is a book'\u0026gt;.\nFor streaming queries, partial-update merge engine must be used together with lookup or full-compaction changelog producer.  By default, Partial update can not accept delete records, you can choose one of the following solutions:\n Configure \u0026lsquo;partial-update.ignore-delete\u0026rsquo; to ignore delete records. Configure \u0026lsquo;sequence-group\u0026rsquo;s to retract partial columns.   Sequence Group #  A sequence-field may not solve the disorder problem of partial-update tables with multiple stream updates, because the sequence-field may be overwritten by the latest data of another stream during multi-stream update.\nSo we introduce sequence group mechanism for partial-update tables. It can solve:\n Disorder during multi-stream update. Each stream defines its own sequence-groups. A true partial-update, not just a non-null update.  See example:\nCREATE TABLE T ( k INT, a INT, b INT, g_1 INT, c INT, d INT, g_2 INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39;=\u0026#39;partial-update\u0026#39;, \u0026#39;fields.g_1.sequence-group\u0026#39;=\u0026#39;a,b\u0026#39;, \u0026#39;fields.g_2.sequence-group\u0026#39;=\u0026#39;c,d\u0026#39; ); INSERT INTO T VALUES (1, 1, 1, 1, 1, 1, 1); -- g_2 is null, c, d should not be updated INSERT INTO T VALUES (1, 2, 2, 2, 2, 2, CAST(NULL AS INT)); SELECT * FROM T; -- output 1, 2, 2, 2, 1, 1, 1  -- g_1 is smaller, a, b should not be updated INSERT INTO T VALUES (1, 3, 3, 1, 3, 3, 3); SELECT * FROM T; -- output 1, 2, 2, 2, 3, 3, 3 For fields..sequence-group, valid comparative data types include: DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP, and TIMESTAMP_LTZ.\nDefault Value #  If the order of the data cannot be guaranteed and field is written only by overwriting null values, fields that have not been overwritten will be displayed as null when reading table.\nCREATE TABLE T ( k INT, a INT, b INT, c INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39;=\u0026#39;partial-update\u0026#39; ); INSERT INTO T VALUES (1, 1,null,null); INSERT INTO T VALUES (1, null,null,1); SELECT * FROM T; -- output 1, 1, null, 1 If it is expected that fields which have not been overwritten have a default value instead of null when reading table, \u0026lsquo;fields.name.default-value\u0026rsquo; is required.\nCREATE TABLE T ( k INT, a INT, b INT, c INT, PRIMARY KEY (k) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39;=\u0026#39;partial-update\u0026#39;, \u0026#39;fields.b.default-value\u0026#39;=\u0026#39;0\u0026#39; ); INSERT INTO T VALUES (1, 1,null,null); INSERT INTO T VALUES (1, null,null,1); SELECT * FROM T; -- output 1, 1, 0, 1 Aggregation #  NOTE: Always set table.exec.sink.upsert-materialize to NONE in Flink SQL TableConfig.  Sometimes users only care about aggregated results. The aggregation merge engine aggregates each value field with the latest data one by one under the same primary key according to the aggregate function.\nEach field not part of the primary keys can be given an aggregate function, specified by the fields.\u0026lt;field-name\u0026gt;.aggregate-function table property, otherwise it will use last_non_null_value aggregation as default. For example, consider the following table definition.\nFlink CREATE TABLE MyTable ( product_id BIGINT, price DOUBLE, sales BIGINT, PRIMARY KEY (product_id) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.price.aggregate-function\u0026#39; = \u0026#39;max\u0026#39;, \u0026#39;fields.sales.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; );  Field price will be aggregated by the max function, and field sales will be aggregated by the sum function. Given two input records \u0026lt;1, 23.0, 15\u0026gt; and \u0026lt;1, 30.2, 20\u0026gt;, the final result will be \u0026lt;1, 30.2, 35\u0026gt;.\nCurrent supported aggregate functions and data types are:\n sum: supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT and DOUBLE. min/max: support DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP and TIMESTAMP_LTZ. last_value / last_non_null_value: support all data types. listagg: supports STRING data type. bool_and / bool_or: support BOOLEAN data type.  Only sum supports retraction (UPDATE_BEFORE and DELETE), others aggregate functions do not support retraction. If you allow some functions to ignore retraction messages, you can configure: 'fields.${field_name}.ignore-retract'='true'.\nFor streaming queries, aggregation merge engine must be used together with lookup or full-compaction changelog producer.  First Row #  This is an experimental feature.  By specifying 'merge-engine' = 'first-row', users can keep the first row of the same primary key. It differs from the deduplicate merge engine that in the first-row merge engine, it will generate insert only changelog.\n first-row merge engine must be used together with lookup changelog producer. You can not specify sequence.field. Not accept DELETE and UPDATE_BEFORE message.  Changelog Producers #  Streaming queries will continuously produce the latest changes. These changes can come from the underlying table files or from an external log system like Kafka. Compared to the external log system, changes from table files have lower cost but higher latency (depending on how often snapshots are created).\nBy specifying the changelog-producer table property when creating the table, users can choose the pattern of changes produced from files.\nThe changelog-producer table property only affects changelog from files. It does not affect the external log system.  None #  By default, no extra changelog producer will be applied to the writer of table. Paimon source can only see the merged changes across snapshots, like what keys are removed and what are the new values of some keys.\nHowever, these merged changes cannot form a complete changelog, because we can\u0026rsquo;t read the old values of the keys directly from them. Merged changes require the consumers to \u0026ldquo;remember\u0026rdquo; the values of each key and to rewrite the values without seeing the old ones. Some consumers, however, need the old values to ensure correctness or efficiency.\nConsider a consumer which calculates the sum on some grouping keys (might not be equal to the primary keys). If the consumer only sees a new value 5, it cannot determine what values should be added to the summing result. For example, if the old value is 4, it should add 1 to the result. But if the old value is 6, it should in turn subtract 1 from the result. Old values are important for these types of consumers.\nTo conclude, none changelog producers are best suited for consumers such as a database system. Flink also has a built-in \u0026ldquo;normalize\u0026rdquo; operator which persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided. (You can force removing \u0026ldquo;normalize\u0026rdquo; operator via 'scan.remove-normalize'.)\nInput #  By specifying 'changelog-producer' = 'input', Paimon writers rely on their inputs as a source of complete changelog. All input records will be saved in separated changelog files and will be given to the consumers by Paimon sources.\ninput changelog producer can be used when Paimon writers' inputs are complete changelog, such as from a database CDC, or generated by Flink stateful computation.\nLookup #  If your input can’t produce a complete changelog but you still want to get rid of the costly normalized operator, you may consider using the 'lookup' changelog producer.\nBy specifying 'changelog-producer' = 'lookup', Paimon will generate changelog through 'lookup' before committing the data writing.\nLookup will cache data on the memory and local disk, you can use the following options to tune performance:\n  Option Default Type Description     lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size unlimited MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.    Lookup changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.\nFull Compaction #  If you think the resource consumption of \u0026lsquo;lookup\u0026rsquo; is too large, you can consider using \u0026lsquo;full-compaction\u0026rsquo; changelog producer, which can decouple data writing and changelog generation, and is more suitable for scenarios with high latency (For example, 10 minutes).\nBy specifying 'changelog-producer' = 'full-compaction', Paimon will compare the results between full compactions and produce the differences as changelog. The latency of changelog is affected by the frequency of full compactions.\nBy specifying full-compaction.delta-commits table property, full compaction will be constantly triggered after delta commits (checkpoints). This is set to 1 by default, so each checkpoint will have a full compression and generate a change log.\nFull compaction changelog producer can produce complete changelog for any type of source. However it is not as efficient as the input changelog producer and the latency to produce changelog might be high.  Full-compaction changelog-producer supports changelog-producer.row-deduplicate to avoid generating -U, +U changelog for the same record.\nSequence Field #  By default, the primary key table determines the merge order according to the input order (the last input record will be the last to merge). However, in distributed computing, there will be some cases that lead to data disorder. At this time, you can use a time field as sequence.field, for example:\nFlink CREATE TABLE MyTable ( pk BIGINT PRIMARY KEY NOT ENFORCED, v1 DOUBLE, v2 BIGINT, dt TIMESTAMP ) WITH ( \u0026#39;sequence.field\u0026#39; = \u0026#39;dt\u0026#39; );  The record with the largest sequence.field value will be the last to merge, regardless of the input order.\nSequence Auto Padding:\nWhen the record is updated or deleted, the sequence.field must become larger and cannot remain unchanged. For -U and +U, their sequence-fields must be different. If you cannot meet this requirement, Paimon provides option to automatically pad the sequence field for you.\n  'sequence.auto-padding' = 'row-kind-flag': If you are using same value for -U and +U, just like \u0026ldquo;op_ts\u0026rdquo; (the time that the change was made in the database) in Mysql Binlog. It is recommended to use the automatic padding for row kind flag, which will automatically distinguish between -U (-D) and +U (+I).\n  Insufficient precision: If the provided sequence.field doesn\u0026rsquo;t meet the precision, like a rough second or millisecond, you can set sequence.auto-padding to second-to-micro or millis-to-micro so that the precision of sequence number will be made up to microsecond by system.\n  Composite pattern: for example, \u0026ldquo;second-to-micro,row-kind-flag\u0026rdquo;, first, add the micro to the second, and then pad the row kind flag.\n  "});index.add({'id':36,'href':'/docs/master/how-to/system-tables/','title':"System Tables",'section':"How to",'content':"System Tables #  Table Specified System Table #  Table specified system tables contain metadata and information about each table, such as the snapshots created and the options in use. Users can access system tables with batch queries.\nCurrently, Flink, Spark and Trino supports querying system tables.\nIn some cases, the table name needs to be enclosed with back quotes to avoid syntax parsing conflicts, for example triple access mode:\nSELECT * FROM my_catalog.my_db.`MyTable$snapshots`; Snapshots Table #  You can query the snapshot history information of the table through snapshots table, including the record count occurred in the snapshot.\nSELECT * FROM MyTable$snapshots; /* +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+-------------------+--------------------+----------------+ | snapshot_id | schema_id | commit_user | commit_identifier | commit_kind | commit_time | base_manifest_list | delta_manifest_list | changelog_manifest_list | total_record_count | delta_record_count | changelog_record_count | added_file_count | delete_file_count | watermark | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+-------------------+--------------------+----------------+ | 2 | 0 | 7ca4cd28-98e... | 2 | APPEND | 2022-10-26 11:44:15.600 | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | manifest-list-31323d5f-76e6... | 2 | 2 | 0 | 2 | 0 | 1666755855600 | | 1 | 0 | 870062aa-3e9... | 1 | APPEND | 2022-10-26 11:44:15.148 | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | manifest-list-31593d5f-76e6... | 1 | 1 | 0 | 1 | 0 | 1666755855148 | +--------------+------------+-----------------+-------------------+--------------+-------------------------+--------------------------------+------------------------------- +--------------------------------+---------------------+---------------------+-------------------------+-------------------+--------------------+----------------+ 2 rows in set */ By querying the snapshots table, you can know the commit and expiration information about that table and time travel through the data.\nSchemas Table #  You can query the historical schemas of the table through schemas table.\nSELECT * FROM MyTable$schemas; /* +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | schema_id | fields | partition_keys | primary_keys | options | comment | update_time | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ | 0 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-28 11:44:20.600 | | 1 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-27 11:44:15.600 | | 2 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | 2022-10-26 11:44:10.600 | +-----------+--------------------------------+----------------+--------------+---------+---------+-------------------------+ 3 rows in set */ You can join the snapshots table and schemas table to get the fields of given snapshots.\nSELECT s.snapshot_id, t.schema_id, t.fields FROM MyTable$snapshots s JOIN MyTable$schemas t ON s.schema_id=t.schema_id where s.snapshot_id=100; Options Table #  You can query the table\u0026rsquo;s option information which is specified from the DDL through options table. The options not shown will be the default value. You can take reference to Configuration.\nSELECT * FROM MyTable$options; /* +------------------------+--------------------+ | key | value | +------------------------+--------------------+ | snapshot.time-retained | 5 h | +------------------------+--------------------+ 1 rows in set */ Audit log Table #  If you need to audit the changelog of the table, you can use the audit_log system table. Through audit_log table, you can get the rowkind column when you get the incremental data of the table. You can use this column for filtering and other operations to complete the audit.\nThere are four values for rowkind:\n +I: Insertion operation. -U: Update operation with the previous content of the updated row. +U: Update operation with new content of the updated row. -D: Deletion operation.  SELECT * FROM MyTable$audit_log; /* +------------------+-----------------+-----------------+ | rowkind | column_0 | column_1 | +------------------+-----------------+-----------------+ | +I | ... | ... | +------------------+-----------------+-----------------+ | -U | ... | ... | +------------------+-----------------+-----------------+ | +U | ... | ... | +------------------+-----------------+-----------------+ 3 rows in set */ Files Table #  You can query the files of the table with specific snapshot.\n-- Query the files of latest snapshot SELECT * FROM MyTable$files; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | [3] | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | [2] | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | [2] | 0 | data-83aa7973-060b-40b6-8c8... | orc | 0 | 0 | 1 | 605 | [d] | [d] | {cnt=0, val=0, word=0} | {cnt=2, val=32, word=d} | {cnt=2, val=32, word=d} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| | [5] | 0 | data-3d304f4a-bcea-44dc-a13... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=5, val=51, word=c} | {cnt=5, val=51, word=c} | 1691551246788 | 1691551246152 |2023-02-24T16:06:21.166| | [1] | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246722 | 1691551246273 |2023-02-24T16:06:21.166| | [4] | 0 | data-2c9b7095-65b7-4013-a7a... | orc | 0 | 0 | 1 | 593 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=4, val=12, word=a} | {cnt=4, val=12, word=a} | 1691551246321 | 1691551246109 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 6 rows in set */ -- You can also query the files with specific snapshot SELECT * FROM MyTable$files /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39;=\u0026#39;1\u0026#39;) */; /* +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | min_sequence_number | max_sequence_number | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ | [3] | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} | 1691551246234 | 1691551246637 |2023-02-24T16:06:21.166| | [2] | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} | 1691551246233 | 1691551246732 |2023-02-24T16:06:21.166| | [1] | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} | 1691551246267 | 1691551246798 |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+ 3 rows in set */ Tags Table #  You can query the tag history information of the table through tags table, including which snapshots are the tags based on and some historical information of the snapshots. You can also get all tag names and time travel to a specific tag data by name.\nSELECT * FROM MyTable$tags; /* +----------+-------------+-----------+-------------------------+--------------+ | tag_name | snapshot_id | schema_id | commit_time | record_count | +----------+-------------+-----------+-------------------------+--------------+ | tag1 | 1 | 0 | 2023-06-28 14:55:29.344 | 3 | | tag3 | 3 | 0 | 2023-06-28 14:58:24.691 | 7 | +----------+-------------+-----------+-------------------------+--------------+ 2 rows in set */ Consumers Table #  You can query all consumers which contains next snapshot.\nSELECT * FROM MyTable$consumers; /* +-------------+------------------+ | consumer_id | next_snapshot_id | +-------------+------------------+ | id1 | 1 | | id2 | 3 | +-------------+------------------+ 2 rows in set */ Manifests Table #  You can query all manifest files contained in the latest snapshot or the specified snapshot of the current table.\n-- Query the manifest of latest snapshot SELECT * FROM MyTable$manifests; /* +--------------------------------+-------------+------------------+-------------------+---------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | +--------------------------------+-------------+------------------+-------------------+---------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | | manifest-f4dcab43-ef6b-4713... | 1648 | 1 | 0 | 0 | +--------------------------------+-------------+------------------+-------------------+---------------+ 2 rows in set */ -- You can also query the manifest with specified snapshot SELECT * FROM MyTable$manifests /*+ OPTIONS(\u0026#39;scan.snapshot-id\u0026#39;=\u0026#39;1\u0026#39;) */; /* +--------------------------------+-------------+------------------+-------------------+---------------+ | file_name | file_size | num_added_files | num_deleted_files | schema_id | +--------------------------------+-------------+------------------+-------------------+---------------+ | manifest-f4dcab43-ef6b-4713... | 12365| 40 | 0 | 0 | +--------------------------------+-------------+------------------+-------------------+---------------+ 1 rows in set */ Partitions Table #  You can query the partition files of the table.\nSELECT * FROM MyTable$partitions; /* +---------------+----------------+--------------------+ | partition | record_count | file_size_in_bytes| +---------------+----------------+--------------------+ | [1] | 1 | 645 | +---------------+----------------+--------------------+ */ Global System Table #  Global system tables contain the statistical information of all the tables exists in paimon. For convenient of searching, we create a reference system database called sys. We can display all the global system tables by sql in flink:\nUSE sys; SHOW TABLES; ALL Options Table #  This table is similar to Options Table, but it shows all the table options is all database.\nSELECT * FROM sys.all_table_options; /* +---------------+--------------------------------+--------------------------------+------------------+ | database_name | table_name | key | value | +---------------+--------------------------------+--------------------------------+------------------+ | my_db | Orders_orc | bucket | -1 | | my_db | Orders2 | bucket | -1 | | my_db | Orders2 | write-mode | append-only | | my_db | Orders2 | sink.parallelism | 7 | | my_db | StockAnalyze | write-mode | change-log | | my_db2| OrdersSum | bucket | 1 | | my_db2| OrdersSum | write-mode | change-log | +---------------+--------------------------------+--------------------------------+------------------+ 7 rows in set */ Catalog Options Table #  You can query the catalog\u0026rsquo;s option information through catalog options table. The options not shown will be the default value. You can take reference to Configuration.\nSELECT * FROM sys.catalog_options; /* +-----------+---------------------------+ | key | value | +-----------+---------------------------+ | warehouse | hdfs:///path/to/warehouse | +-----------+---------------------------+ 1 rows in set */ "});index.add({'id':37,'href':'/docs/master/engines/trino/','title':"Trino",'section':"Engines",'content':"Trino #  This documentation is a guide for using Paimon in Trino.\nVersion #  Paimon currently supports Trino 358 and above.\nPreparing Paimon Jar File #     Version Jar     [358, 368) paimon-trino-358-0.5-SNAPSHOT.jar   [368, 369) paimon-trino-368-0.5-SNAPSHOT.jar   [369, 370) paimon-trino-369-0.5-SNAPSHOT.jar   [370, 388) paimon-trino-370-0.5-SNAPSHOT.jar   [388, 393) paimon-trino-388-0.5-SNAPSHOT.jar   [393, 422] paimon-trino-393-0.5-SNAPSHOT.jar   [422, latest] paimon-trino-422-0.5-SNAPSHOT.jar    You can also manually build a bundled jar from the source code. However, there are a few preliminary steps that need to be taken before compiling:\n To build from the source code, clone the git repository. Install JDK11 and JDK17 locally, and configure JDK11 as a global environment variable; Configure the toolchains.xml file in ${{ MAVEN_HOME }}, the content is as follows.   \u0026lt;toolchains\u0026gt; \u0026lt;toolchain\u0026gt; \u0026lt;type\u0026gt;jdk\u0026lt;/type\u0026gt; \u0026lt;provides\u0026gt; \u0026lt;version\u0026gt;17\u0026lt;/version\u0026gt; \u0026lt;vendor\u0026gt;adopt\u0026lt;/vendor\u0026gt; \u0026lt;/provides\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;jdkHome\u0026gt;${{ JAVA_HOME }}\u0026lt;/jdkHome\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/toolchain\u0026gt; \u0026lt;/toolchains\u0026gt; Then,you can build bundled jar with the following command:\nmvn clean install -DskipTests You can find Trino connector jar in ./paimon-trino-\u0026lt;trino-version\u0026gt;/target/paimon-trino-*.jar.\nThen, copy paimon-trino-*.jar and flink-shaded-hadoop-*-uber-*.jar to plugin/paimon.\n NOTE: For JDK 17, when Deploying Trino, should add jvm options: --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED\n Tmp Dir #  Paimon will unzip some jars to the tmp directory for codegen. By default, Trino will use '/tmp' as the temporary directory, but '/tmp' may be periodically deleted.\nYou can configure this environment variable when Trino starts:\n-Djava.io.tmpdir=/path/to/other/tmpdir Let Paimon use a secure temporary directory.\nConfigure Paimon Catalog #  Catalogs are registered by creating a catalog properties file in the etc/catalog directory. For example, create etc/catalog/paimon.properties with the following contents to mount the paimon connector as the paimon catalog:\nconnector.name=paimon warehouse=file:/tmp/warehouse If you are using HDFS, choose one of the following ways to configure your HDFS:\n set environment variable HADOOP_HOME. set environment variable HADOOP_CONF_DIR. configure fs.hdfs.hadoopconf in the properties.  Kerberos #  You can configure kerberos keytab file when using KERBEROS authentication in the properties.\nsecurity.kerberos.login.principal=hadoop-user security.kerberos.login.keytab=/etc/trino/hdfs.keytab Keytab files must be distributed to every node in the cluster that runs Trino.\nCreate Schema #  CREATE SCHEMA paimon.test_db; Create Table #  CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = 'ORC', primary_key = ARRAY['order_key','order_date'], partitioned_by = ARRAY['order_date'], bucket = '2', bucket_key = 'order_key', changelog_producer = 'input' ) Add Column #  CREATE TABLE paimon.test_db.orders ( order_key bigint, orders_tatus varchar, total_price decimal(18,4), order_date date ) WITH ( file_format = 'ORC', primary_key = ARRAY['order_key','order_date'], partitioned_by = ARRAY['order_date'], bucket = '2', bucket_key = 'order_key', changelog_producer = 'input' ) ALTER TABLE paimon.test_db.orders ADD COLUMN shipping_address varchar; Query #  SELECT * FROM paimon.test_db.orders Query with Time Traveling #  SET SESSION paimon.scan_timestamp_millis=1679486589444; SELECT * FROM paimon.test_db.orders; Trino to Paimon type mapping #  This section lists all supported type conversion between Trino and Paimon. All Trino\u0026rsquo;s data types are available in package io.trino.spi.type.\n  Trino Data Type Paimon Data Type Atomic Type     RowType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   TinyintType TinyIntType true   SmallintType SmallIntType true   IntegerType IntType true   BigintType BigIntType true   RealType FloatType true   DoubleType DoubleType true   CharType(length) CharType(length) true   VarCharType(VarCharType.MAX_LENGTH) VarCharType(VarCharType.MAX_LENGTH) true   VarCharType(length) VarCharType(length), length is less than VarCharType.MAX_LENGTH true   DateType DateType true   TimestampType TimestampType true   DecimalType(precision, scale) DecimalType(precision, scale) true   VarBinaryType(length) VarBinaryType(length) true   TimestampWithTimeZoneType LocalZonedTimestampType true    "});index.add({'id':38,'href':'/docs/master/api/','title':"API",'section':"Apache Paimon",'content':""});index.add({'id':39,'href':'/docs/master/concepts/append-only-table/','title':"Append Only Table",'section':"Concepts",'content':"Append Only Table #  If a table does not have a primary key defined, it is an append-only table by default. Separated by the definition of bucket, we have two different append-only mode: \u0026ldquo;Append For Queue\u0026rdquo; and \u0026ldquo;Append For Scalable Table\u0026rdquo;.\nAppend For Queue #  You can only insert a complete record into the table. No delete or update is supported, and you cannot define primary keys. This type of table is suitable for use cases that do not require updates (such as log data synchronization).\nDefinition #  In this mode, you can regard append-only table as a queue separated by bucket. Every record in the same bucket is ordered strictly, streaming read will transfer the record to down-stream exactly in the order of writing. To use this mode, you do not need to config special configurations, all the data will go into one bucket as a queue. You can also define the bucket and bucket-key to enable larger parallelism and disperse data (see Example).\nCompaction #  By default, the sink node will automatically perform compaction to control the number of files. The following options control the strategy of compaction:\n  Key Default Type Description     write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number which satisfies sum(size(f_i)) \u0026gt;= targetFileSize to trigger a compaction for append-only table. This value avoids almost-full-file to be compacted, which is not cost-effective.   compaction.max.file-num 50 Integer For file set [f_0,...,f_N], the maximum file number to trigger a compaction for append-only table, even if sum(size(f_i)) \u0026lt; targetFileSize. This value avoids pending too much small files, which slows down the performance.   full-compaction.delta-commits (none) Integer Full compaction will be constantly triggered after delta commits.    Streaming Source #  Streaming source behavior is only supported in Flink engine at present.\nStreaming Read Order #  For streaming reads, records are produced in the following order:\n For any two records from two different partitions  If scan.plan-sort-partition is set to true, the record with a smaller partition value will be produced first. Otherwise, the record with an earlier partition creation time will be produced first.   For any two records from the same partition and the same bucket, the first written record will be produced first. For any two records from the same partition but two different buckets, different buckets are processed by different tasks, there is no order guarantee between them.  Watermark Definition #  You can define watermark for reading Paimon tables:\nCREATE TABLE T ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (...); -- launch a bounded streaming job to read paimon_table SELECT window_start, window_end, COUNT(`user`) FROM TABLE( TUMBLE(TABLE T, DESCRIPTOR(order_time), INTERVAL \u0026#39;10\u0026#39; MINUTES)) GROUP BY window_start, window_end; You can also enable Flink Watermark alignment, which will make sure no sources/splits/shards/partitions increase their watermarks too far ahead of the rest:\n  Key Default Type Description     scan.watermark.alignment.group (none) String A group of sources to align watermarks.   scan.watermark.alignment.max-drift (none) Duration Maximal drift to align watermarks, before we pause consuming from the source/task/partition.    Bounded Stream #  Streaming Source can also be bounded, you can specify \u0026lsquo;scan.bounded.watermark\u0026rsquo; to define the end condition for bounded streaming mode, stream reading will end until a larger watermark snapshot is encountered.\nWatermark in snapshot is generated by writer, for example, you can specify a kafka source and declare the definition of watermark. When using this kafka source to write to Paimon table, the snapshots of Paimon table will generate the corresponding watermark, so that you can use the feature of bounded watermark when streaming reads of this Paimon table.\nCREATE TABLE kafka_table ( `user` BIGINT, product STRING, order_time TIMESTAMP(3), WATERMARK FOR order_time AS order_time - INTERVAL \u0026#39;5\u0026#39; SECOND ) WITH (\u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;...); -- launch a streaming insert job INSERT INTO paimon_table SELECT * FROM kakfa_table; -- launch a bounded streaming job to read paimon_table SELECT * FROM paimon_table /*+ OPTIONS(\u0026#39;scan.bounded.watermark\u0026#39;=\u0026#39;...\u0026#39;) */; Example #  The following is an example of creating the Append-Only table and specifying the bucket key.\nFlink CREATE TABLE MyTable ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;product_id\u0026#39; );  Append For Scalable Table #  This is an experimental feature.  Definition #  By defining 'bucket' = '-1' in table properties, you can assign a special mode (we call it \u0026ldquo;unaware-bucket mode\u0026rdquo;) to this table (see Example). In this mode, all the things are different. We don\u0026rsquo;t have the concept of bucket anymore, and we don\u0026rsquo;t guarantee the order of streaming read. We regard this table as a batch off-line table ( although we can stream read and write still). All the records will go into one directory (for compatibility, we put them in bucket-0), and we do not maintain the order anymore. As we don\u0026rsquo;t have the concept of bucket, we will not shuffle the input records by bucket anymore, which will speed up the inserting.\nCompaction #  In unaware-bucket mode, we don\u0026rsquo;t do compaction in writer, instead, we use Compact Coordinator to scan the small files and submit compaction task to Compact Worker. By this, we can easily do compaction for one simple data directory in parallel. In streaming mode, if you run insert sql in flink, the topology will be like this:\nIt will do its best to compact small files, but when a single small file in one partition remains long time and no new file added to the partition, the Compact Coordinator will remove it from memory to reduce memory usage. After you restart the job, it will scan the small files and add it to memory again. The options to control the compact behavior is exactly the same as Append For Qeueue. If you set write-only to true, the Compact Coordinator and Compact Worker will be removed in the topology.\nThe auto compaction is only supported in Flink engine streaming mode. You can also start a compaction job in flink by flink action in paimon and disable all the other compaction by set write-only.\nSort Compact #  The data in a per-partition out of order will lead a slow select, compaction may slow down the inserting. It is a good choice for you to set write-only for inserting job, and after per-partition data done, trigger a partition Sort Compact action.\nYou can trigger action by shell script:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  compact \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table \u0026lt;tableName\u0026gt; \\  --order-strategy \u0026lt;orderType\u0026gt; \\  --order-by \u0026lt;col1,col2,...\u0026gt;    Configuration Description     --order-strategy the order strategy now only support zorder. For example: --order-strategy zorder   --order-by Specify the order columns. For example: --order-by col0, col1    Other config is the same as Compact Table\nStreaming Source #  Unaware-bucket mode append-only table supported streaming read and write, but no longer guarantee order anymore. You cannot regard it as a queue, instead, as a lake with storage bins. Every commit will generate a new record bin, we can read the increase by reading the new record bin, but records in one bin are flowing to anywhere they want, and we fetch them in any possible order. While in the Append For Queue mode, records are not stored in bins, but in record pipe. We can see the difference below.\nExample #  The following is an example of creating the Append-Only table and specifying the bucket key.\nFlink CREATE TABLE MyTable ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;-1\u0026#39; );  "});index.add({'id':40,'href':'/docs/master/how-to/lookup-joins/','title':"Lookup Joins",'section':"How to",'content':"Lookup Joins #  Lookup Joins are a type of join in streaming queries. It is used to enrich a table with data that is queried from Paimon. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.\nPaimon supports lookup joins on tables with primary keys and append-only tables in Flink. The following example illustrates this feature.\nPrepare #  First, let\u0026rsquo;s create a Paimon table and update it in real-time.\n-- Create a paimon catalog CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;paimon\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;hdfs://nn:8020/warehouse/path\u0026#39; -- or \u0026#39;file://tmp/foo/bar\u0026#39; ); USE CATALOG my_catalog; -- Create a table in paimon catalog CREATE TABLE customers ( id INT PRIMARY KEY NOT ENFORCED, name STRING, country STRING, zip STRING ); -- Launch a streaming job to update customers table INSERT INTO customers ... -- Create a temporary left table, like from kafka CREATE TEMPORARY TABLE Orders ( order_id INT, total INT, customer_id INT, proc_time AS PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); Normal Lookup #  You can now use customers in a lookup join query.\n-- enrich each order with customer information SELECT o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Retry Lookup #  If the records of Orders (main table) join missing because the corresponding data of customers (lookup table) is not ready. You can consider using Flink\u0026rsquo;s Delayed Retry Strategy For Lookup. Only for Flink 1.16+.\n-- enrich each order with customer information SELECT /*+ LOOKUP(\u0026#39;table\u0026#39;=\u0026#39;c\u0026#39;, \u0026#39;retry-predicate\u0026#39;=\u0026#39;lookup_miss\u0026#39;, \u0026#39;retry-strategy\u0026#39;=\u0026#39;fixed_delay\u0026#39;, \u0026#39;fixed-delay\u0026#39;=\u0026#39;1s\u0026#39;, \u0026#39;max-attempts\u0026#39;=\u0026#39;600\u0026#39;) */ o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; Async Retry Lookup #  The problem with synchronous retry is that one record will block subsequent records, causing the entire job to be blocked. You can consider using async + allow_unordered to avoid blocking, the records that join missing will no longer block other records.\n-- enrich each order with customer information SELECT /*+ LOOKUP(\u0026#39;table\u0026#39;=\u0026#39;c\u0026#39;, \u0026#39;retry-predicate\u0026#39;=\u0026#39;lookup_miss\u0026#39;, \u0026#39;output-mode\u0026#39;=\u0026#39;allow_unordered\u0026#39;, \u0026#39;retry-strategy\u0026#39;=\u0026#39;fixed_delay\u0026#39;, \u0026#39;fixed-delay\u0026#39;=\u0026#39;1s\u0026#39;, \u0026#39;max-attempts\u0026#39;=\u0026#39;600\u0026#39;) */ o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN customers /*+ OPTIONS(\u0026#39;lookup.async\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;lookup.async-thread-number\u0026#39;=\u0026#39;16\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; If the main table (Orders) is CDC stream, allow_unordered will be ignored by Flink SQL (only supports append stream), your streaming job may be blocked. You can try to use audit_log system table feature of Paimon to walk around (convert CDC stream to append stream).  Performance #  The lookup join operator will maintain a RocksDB cache locally and pull the latest updates of the table in real time. Lookup join operator will only pull the necessary data, so your filter conditions are very important for performance.\nThis feature is only suitable for tables containing at most tens of millions of records to avoid excessive use of local disks.\nRocksDB Cache Options #  The following options allow users to finely adjust RocksDB for better performance. You can either specify them in table properties or in dynamic table hints.\n-- dynamic table hints example SELECT o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN customers /*+ OPTIONS(\u0026#39;lookup.cache-rows\u0026#39;=\u0026#39;20000\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id;    Key Default Type Description     lookup.cache-rows 10000 Long The maximum number of rows to store in the cache.   rocksdb.block.blocksize 4 kb MemorySize The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'.   rocksdb.block.cache-size 8 mb MemorySize The amount of the cache for data blocks in RocksDB. The default block-cache size is '8MB'.   rocksdb.block.metadata-blocksize 4 kb MemorySize Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'.   rocksdb.bloom-filter.bits-per-key 10.0 Double Bits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0.   rocksdb.bloom-filter.block-based-mode false Boolean If true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'.   rocksdb.compaction.level.max-size-level-base 256 mb MemorySize The upper-bound of the total size of level base files in bytes. The default value is '256MB'.   rocksdb.compaction.level.target-file-size-base 64 mb MemorySize The target file size for compaction, which determines a level-1 file size. The default value is '64MB'.   rocksdb.compaction.level.use-dynamic-size false Boolean If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc.   rocksdb.compaction.style LEVEL Enum\n The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.\nPossible values:\"LEVEL\"\"UNIVERSAL\"\"FIFO\"\"NONE\"   rocksdb.compression.type LZ4_COMPRESSION Enum\n The compression type.\nPossible values:\"NO_COMPRESSION\"\"SNAPPY_COMPRESSION\"\"ZLIB_COMPRESSION\"\"BZLIB2_COMPRESSION\"\"LZ4_COMPRESSION\"\"LZ4HC_COMPRESSION\"\"XPRESS_COMPRESSION\"\"ZSTD_COMPRESSION\"\"DISABLE_COMPRESSION_OPTION\"   rocksdb.files.open -1 Integer The maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'.   rocksdb.thread.num 2 Integer The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'.   rocksdb.use-bloom-filter false Boolean If true, every newly created SST file will contain a Bloom filter. It is disabled by default.   rocksdb.writebuffer.count 2 Integer The maximum number of write buffers that are built up in memory. The default value is '2'.   rocksdb.writebuffer.number-to-merge 1 Integer The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'.   rocksdb.writebuffer.size 64 mb MemorySize The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'.    "});index.add({'id':41,'href':'/docs/master/project/','title':"Project",'section':"Apache Paimon",'content':""});index.add({'id':42,'href':'/docs/master/maintenance/rescale-bucket/','title':"Rescale Bucket",'section':"Maintenance",'content':"Rescale Bucket #  Since the number of total buckets dramatically influences the performance, Paimon allows users to tune bucket numbers by ALTER TABLE command and reorganize data layout by INSERT OVERWRITE without recreating the table/partition. When executing overwrite jobs, the framework will automatically scan the data with the old bucket number and hash the record according to the current bucket number.\nRescale Overwrite #  -- rescale number of total buckets ALTER TABLE table_identifier SET (\u0026#39;bucket\u0026#39; = \u0026#39;...\u0026#39;); -- reorganize data layout of table/partition INSERT OVERWRITE table_identifier [PARTITION (part_spec)] SELECT ... FROM table_identifier [WHERE part_spec]; Please note that\n ALTER TABLE only modifies the table\u0026rsquo;s metadata and will NOT reorganize or reformat existing data. Reorganize existing data must be achieved by INSERT OVERWRITE. Rescale bucket number does not influence the read and running write jobs. Once the bucket number is changed, any newly scheduled INSERT INTO jobs which write to without-reorganized existing table/partition will throw a TableException with message like Try to write table/partition ... with a new bucket num ..., but the previous bucket num is ... Please switch to batch mode, and perform INSERT OVERWRITE to rescale current data layout first.  For partitioned table, it is possible to have different bucket number for different partitions. E.g. ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;4\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-01\u0026#39;) SELECT * FROM ...; ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-02\u0026#39;) SELECT * FROM ...;  During overwrite period, make sure there are no other jobs writing the same table/partition.  Note: For the table which enables log system(e.g. Kafka), please rescale the topic\u0026rsquo;s partition as well to keep consistency.  Use Case #  Rescale bucket helps to handle sudden spikes in throughput. Suppose there is a daily streaming ETL task to sync transaction data. The table\u0026rsquo;s DDL and pipeline are listed as follows.\n-- table DDL CREATE TABLE verified_orders ( trade_order_id BIGINT, item_id BIGINT, item_price DOUBLE, dt STRING, PRIMARY KEY (dt, trade_order_id, item_id) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;16\u0026#39; ); -- like from a kafka table CREATE temporary TABLE raw_orders( trade_order_id BIGINT, item_id BIGINT, item_price BIGINT, gmt_create STRING, order_status STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); -- streaming insert as bucket num = 16 INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;; The pipeline has been running well for the past few weeks. However, the data volume has grown fast recently, and the job\u0026rsquo;s latency keeps increasing. To improve the data freshness, users can\n Suspend the streaming job with a savepoint ( see Suspended State and Stopping a Job Gracefully Creating a Final Savepoint ) $ ./bin/flink stop \\  --savepointPath /tmp/flink-savepoints \\  $JOB_ID  Increase the bucket number -- scaling out ALTER TABLE verified_orders SET (\u0026#39;bucket\u0026#39; = \u0026#39;32\u0026#39;);  Switch to the batch mode and overwrite the current partition(s) to which the streaming job is writing SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- suppose today is 2022-06-22 -- case 1: there is no late event which updates the historical partitions, thus overwrite today\u0026#39;s partition is enough INSERT OVERWRITE verified_orders PARTITION (dt = \u0026#39;2022-06-22\u0026#39;) SELECT trade_order_id, item_id, item_price FROM verified_orders WHERE dt = \u0026#39;2022-06-22\u0026#39;; -- case 2: there are late events updating the historical partitions, but the range does not exceed 3 days INSERT OVERWRITE verified_orders SELECT trade_order_id, item_id, item_price, dt FROM verified_orders WHERE dt IN (\u0026#39;2022-06-20\u0026#39;, \u0026#39;2022-06-21\u0026#39;, \u0026#39;2022-06-22\u0026#39;);  After overwrite job has finished, switch back to streaming mode. And now, the parallelism can be increased alongside with bucket number to restore the streaming job from the savepoint ( see Start a SQL Job from a savepoint ) SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026lt;savepointPath\u0026gt;; INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;;   "});index.add({'id':43,'href':'/docs/master/how-to/cdc-ingestion/','title':"CDC Ingestion",'section':"How to",'content':"CDC Ingestion #  Paimon supports a variety of ways to ingest data into Paimon tables with schema evolution. This means that the added columns are synchronized to the Paimon table in real time and the synchronization job will not be restarted for this purpose.\nWe currently support the following sync ways:\n MySQL Synchronizing Table: synchronize one or multiple tables from MySQL into one Paimon table. MySQL Synchronizing Database: synchronize the whole MySQL database into one Paimon database. API Synchronizing Table: synchronize your custom DataStream input into one Paimon table. Kafka Synchronizing Table: synchronize one Kafka topic\u0026rsquo;s table into one Paimon table. Kafka Synchronizing Database: synchronize one Kafka topic containing multiple tables or multiple topics containing one table each into one Paimon database. MongoDB Synchronizing Collection: synchronize one Collection from MongoDB into one Paimon table. MongoDB Synchronizing Database: synchronize the whole MongoDB database into one Paimon database.  MySQL #  Paimon supports synchronizing changes from different databases using change data capture (CDC). This feature requires Flink and its CDC connectors.\nPrepare CDC Bundled Jar #  flink-sql-connector-mysql-cdc-*.jar Synchronizing Tables #  By using MySqlSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from MySQL into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mysql-sync-table --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition-keys \u0026lt;partition-keys\u0026gt;] \\  [--primary-keys \u0026lt;primary-keys\u0026gt;] \\  [--type-mapping \u0026lt;option1,option2...\u0026gt;] \\  [--computed-column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed-column ...]] \\  [--mysql-conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql-conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition-keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary-keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type-mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING.     --computed-column The definitions of computed columns. The argument field is from MySQL table field name. See here for a complete list of configurations.    --mysql-conf The configuration for Flink CDC MySQL sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog-conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table-conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nExample 1: synchronize tables into one Paimon table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mysql-sync-table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition-keys pt \\  --primary-keys pt,uid \\  --computed-column \u0026#39;_year=year(age)\u0026#39; \\  --mysql-conf hostname=127.0.0.1 \\  --mysql-conf username=root \\  --mysql-conf password=123456 \\  --mysql-conf database-name=\u0026#39;source_db\u0026#39; \\  --mysql-conf table-name=\u0026#39;source_table1|source_table2\u0026#39; \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 As example shows, the mysql-conf\u0026rsquo;s table-name supports regular expressions to monitor multiple tables that satisfy the regular expressions. The schemas of all the tables will be merged into one Paimon table schema.\nExample 2: synchronize shards into one Paimon table\nYou can also set \u0026lsquo;database-name\u0026rsquo; with a regular expression to capture multiple databases. A typical scenario is that a table \u0026lsquo;source_table\u0026rsquo; is split into database \u0026lsquo;source_db1\u0026rsquo;, \u0026lsquo;source_db2\u0026rsquo; \u0026hellip;, then you can synchronize data of all the \u0026lsquo;source_table\u0026rsquo;s into one Paimon table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mysql-sync-table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition-keys pt \\  --primary-keys pt,uid \\  --computed-column \u0026#39;_year=year(age)\u0026#39; \\  --mysql-conf hostname=127.0.0.1 \\  --mysql-conf username=root \\  --mysql-conf password=123456 \\  --mysql-conf database-name=\u0026#39;source_db.+\u0026#39; \\  --mysql-conf table-name=\u0026#39;source_table\u0026#39; \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 Synchronizing Databases #  By using MySqlSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MySQL database into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mysql-sync-database --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--ignore-incompatible \u0026lt;true/false\u0026gt;] \\  [--merge-shards \u0026lt;true/false\u0026gt;] \\  [--table-prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table-suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including-tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\  [--excluding-tables \u0026lt;mysql-table-name|name-regular-expr\u0026gt;] \\  [--mode \u0026lt;sync-mode\u0026gt;] \\  [--type-mapping \u0026lt;option1,option2...\u0026gt;] \\  [--mysql-conf \u0026lt;mysql-cdc-source-conf\u0026gt; [--mysql-conf \u0026lt;mysql-cdc-source-conf\u0026gt; ...]] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --ignore-incompatible It is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.   --merge-shards It is default true, in this case, if some tables in different databases have the same name, their schemas will be merged and their records will be synchronized into one Paimon table. Otherwise, each table's records will be synchronized to a corresponding Paimon table, and the Paimon table will be named to 'databaseName_tableName' to avoid potential name conflict.   --table-prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table-prefix ods_\".   --table-suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table-prefix\".   --including-tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including-tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding-tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including-tables\". \"--excluding-tables\" has higher priority than \"--including-tables\" if you specified both.   --mode It is used to specify synchronization mode.\nPossible values:\"divided\" (the default mode if you haven't specified one): start a sink for each table, the synchronization of the new table requires restarting the job.\"combined\": start a single combined sink for all tables, the new table will be automatically synchronized.   --type-mapping It is used to specify how to map MySQL data type to Paimon type.\nSupported options:  \"tinyint1-not-bool\": maps MySQL TINYINT(1) to TINYINT instead of BOOLEAN. \"to-nullable\": ignores all NOT NULL constraints (except for primary keys). This is used to solve the problem that Flink cannot accept the MySQL 'ALTER TABLE ADD COLUMN column type NOT NULL DEFAULT x' operation.  \"to-string\": maps all MySQL types to STRING.     --mysql-conf The configuration for Flink CDC MySQL sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog-conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table-conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Only tables with primary keys will be synchronized.\nFor each MySQL table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MySQL tables. If the Paimon table already exists, its schema will be compared against the schema of all specified MySQL tables.\nExample 1: synchronize entire database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mysql-sync-database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql-conf hostname=127.0.0.1 \\  --mysql-conf username=root \\  --mysql-conf password=123456 \\  --mysql-conf database-name=source_db \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 Example 2: synchronize newly added tables under database\nLet\u0026rsquo;s say at first a Flink job is synchronizing tables [product, user, address] under database source_db. The command to submit the job looks like:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mysql-sync-database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql-conf hostname=127.0.0.1 \\  --mysql-conf username=root \\  --mysql-conf password=123456 \\  --mysql-conf database-name=source_db \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 \\  --including-tables \u0026#39;product|user|address\u0026#39; At a later point we would like the job to also synchronize tables [order, custom], which contains history data. We can achieve this by recovering from the previous snapshot of the job and thus reusing existing state of the job. The recovered job will first snapshot newly added tables, and then continue reading changelog from previous position automatically.\nThe command to recover from previous snapshot and add new tables to synchronize looks like:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  --fromSavepoint savepointPath \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mysql-sync-database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql-conf hostname=127.0.0.1 \\  --mysql-conf username=root \\  --mysql-conf password=123456 \\  --mysql-conf database-name=source_db \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --including-tables \u0026#39;product|user|address|order|custom\u0026#39; You can set --mode combined to enable synchronizing newly added tables without restarting job.  Example 3: synchronize and merge multiple shards\nLet\u0026rsquo;s say you have multiple database shards db1, db2, \u0026hellip; and each database has tables tbl1, tbl2, \u0026hellip;. You can synchronize all the db.+.tbl.+ into tables test_db.tbl1, test_db.tbl2 \u0026hellip; by following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mysql-sync-database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mysql-conf hostname=127.0.0.1 \\  --mysql-conf username=root \\  --mysql-conf password=123456 \\  --mysql-conf database-name=\u0026#39;db.+\u0026#39; \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 \\  --including-tables \u0026#39;tbl.+\u0026#39; By setting database-name to a regular expression, the synchronization job will capture all tables under matched databases and merge tables of the same name into one table.\nYou can set --merge-shards false to prevent merging shards. The synchronized tables will be named to \u0026lsquo;databaseName_tableName\u0026rsquo; to avoid potential name conflict.  Kafka #  Prepare Kafka Bundled Jar #  flink-sql-connector-kafka-*.jar Supported Formats #  Flink provides several Kafka CDC formats: Canal, Debezium, Ogg and Maxwell JSON. If a message in a Kafka topic is a change event captured from another database using the Change Data Capture (CDC) tool, then you can use the Paimon Kafka CDC. Write the INSERT, UPDATE, DELETE messages parsed into the paimon table.\n  Formats Supported     Canal CDC True   Debezium CDC False   }}\"Maxwell CDC False   OGG CDC True    In Oracle GoldenGate, the data format synchronized to Kafka does not include field data type information. As a result, Paimon sets the data type for all fields to \u0026ldquo;String\u0026rdquo; by default.  Synchronizing Tables #  By using KafkaSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one or multiple tables from Kafka\u0026rsquo;s one topic into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  kafka-sync-table --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition-keys \u0026lt;partition-keys\u0026gt;] \\  [--primary-keys \u0026lt;primary-keys\u0026gt;] \\  [--type-mapping to-string] \\  [--computed-column \u0026lt;\u0026#39;column-name=expr-name(args[, ...])\u0026#39;\u0026gt; [--computed-column ...]] \\  [--kafka-conf \u0026lt;kafka-source-conf\u0026gt; [--kafka-conf \u0026lt;kafka-source-conf\u0026gt; ...]] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition-keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --primary-keys The primary keys for Paimon table. If there are multiple primary keys, connect them with comma, for example \"buyer_id,seller_id\".   --type-mapping It is used to specify how to map MySQL data type to Paimon type. Currently, only support option \"to-string\": maps all MySQL types to STRING.   --computed-column The definitions of computed columns. The argument field is from Kafka topic's table field name. See here for a complete list of configurations.    --kafka-conf The configuration for Flink Kafka sources. Each configuration should be specified in the format `key=value`. `properties.bootstrap.servers`, `topic`, `properties.group.id`, and `value.format` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog-conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table-conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from all specified Kafka topic\u0026rsquo;s tables,it gets the earliest non-DDL data parsing schema from topic. If the Paimon table already exists, its schema will be compared against the schema of all specified Kafka topic\u0026rsquo;s tables.\nExample\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  kafka-sync-table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition-keys pt \\  --primary-keys pt,uid \\  --computed-column \u0026#39;_year=year(age)\u0026#39; \\  --kafka-conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka-conf topic=order \\  --kafka-conf properties.group.id=123456 \\  --kafka-conf value.format=canal-json \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 Synchronizing Databases #  By using KafkaSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the multi topic or one topic into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  kafka-sync-database --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--table-prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table-suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including-tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--excluding-tables \u0026lt;table-name|name-regular-expr\u0026gt;] \\  [--type-mapping to-string] \\  [--kafka-conf \u0026lt;kafka-source-conf\u0026gt; [--kafka-conf \u0026lt;kafka-source-conf\u0026gt; ...]] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --schema-init-max-read If your tables are all from a topic, you can set this parameter to initialize the number of tables to be synchronized. The default value is 1000.   --ignore-incompatible It is default false, in this case, if MySQL table name exists in Paimon and their schema is incompatible,an exception will be thrown. You can specify it to true explicitly to ignore the incompatible tables and exception.   --table-prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table-prefix ods_\".   --table-suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table-prefix\".   --including-tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including-tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding-tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including-tables\". \"--excluding-tables\" has higher priority than \"--including-tables\" if you specified both.   --type-mapping It is used to specify how to map MySQL data type to Paimon type. Currently, only support option \"to-string\": maps all MySQL types to STRING.   --kafka-conf The configuration for Flink Kafka sources. Each configuration should be specified in the format `key=value`. `properties.bootstrap.servers`, `topic`, `properties.group.id`, and `value.format` are required configurations, others are optional.See its document for a complete list of configurations.   --catalog-conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table-conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Only tables with primary keys will be synchronized.\nThis action will build a single combined sink for all tables. For each Kafka topic\u0026rsquo;s table to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table, and its schema will be derived from all specified Kafka topic\u0026rsquo;s tables. If the Paimon table already exists and its schema is different from that parsed from Kafka record, this action will try to preform schema evolution.\nExample\nSynchronization from one Kafka topic to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  kafka-sync-database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --kafka-conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka-conf topic=order \\  --kafka-conf properties.group.id=123456 \\  --kafka-conf value.format=canal-json \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 Synchronization from multiple Kafka topics to Paimon database.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  kafka-sync-database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --kafka-conf properties.bootstrap.servers=127.0.0.1:9020 \\  --kafka-conf topic=order\\;logistic_order\\;user \\  --kafka-conf properties.group.id=123456 \\  --kafka-conf value.format=canal-json \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 MongoDB #  Prepare MongoDB Bundled Jar #  flink-sql-connector-mongodb-*.jar Synchronizing Tables #  By using MongoDBSyncTableAction in a Flink DataStream job or directly through flink run, users can synchronize one collection from MongoDB into one Paimon table.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mongodb-sync-table --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  [--partition-keys \u0026lt;partition-keys\u0026gt;] \\  [--mongodb-conf \u0026lt;mongodb-cdc-source-conf\u0026gt; [--mongodb-conf \u0026lt;mongodb-cdc-source-conf\u0026gt; ...]] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table The Paimon table name.   --partition-keys The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \"dt,hh,mm\".   --mongodb-conf The configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog-conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table-conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    Here are a few points to take note of:\n The \u0026ldquo;mongodb-conf\u0026rdquo; introduces the \u0026ldquo;schema.start.mode\u0026rdquo; parameter on top of the MongoDB CDC source configuration.\u0026ldquo;schema.start.mode\u0026rdquo; provides two modes: \u0026ldquo;dynamic\u0026rdquo; (default) and \u0026ldquo;specified\u0026rdquo;. In \u0026ldquo;dynamic\u0026rdquo; mode, MongoDB schema information is parsed at one level, which forms the basis for schema change evolution. In \u0026ldquo;specified\u0026rdquo; mode, synchronization takes place according to specified criteria. This can be done by configuring \u0026ldquo;field.name\u0026rdquo; to specify the synchronization fields and \u0026ldquo;parser.path\u0026rdquo; to specify the JSON parsing path for those fields. The difference between the two is that the \u0026ldquo;specify\u0026rdquo; mode requires the user to explicitly identify the fields to be used and create a mapping table based on those fields. Dynamic mode, on the other hand, ensures that Paimon and MongoDB always keep the top-level fields consistent, eliminating the need to focus on specific fields. Further processing of the data table is required when using values from nested fields.    Operator Description     $ The root element to query. This starts all path expressions.   @ The current node being processed by a filter predicate.   * Wildcard. Available anywhere a name or numeric are required.   .. Deep scan. Available anywhere a name is required.   . Dot-notated child.   ['{name}' (, '{name}')] Bracket-notated child or children.   [{number} (, {number})] Bracket-notated child or children.   [start:end] Array index or indexes.   [?({expression})] Filter expression. Expression must evaluate to a boolean value.    Functions can be invoked at the tail end of a path - the input to a function is the output of the path expression. The function output is dictated by the function itself.\n  Function Description Output type     min() Provides the min value of an array of numbers. Double   max() Provides the max value of an array of numbers. Double   avg() Provides the average value of an array of numbers. Double   stddev() Provides the standard deviation value of an array of numbers Double   length() Provides the length of an array Integer   sum() Provides the sum value of an array of numbers. Double   keys() Provides the property keys (An alternative for terminal tilde ~) Set   concat(X) Provides a concatinated version of the path output with a new item. like input   append(X) add an item to the json path output array like input   append(X) add an item to the json path output array like input   first() Provides the first item of an array Depends on the array   last() Provides the last item of an array Depends on the array   index(X) Provides the item of an array of index: X, if the X is negative, take from backwards Depends on the array    Path Examples\n{ \u0026#34;store\u0026#34;: { \u0026#34;book\u0026#34;: [ { \u0026#34;category\u0026#34;: \u0026#34;reference\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Nigel Rees\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sayings of the Century\u0026#34;, \u0026#34;price\u0026#34;: 8.95 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Evelyn Waugh\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sword of Honour\u0026#34;, \u0026#34;price\u0026#34;: 12.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Herman Melville\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Moby Dick\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-553-21311-3\u0026#34;, \u0026#34;price\u0026#34;: 8.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;J. R. R. Tolkien\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;The Lord of the Rings\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-395-19395-8\u0026#34;, \u0026#34;price\u0026#34;: 22.99 } ], \u0026#34;bicycle\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;price\u0026#34;: 19.95 } }, \u0026#34;expensive\u0026#34;: 10 }    JsonPath Result     $.store.book[*].author Provides the min value of an array of numbers.   $..author All authors.   $.store.* All things, both books and bicycles.   $.store..price Provides the standard deviation value of an array of numbers.   $..book[2] The third book.   $..book[-2] The second to last book.   $..book[0,1] The first two books.   $..book[:2] All books from index 0 (inclusive) until index 2 (exclusive).   $..book[1:2] All books from index 1 (inclusive) until index 2 (exclusive)   $..book[-2:] Last two books   $..book[2:] All books from index 2 (inclusive) to last   $..book[?(@.isbn)] All books with an ISBN number   $.store.book[?(@.price \u0026lt; 10)] All books in store cheaper than 10   $..book[?(@.price \u0026lt;= $['expensive'])] All books in store that are not \"expensive\"   $..book[?(@.author =~ /.*REES/i)] All books matching regex (ignore case)   $..* Give me every thing   $..book.length() The number of books     The synchronized table is required to have its primary key set as _id. This is because MongoDB\u0026rsquo;s change events are recorded before updates in messages. Consequently, we can only convert them into Flink\u0026rsquo;s UPSERT change log stream. The upstart stream demands a unique key, which is why we must declare _id as the primary key. Declaring other columns as primary keys is not feasible, as delete operations only encompass the _id and sharding key, excluding other keys and values.\n  MongoDB Change Streams are designed to return simple JSON documents without any data type definitions. This is because MongoDB is a document-oriented database, and one of its core features is the dynamic schema, where documents can contain different fields, and the data types of fields can be flexible. Therefore, the absence of data type definitions in Change Streams is to maintain this flexibility and extensibility. For this reason, we have set all field data types for synchronizing MongoDB to Paimon as String to address the issue of not being able to obtain data types.\n  If the Paimon table you specify does not exist, this action will automatically create the table. Its schema will be derived from MongoDB collection.\nExample 1: synchronize collection into one Paimon table\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mongodb-sync-table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition-keys pt \\  --mongodb-conf hosts=127.0.0.1:27017 \\  --mongodb-conf username=root \\  --mongodb-conf password=123456 \\  --mongodb-conf database=source_db \\  --mongodb-conf collection=source_table1 \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 Example 2: Synchronize collection into a Paimon table according to the specified field mapping.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mongodb-sync-table \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --table test_table \\  --partition-keys pt \\  --mongodb-conf hosts=127.0.0.1:27017 \\  --mongodb-conf username=root \\  --mongodb-conf password=123456 \\  --mongodb-conf database=source_db \\  --mongodb-conf collection=source_table1 \\  --mongodb-conf schema.start.mode=specified \\  --mongodb-conf field.name=_id,name,description \\  --mongodb-conf parser.path=$._id,$.name,$.description \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 Synchronizing Databases #  By using MongoDBSyncDatabaseAction in a Flink DataStream job or directly through flink run, users can synchronize the whole MongoDB database into one Paimon database.\nTo use this feature through flink run, run the following shell command.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mongodb-sync-database --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  [--table-prefix \u0026lt;paimon-table-prefix\u0026gt;] \\  [--table-suffix \u0026lt;paimon-table-suffix\u0026gt;] \\  [--including-tables \u0026lt;mongodb-table-name|name-regular-expr\u0026gt;] \\  [--excluding-tables \u0026lt;mongodb-table-name|name-regular-expr\u0026gt;] \\  [--mongodb-conf \u0026lt;mongodb-cdc-source-conf\u0026gt; [--mongodb-conf \u0026lt;mongodb-cdc-source-conf\u0026gt; ...]] \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] \\  [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; [--table-conf \u0026lt;paimon-table-sink-conf\u0026gt; ...]]    Configuration Description     --warehouse The path to Paimon warehouse.   --database The database name in Paimon catalog.   --table-prefix The prefix of all Paimon tables to be synchronized. For example, if you want all synchronized tables to have \"ods_\" as prefix, you can specify \"--table-prefix ods_\".   --table-suffix The suffix of all Paimon tables to be synchronized. The usage is same as \"--table-prefix\".   --including-tables It is used to specify which source tables are to be synchronized. You must use '|' to separate multiple tables.Because '|' is a special character, a comma is required, for example: 'a|b|c'.Regular expression is supported, for example, specifying \"--including-tables test|paimon.*\" means to synchronize table 'test' and all tables start with 'paimon'.   --excluding-tables It is used to specify which source tables are not to be synchronized. The usage is same as \"--including-tables\". \"--excluding-tables\" has higher priority than \"--including-tables\" if you specified both.   --mongodb-conf The configuration for Flink CDC MongoDB sources. Each configuration should be specified in the format \"key=value\". hostname, username, password, database-name and table-name are required configurations, others are optional. See its document for a complete list of configurations.   --catalog-conf The configuration for Paimon catalog. Each configuration should be specified in the format \"key=value\". See here for a complete list of catalog configurations.   --table-conf The configuration for Paimon table sink. Each configuration should be specified in the format \"key=value\". See here for a complete list of table configurations.    All collections to be synchronized need to set _id as the primary key. For each MongoDB collection to be synchronized, if the corresponding Paimon table does not exist, this action will automatically create the table. Its schema will be derived from all specified MongoDB collection. If the Paimon table already exists, its schema will be compared against the schema of all specified MongoDB collection. Any MongoDB tables created after the commencement of the task will automatically be included.\nExample 1: synchronize entire database\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  mongodb-sync-database \\  --warehouse hdfs:///path/to/warehouse \\  --database test_db \\  --mongodb-conf hosts=127.0.0.1:27017 \\  --mongodb-conf username=root \\  --mongodb-conf password=123456 \\  --mongodb-conf database=source_db \\  --catalog-conf metastore=hive \\  --catalog-conf uri=thrift://hive-metastore:9083 \\  --table-conf bucket=4 \\  --table-conf changelog-producer=input \\  --table-conf sink.parallelism=4 Example 2: Synchronize the specified table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\ --fromSavepoint savepointPath \\ /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\ mongodb-sync-database \\ --warehouse hdfs:///path/to/warehouse \\ --database test_db \\ --mongodb-conf hosts=127.0.0.1:27017 \\ --mongodb-conf username=root \\ --mongodb-conf password=123456 \\ --mongodb-conf database=source_db \\ --catalog-conf metastore=hive \\ --catalog-conf uri=thrift://hive-metastore:9083 \\ --table-conf bucket=4 \\ --including-tables \u0026#39;product|user|address|order|custom\u0026#39; Schema Change Evolution #  Cdc Ingestion supports a limited number of schema changes. Currently, the framework can not rename table, drop columns, so the behaviors of RENAME TABLE and DROP COLUMN will be ignored, RENAME COLUMN will add a new column. Currently supported schema changes includes:\n  Adding columns.\n  Altering column types. More specifically,\n altering from a string type (char, varchar, text) to another string type with longer length, altering from a binary type (binary, varbinary, blob) to another binary type with longer length, altering from an integer type (tinyint, smallint, int, bigint) to another integer type with wider range, altering from a floating-point type (float, double) to another floating-point type with wider range,  are supported.\n  Computed Functions #  --computed-column are the definitions of computed columns. The argument field is from Kafka topic\u0026rsquo;s table field name. Supported expressions are:\n  Function Description     year(date-column) Extract year from a DATE, DATETIME or TIMESTAMP (or its corresponding string format). Output is an INT value represent the year.   month(date-column) Extract month of year from a DATE, DATETIME or TIMESTAMP (or its corresponding string format). Output is an INT value represent the month of year.   day(date-column) Extract day of month from a DATE, DATETIME or TIMESTAMP (or its corresponding string format). Output is an INT value represent the day of month.   hour(date-column) Extract hour from a DATE, DATETIME or TIMESTAMP (or its corresponding string format). Output is an INT value represent the hour.   date_format(date-column) Convert date format from a DATE, DATETIME or TIMESTAMP (or its corresponding string format). Output is a string value in converted date format.   substring(column,beginInclusive) Get column.substring(beginInclusive). Output is a STRING.   substring(column,beginInclusive,endExclusive) Get column.substring(beginInclusive,endExclusive). Output is a STRING.   truncate(column,width) truncate column by width. Output type is same with column.If the column is a STRING, truncate(column,width) will truncate the string to width characters, namely `value.substring(0, width)`. If the column is an INT or LONG, truncate(column,width) will truncate the number with the algorithm `v - (((v % W) + W) % W)`. The `redundant` compute part is to keep the result always positive. If the column is a DECIMAL, truncate(column,width) will truncate the decimal with the algorithm: let `scaled_W = decimal(W, scale(v))`, then return `v - (v % scaled_W)`.    Special Data Type Mapping #   MySQL TINYINT(1) type will be mapped to Boolean by default. If you want to store number (-128~127) in it like MySQL, you can specify type mapping option tinyint1-not-bool (Use --type-mapping), then the column will be mapped to TINYINT in Paimon table. You can use type mapping option to-nullable (Use --type-mapping) to ignore all NOT NULL constraints (except primary keys). You can use type mapping option to-string (Use --type-mapping) to map all MySQL data type to STRING. MySQL BIT(1) type will be mapped to Boolean. When using Hive catalog, MySQL TIME type will be mapped to STRING. MySQL BINARY will be mapped to Paimon VARBINARY. This is because the binary value is passed as bytes in binlog, so it should be mapped to byte type (BYTES or VARBINARY). We choose VARBINARY because it can retain the length information.  FAQ #   Chinese characters in records ingested from MySQL are garbled.   Try to set env.java.opts: -Dfile.encoding=UTF-8 in flink-conf.yaml (the option is changed to env.java.opts.all since Flink-1.17).  "});index.add({'id':44,'href':'/docs/master/concepts/external-log-systems/','title':"External Log Systems",'section':"Concepts",'content':"External Log Systems #  Aside from underlying table files, changelog of Paimon can also be stored into or consumed from an external log system, such as Kafka. By specifying log.system table property, users can choose which external log system to use.\nIf an external log system is used, all records written into table files will also be written into the log system. Changes produced by the streaming queries will thus come from the log system instead of table files.\nConsistency Guarantees #  By default, changes in the log systems are visible to consumers only after a snapshot, just like table files. This behavior guarantees the exactly-once semantics. That is, each record is seen by the consumers exactly once.\nHowever, users can also specify the table property 'log.consistency' = 'eventual' so that changelog written into the log system can be immediately consumed by the consumers, without waiting for the next snapshot. This behavior decreases the latency of changelog, but it can only guarantee the at-least-once semantics (that is, consumers might see duplicated records) due to possible failures.\nIf 'log.consistency' = 'eventual' is set, in order to achieve correct results, Paimon source in Flink will automatically adds a \u0026ldquo;normalize\u0026rdquo; operator for deduplication. This operator persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided.\nSupported Log Systems #  Kafka #  Preparing flink-sql-connector-kafka Jar File #  Paimon currently supports Flink 1.17, 1.16, 1.15 and 1.14. We recommend the latest Flink version for a better experience.\nDownload the flink-sql-connector-kafka jar file with corresponding version.\n   Version Jar     Flink 1.17 flink-sql-connector-kafka-1.17.0.jar   Flink 1.16 flink-sql-connector-kafka-1.16.1.jar   Flink 1.15 flink-sql-connector-kafka-1.15.4.jar   Flink 1.14 flink-sql-connector-kafka_2.11-1.14.4.jar    By specifying 'log.system' = 'kafka', users can write changes into Kafka along with table files.\nFlink CREATE TABLE T (...) WITH ( \u0026#39;log.system\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;kafka.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;kafka.topic\u0026#39; = \u0026#39;...\u0026#39; );  Table Properties for Kafka are listed as follows.\n  Key Default Type Description     kafka.bootstrap.servers (none) String Required Kafka server connection string.   kafka.topic (none) String Topic of this kafka table.    Automatically Register #  Currently only Flink supports automatic registration.  If 'log.system.auto-register' = 'true' is set, paimon will automatically create and delete the topic in log system for the table. Otherwise, user needs to manually manage the topic in the log system. More related table properties can refer here.\n"});index.add({'id':45,'href':'/docs/master/maintenance/manage-tags/','title':"Manage Tags",'section':"Maintenance",'content':"Manage Tags #  Paimon\u0026rsquo;s snapshots can provide a easy way to query historical data. But in most scenarios, a job will generate too many snapshots and table will expire old snapshots according to table configuration. Snapshot expiration will also delete old data files, and the historical data of expired snapshots cannot be queried anymore.\nTo solve this problem, you can create a tag based on a snapshot. The tag will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.\nAutomatic Creation #  Paimon supports automatic creation of tags in writing job.\nStep 1: Choose Creation Mode\nYou can set 'tag.automatic-creation' to process-time or watermark:\n process-time: Create TAG based on the time of the machine. watermark: Create TAG based on the watermark of the Sink input.  If you choose Watermark, you may need to specify the time zone of watermark, if watermark is not in the UTC time zone, please configure 'sink.watermark-time-zone'.  Step 2: Choose Creation Period\nWhat frequency is used to generate tags. You can choose 'daily', 'hourly' and 'two-hours' for 'tag.creation-period'.\nIf you need to wait for late data, you can configure a delay time: 'tag.creation-delay'.\nStep 3: Automatic deletion of tags\nYou can configure 'tag.num-retained-max' to delete tags automatically.\nExample, configure table to create a tag at 0:10 every day, with a maximum retention time of 3 months:\n-- Flink SQL CREATE TABLE T ( k INT PRIMARY KEY NOT ENFORCED, f0 INT, ... ) WITH ( \u0026#39;tag.automatic-creation\u0026#39; = \u0026#39;process-time\u0026#39;, \u0026#39;tag.creation-period\u0026#39; = \u0026#39;daily\u0026#39;, \u0026#39;tag.creation-delay\u0026#39; = \u0026#39;10 m\u0026#39;, \u0026#39;tag.num-retained-max\u0026#39; = \u0026#39;90\u0026#39; ); INSERT INTO T SELECT ...; -- Spark SQL  -- Read latest snapshot SELECT * FROM T; -- Read Tag snapshot SELECT * FROM T VERSION AS OF \u0026#39;2023-07-26\u0026#39;; -- Read Incremental between Tags SELECT * FROM paimon_incremental_query(\u0026#39;T\u0026#39;, \u0026#39;2023-07-25\u0026#39;, \u0026#39;2023-07-26\u0026#39;); Create Tags #  You can create a tag with given name (cannot be number) and snapshot ID.\nFlink Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  create-tag \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --tag-name \u0026lt;tag-name\u0026gt; \\  --snapshot \u0026lt;snapshot-id\u0026gt; \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class CreateTag { public static void main(String[] args) { Table table = ...; table.createTag(\u0026#34;my-tag\u0026#34;, 1); } } Spark Run the following sql:\nCALL create_tag(table =\u0026gt; \u0026#39;test.T\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;, snapshot =\u0026gt; 2);  Delete Tags #  You can delete a tag by its name.\nFlink Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  delete-tag \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --tag-name \u0026lt;tag-name\u0026gt; \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class DeleteTag { public static void main(String[] args) { Table table = ...; table.deleteTag(\u0026#34;my-tag\u0026#34;); } } Spark Run the following sql:\nCALL delete_tag(table =\u0026gt; \u0026#39;test.T\u0026#39;, tag =\u0026gt; \u0026#39;test_tag\u0026#39;);  Rollback to Tag #  Rollback table to a specific tag. All snapshots and tags whose snapshot id is larger than the tag will be deleted (and the data will be deleted too).\nFlink Run the following command:\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  /path/to/paimon-flink-action-0.5-SNAPSHOT.jar \\  rollback-to \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; \\  --vesion \u0026lt;tag-name\u0026gt; \\  [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; [--catalog-conf \u0026lt;paimon-catalog-conf\u0026gt; ...]] Java API import org.apache.paimon.table.Table; public class RollbackTo { public static void main(String[] args) { // before rollback:  // snapshot-3 [expired] -\u0026gt; tag3  // snapshot-4 [expired]  // snapshot-5 -\u0026gt; tag5  // snapshot-6  // snapshot-7  table.rollbackTo(\u0026#34;tag3\u0026#34;); // after rollback:  // snapshot-3 -\u0026gt; tag3  } } Spark Run the following sql:\nCALL rollback(table =\u0026gt; \u0026#39;test.T\u0026#39;, version =\u0026gt; \u0026#39;2\u0026#39;);  Work with Flink Savepoint #  In Flink, we may consume from kafka and then write to paimon. Since flink\u0026rsquo;s checkpoint only retains a limited number, we will trigger a savepoint at certain time (such as code upgrades, data updates, etc.) to ensure that the state can be retained for a longer time, so that the job can be restored incrementally.\nPaimon\u0026rsquo;s snapshot is similar to flink\u0026rsquo;s checkpoint, and both will automatically expire, but the tag feature of paimon allows snapshots to be retained for a long time. Therefore, we can combine the two features of paimon\u0026rsquo;s tag and flink\u0026rsquo;s savepoint to achieve incremental recovery of job from the specified savepoint.\nStep 1: Enable automatically create tags for savepoint.\nYou can set sink.savepoint.auto-tag to true to enable the feature of automatically creating tags for savepoint.\nStep 2: Trigger savepoint.\nYou can refer to flink savepoint to learn how to configure and trigger savepoint.\nStep 3: Choose the tag corresponding to the savepoint.\nThe tag corresponding to the savepoint will be named in the form of savepoint-${savepointID}. You can refer to Tags Table to query.\nStep 4: Rollback the paimon table.\nRollback the paimon table to the specified tag.\nStep 5: Restart from the savepoint.\nYou can refer to here to learn how to restart from a specified savepoint.\n"});index.add({'id':46,'href':'/docs/master/maintenance/configurations/','title':"Configurations",'section':"Maintenance",'content':"Configuration #  CoreOptions #  Core options for paimon.\n  Key Default Type Description     auto-create false Boolean Whether to create underlying storage when reading and writing the table.   bucket 1 Integer Bucket number for file store.   bucket-key (none) String Specify the paimon distribution policy. Data is assigned to each bucket according to the hash value of bucket-key.\nIf you specify multiple fields, delimiter is ','.\nIf not specified, the primary key will be used; if there is no primary key, the full row will be used.   changelog-producer none Enum\n Whether to double write to a changelog file. This changelog file keeps the details of data changes, it can be read directly during stream reads. This can be applied to tables with primary keys. Possible values:\"none\": No changelog file.\"input\": Double write to a changelog file when flushing memory table, the changelog is from input.\"full-compaction\": Generate changelog files with each full compaction.\"lookup\": Generate changelog files through 'lookup' before committing the data writing.   changelog-producer.row-deduplicate false Boolean Whether to generate -U, +U changelog for the same record. This configuration is only valid for the changelog-producer is lookup or full-compaction.   commit.callback.#.param (none) String Parameter string for the constructor of class #. Callback class should parse the parameter by itself.   commit.callbacks (none) String A list of commit callback classes to be called after a successful commit. Class names are connected with comma (example: com.test.CallbackA,com.sample.CallbackB).   commit.force-compact false Boolean Whether to force a compaction before commit.   compaction.max-size-amplification-percent 200 Integer The size amplification is defined as the amount (in percentage) of additional storage needed to store a single byte of data in the merge tree for changelog mode table.   compaction.max.file-num 50 Integer For file set [f_0,...,f_N], the maximum file number to trigger a compaction for append-only table, even if sum(size(f_i)) \u0026lt; targetFileSize. This value avoids pending too much small files, which slows down the performance.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number which satisfies sum(size(f_i)) \u0026gt;= targetFileSize to trigger a compaction for append-only table. This value avoids almost-full-file to be compacted, which is not cost-effective.   compaction.size-ratio 1 Integer Percentage flexibility while comparing sorted run size for changelog mode table. If the candidate sorted run(s) size is 1% smaller than the next sorted run's size, then include next sorted run into this candidate set.   consumer-id (none) String Consumer id for recording the offset of consumption in the storage.   consumer.expiration-time (none) Duration The expiration interval of consumer files. A consumer file will be expired if it's lifetime after last modification is over this value.   continuous.discovery-interval 10 s Duration The discovery interval of continuous reading.   dynamic-bucket.assigner-parallelism (none) Integer Parallelism of assigner operator for dynamic bucket mode, it is related to the number of initialized bucket, too small will lead to insufficient processing speed of assigner.   dynamic-bucket.target-row-num 2000000 Long If the bucket is -1, for primary key table, is dynamic bucket mode, this option controls the target row number for one bucket.   dynamic-partition-overwrite true Boolean Whether only overwrite dynamic partition when overwriting a partitioned table with dynamic partition columns. Works only when the table has partition keys.   file.compression (none) String Default file compression format, can be overridden by file.compression.per.level   file.compression.per.level  Map Define different compression policies for different level, you can add the conf like this: 'file.compression.per.level' = '0:lz4,1:zlib', for orc file format, the compression value could be NONE, ZLIB, SNAPPY, LZO, LZ4, for parquet file format, the compression value could be UNCOMPRESSED, SNAPPY, GZIP, LZO, BROTLI, LZ4, ZSTD.   file.format orc Enum\n Specify the message format of data files, currently orc, parquet and avro are supported.\nPossible values:\"orc\": ORC file format.\"parquet\": Parquet file format.\"avro\": Avro file format.   file.format.per.level  Map Define different file format for different level, you can add the conf like this: 'file.format.per.level' = '0:avro,3:parquet', if the file format for level is not provided, the default format which set by `file.format` will be used.   full-compaction.delta-commits (none) Integer Full compaction will be constantly triggered after delta commits.   incremental-between (none) String Read incremental changes between start snapshot (exclusive) and end snapshot, for example, '5,10' means changes between snapshot 5 and snapshot 10.   incremental-between-timestamp (none) String Read incremental changes between start timestamp (exclusive) and end timestamp, for example, 't1,t2' means changes between timestamp t1 and timestamp t2.   local-merge-buffer-size (none) MemorySize Local merge will buffer and merge input records before they're shuffled by bucket and written into sink. The buffer will be flushed when it is full. Mainly to resolve data skew on primary keys. We recommend starting with 64 mb when trying out this feature.   local-sort.max-num-file-handles 128 Integer The maximal fan-in for external merge sort. It limits the number of file handles. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading.   log.changelog-mode auto Enum\n Specify the log changelog mode for table.\nPossible values:\"auto\": Upsert for table with primary key, all for table without primary key.\"all\": The log system stores all changes including UPDATE_BEFORE.\"upsert\": The log system does not store the UPDATE_BEFORE changes, the log consumed job will automatically add the normalized node, relying on the state to generate the required update_before.   log.consistency transactional Enum\n Specify the log consistency mode for table.\nPossible values:\"transactional\": Only the data after the checkpoint can be seen by readers, the latency depends on checkpoint interval.\"eventual\": Immediate data visibility, you may see some intermediate states, but eventually the right results will be produced, only works for table with primary key.   log.format \"debezium-json\" String Specify the message format of log system.   log.key.format \"json\" String Specify the key message format of log system with primary key.   lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size 9223372036854775807 bytes MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.   lookup.hash-load-factor 0.75 Float The index load factor for lookup.   manifest.format avro Enum\n Specify the message format of manifest files.\nPossible values:\"orc\": ORC file format.\"parquet\": Parquet file format.\"avro\": Avro file format.   manifest.full-compaction-threshold-size 16 mb MemorySize The size threshold for triggering full compaction of manifest.   manifest.merge-min-count 30 Integer To avoid frequent manifest merges, this parameter specifies the minimum number of ManifestFileMeta to merge.   manifest.target-file-size 8 mb MemorySize Suggested file size of a manifest file.   merge-engine deduplicate Enum\n Specify the merge engine for table with primary key.\nPossible values:\"deduplicate\": De-duplicate and keep the last row.\"partial-update\": Partial update non-null fields.\"aggregation\": Aggregate fields with same primary key.\"first-row\": De-duplicate and keep the first row.   metadata.stats-mode \"truncate(16)\" String The mode of metadata stats collection. none, counts, truncate(16), full is available.\n\"none\": means disable the metadata stats collection.\"counts\" means only collect the null count.\"full\": means collect the null count, min/max value.\"truncate(16)\": means collect the null count, min/max value with truncated length of 16.Field level stats mode can be specified by fields.{field_name}.stats-mode   metastore.partitioned-table false Boolean Whether to create this table as a partitioned table in metastore. For example, if you want to list all partitions of a Paimon table in Hive, you need to create this table as a partitioned table in Hive metastore. This config option does not affect the default filesystem metastore.   num-levels (none) Integer Total level number, for example, there are 3 levels, including 0,1,2 levels.   num-sorted-run.compaction-trigger 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).   num-sorted-run.stop-trigger (none) Integer The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 1.   orc.bloom.filter.columns (none) String A comma-separated list of columns for which to create a bloom filter when writing.   orc.bloom.filter.fpp 0.05 Double Define the default false positive probability for bloom filters.   orc.column.encoding.direct (none) Integer Comma-separated list of fields for which dictionary encoding is to be skipped in orc.   orc.dictionary.key.threshold (none) Integer If the number of distinct keys in a dictionary is greater than this fraction of the total number of non-null rows, turn off dictionary encoding in orc. Use 1 to always use dictionary encoding.   orc.write.batch-size 1024 Integer write batch size for orc.   page-size 64 kb MemorySize Memory page size.   parquet.enable.dictionary (none) Integer Turn off the dictionary encoding for all fields in parquet.   partial-update.ignore-delete false Boolean Whether to ignore delete records in partial-update mode.   partition (none) String Define partition by table options, cannot define partition on DDL and table options at the same time.   partition.default-name \"__DEFAULT_PARTITION__\" String The default partition name in case the dynamic partition column value is null/empty string.   partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.   primary-key (none) String Define primary key by table options, cannot define primary key on DDL and table options at the same time.   read.batch-size 1024 Integer Read batch size for orc and parquet.   scan.bounded.watermark (none) Long End condition \"watermark\" for bounded streaming mode. Stream reading will end when a larger watermark snapshot is encountered.   scan.manifest.parallelism (none) Integer The parallelism of scanning manifest files, default value is the size of cpu processor. Note: Scale-up this parameter will increase memory usage while scanning manifest files. We can consider downsize it when we encounter an out of memory exception while scanning   scan.mode default Enum\n Specify the scanning behavior of the source.\nPossible values:\"default\": Determines actual startup mode according to other table properties. If \"scan.timestamp-millis\" is set the actual startup mode will be \"from-timestamp\", and if \"scan.snapshot-id\" or \"scan.tag-name\" is set the actual startup mode will be \"from-snapshot\". Otherwise the actual startup mode will be \"latest-full\".\"latest-full\": For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes. For batch sources, just produce the latest snapshot but does not read new changes.\"full\": Deprecated. Same as \"latest-full\".\"latest\": For streaming sources, continuously reads latest changes without producing a snapshot at the beginning. For batch sources, behaves the same as the \"latest-full\" startup mode.\"compacted-full\": For streaming sources, produces a snapshot after the latest compaction on the table upon first startup, and continue to read the latest changes. For batch sources, just produce a snapshot after the latest compaction but does not read new changes. Snapshots of full compaction are picked when scheduled full-compaction is enabled.\"from-timestamp\": For streaming sources, continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning. For batch sources, produces a snapshot at timestamp specified by \"scan.timestamp-millis\" but does not read new changes.\"from-snapshot\": For streaming sources, continuously reads changes starting from snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" or \"scan.tag-name\" but does not read new changes.\"from-snapshot-full\": For streaming sources, produces from snapshot specified by \"scan.snapshot-id\" on the table upon first startup, and continuously reads changes. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" but does not read new changes.\"incremental\": Read incremental changes between start and end snapshot or timestamp.   scan.plan-sort-partition false Boolean Whether to sort plan files by partition fields, this allows you to read according to the partition order, even if your partition writes are out of order.\nIt is recommended that you use this for streaming read of the 'append-only' table. By default, streaming read will read the full snapshot first. In order to avoid the disorder reading for partitions, you can open this option.   scan.snapshot-id (none) Long Optional snapshot id used in case of \"from-snapshot\" or \"from-snapshot-full\" scan mode   scan.tag-name (none) String Optional tag name used in case of \"from-snapshot\" scan mode.   scan.timestamp-millis (none) Long Optional timestamp used in case of \"from-timestamp\" scan mode.   sequence.auto-padding (none) String Specify the way of padding precision, if the provided sequence field is used to indicate \"time\" but doesn't meet the precise.You can specific:1. \"row-kind-flag\": Pads a bit flag to indicate whether it is retract (0) or add (1) message.2. \"second-to-micro\": Pads the sequence field that indicates time with precision of seconds to micro-second.3. \"millis-to-micro\": Pads the sequence field that indicates time with precision of milli-second to micro-second.4. Composite pattern: for example, \"second-to-micro,row-kind-flag\".   sequence.field (none) String The field that generates the sequence number for primary key table, the sequence number determines which data is the most recent.   sink.watermark-time-zone \"UTC\" String The time zone to parse the long watermark value to TIMESTAMP value. The default value is 'UTC', which means the watermark is defined on TIMESTAMP column or not defined. If the watermark is defined on TIMESTAMP_LTZ column, the time zone of watermark is user configured time zone, the value should be the user configured local time zone. The option value is either a full name such as 'America/Los_Angeles', or a custom timezone id such as 'GMT-08:00'.   snapshot.num-retained.max 2147483647 Integer The maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.   snapshot.num-retained.min 10 Integer The minimum number of completed snapshots to retain. Should be greater than or equal to 1.   snapshot.time-retained 1 h Duration The maximum time of completed snapshots to retain.   sort-engine loser-tree Enum\n Specify the sort engine for table with primary key.\nPossible values:\"min-heap\": Use min-heap for multiway sorting.\"loser-tree\": Use loser-tree for multiway sorting. Compared with heapsort, loser-tree has fewer comparisons and is more efficient.   sort-spill-buffer-size 64 mb MemorySize Amount of data to spill records to disk in spilled sort.   sort-spill-threshold (none) Integer If the maximum number of sort readers exceeds this value, a spill will be attempted. This prevents too many readers from consuming too much memory and causing OOM.   source.split.open-file-cost 4 mb MemorySize Open file cost of a source file. It is used to avoid reading too many files with a source split, which can be very slow.   source.split.target-size 128 mb MemorySize Target size of a source split when scanning a bucket.   streaming-read-mode (none) Enum\n The mode of streaming read that specifies to read the data of table file or log\nPossible values:\nfile: Reads from the data of table file store.log: Read from the data of table log store.\nPossible values:\"log\": Reads from the log store.\"file\": Reads from the file store.   streaming-read-overwrite false Boolean Whether to read the changes from overwrite in streaming mode.   tag.automatic-creation none Enum\n Whether to create tag automatically. And how to generate tags.\nPossible values:\"none\": No automatically created tags.\"process-time\": Based on the time of the machine, create TAG once the processing time passes period time plus delay.\"watermark\": Based on the watermark of the input, create TAG once the watermark passes period time plus delay.   tag.creation-delay 0 ms Duration How long is the delay after the period ends before creating a tag. This can allow some late data to enter the Tag.   tag.creation-period daily Enum\n What frequency is used to generate tags.\nPossible values:\"daily\": Generate a tag every day.\"hourly\": Generate a tag every hour.\"two-hours\": Generate a tag every two hours.   tag.num-retained-max (none) Integer The maximum number of tags to retain.   target-file-size 128 mb MemorySize Target size of a file.   write-buffer-size 256 mb MemorySize Amount of data to build up in memory before converting to a sorted on-disk file.   write-buffer-spillable (none) Boolean Whether the write buffer can be spillable. Enabled by default when using object storage.   write-manifest-cache 0 bytes MemorySize Cache size for reading manifest files for write initialization.   write-mode auto Enum\n Specify the write mode for table.\nPossible values:\"auto\": The change-log for table with primary key, append-only for table without primary key.\"append-only\": The table can only accept append-only insert operations. Neither data deduplication nor any primary key constraints will be done when inserting rows into paimon.\"change-log\": The table can accept insert/delete/update operations.   write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.    CatalogOptions #  Options for paimon catalog.\n  Key Default Type Description     fs.allow-hadoop-fallback true Boolean Allow to fallback to hadoop File IO when no file io found for the scheme.   lineage-meta (none) String The lineage meta to store table and data lineage information.\nPossible values:\n\"jdbc\": Use standard jdbc to store table and data lineage information.\"custom\": You can implement LineageMetaFactory and LineageMeta to store lineage information in customized storage.   lock-acquire-timeout 8 min Duration The maximum time to wait for acquiring the lock.   lock-check-max-sleep 8 s Duration The maximum sleep time when retrying to check the lock.   lock.enabled false Boolean Enable Catalog Lock.   metastore \"filesystem\" String Metastore of paimon catalog, supports filesystem and hive.   table.type managed Enum\n Type of table.\nPossible values:\"managed\": Paimon owned table where the entire lifecycle of the table data is managed.\"external\": The table where Paimon has loose coupling with the data stored in external locations.   uri (none) String Uri of metastore server.   warehouse (none) String The warehouse root path of catalog.    HiveCatalogOptions #  Options for Hive catalog.\n  Key Default Type Description     hadoop-conf-dir (none) String File directory of the core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml. Currently, only local file system paths are supported.   hive-conf-dir (none) String File directory of the hive-site.xml , used to create HiveMetastoreClient and security authentication, such as Kerberos, LDAP, Ranger and so on   location-in-properties false Boolean Setting the location in properties of hive table/database. If you don't want to access the location by the filesystem of hive when using a object storage such as s3,oss you can set this option to true.     FlinkCatalogOptions #  Flink catalog options for paimon.\n  Key Default Type Description     default-database \"default\" String    log.system.auto-register false Boolean If true, the register will automatically create and delete a topic in log system for Paimon table. Default kafka log store register is supported, users can implement customized register for log system, for example, create a new class which extends KafkaLogStoreFactory and return a customized LogStoreRegister for their kafka cluster to create/delete topics.   log.system.auto-register-timeout 1 min Duration The timeout for register to create or delete topic in log system.    FlinkConnectorOptions #  Flink connector options for paimon.\n  Key Default Type Description     changelog-producer.lookup-wait true Boolean When changelog-producer is set to LOOKUP, commit will wait for changelog generation by lookup.   log.system \"none\" String The log system used to keep changes of the table.\nPossible values:\n\"none\": No log system, the data is written only to file store, and the streaming read will be directly read from the file store.\"kafka\": Kafka log system, the data is double written to file store and kafka, and the streaming read will be read from kafka. If streaming read from file, configures streaming-read-mode to file.   log.system.partitions 1 Integer The number of partitions of the log system. If log system is kafka, this is kafka partitions.   log.system.replication 1 Integer The number of replication of the log system. If log system is kafka, this is kafka replicationFactor.   lookup.async false Boolean Whether to enable async lookup join.   lookup.async-thread-number 16 Integer The thread number for lookup async.   scan.infer-parallelism true Boolean If it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).   scan.parallelism (none) Integer Define a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.   scan.push-down true Boolean If true, flink will push down projection, filters, limit to the source. The cost is that it is difficult to reuse the source in a job.   scan.remove-normalize false Boolean Whether to force the removal of the normalize node when streaming read. Note: This is dangerous and is likely to cause data errors if downstream is used to calculate aggregation and the input is not complete changelog.   scan.split-enumerator.batch-size 10 Integer How many splits should assign to subtask per batch in StaticFileStoreSplitEnumerator to avoid exceed `akka.framesize` limit.   scan.split-enumerator.mode fair Enum\n The mode used by StaticFileStoreSplitEnumerator to assign splits.\nPossible values:\"fair\": Distribute splits evenly when batch reading to prevent a few tasks from reading all.\"preemptive\": Distribute splits preemptively according to the consumption speed of the task.   scan.watermark.alignment.group (none) String A group of sources to align watermarks.   scan.watermark.alignment.max-drift (none) Duration Maximal drift to align watermarks, before we pause consuming from the source/task/partition.   scan.watermark.alignment.update-interval 1 s Duration How often tasks should notify coordinator about the current watermark and how often the coordinator should announce the maximal aligned watermark.   scan.watermark.emit.strategy on-event Enum\n Emit strategy for watermark generation.\nPossible values:\"on-periodic\": Emit watermark periodically, interval is controlled by Flink 'pipeline.auto-watermark-interval'.\"on-event\": Emit watermark per record.   scan.watermark.idle-timeout (none) Duration If no records flow in a partition of a stream for that amount of time, then that partition is considered \"idle\" and will not hold back the progress of watermarks in downstream operators.   sink.managed.writer-buffer-memory 256 mb MemorySize Weight of writer buffer in managed memory, Flink will compute the memory size for writer according to the weight, the actual memory used depends on the running environment.   sink.parallelism (none) Integer Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.   sink.savepoint.auto-tag false Boolean If true, a tag will be automatically created for the snapshot created by flink savepoint.   sink.use-managed-memory-allocator false Boolean If true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator.   source.checkpoint-align.enabled false Boolean Whether to align the flink checkpoint with the snapshot of the paimon table, If true, a checkpoint will only be made if a snapshot is consumed.   source.checkpoint-align.timeout 30 s Duration If the new snapshot has not been generated when the checkpoint starts to trigger, the enumerator will block the checkpoint and wait for the new snapshot. Set the maximum waiting time to avoid infinite waiting, if timeout, the checkpoint will fail. Note that it should be set smaller than the checkpoint timeout.   unaware-bucket.compaction.parallelism (none) Integer Defines a custom parallelism for the unaware-bucket table compaction job. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.    SparkConnectorOptions #  Spark connector options for paimon.\n  Key Default Type Description     write.merge-schema false Boolean If true, merge the data schema and the table schema automatically before write data.   write.merge-schema.explicit-cast false Boolean If true, allow to merge data types if the two types meet the rules for explicit casting.    "});index.add({'id':47,'href':'/docs/master/versions/','title':"Versions",'section':"Apache Paimon",'content':"Versions #  An appendix of hosted documentation for all versions of Apache Paimon.\n master    stable    0.4    "});})();