'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/master/docs/concepts/','title':"Concepts",'section':"Docs",'content':""});index.add({'id':1,'href':'/docs/master/docs/how-to/creating-catalogs/','title':"Creating Catalogs",'section':"How to",'content':"Creating Catalogs #  Table Store catalogs currently support two types of metastores:\n filesystem metastore (default), which stores both metadata and table files in filesystems. hive metastore, which additionally stores metadata in Hive metastore. Users can directly access the tables from Hive.  See CatalogOptions for detailed options when creating a catalog.\nCreating a Catalog with Filesystem Metastore #  Flink The following Flink SQL registers and uses a Table Store catalog named my_catalog. Metadata and table files are stored under hdfs://path/to/warehouse.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;table-store\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs://path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; Spark3 The following shell command registers a Table Store catalog named tablestore. Metadata and table files are stored under hdfs://path/to/warehouse.\nspark-sql ... \\  --conf spark.sql.catalog.tablestore=org.apache.flink.table.store.spark.SparkCatalog \\  --conf spark.sql.catalog.tablestore.warehouse=hdfs://path/to/warehouse After spark-sql is started, you can switch to the default database of the tablestore catalog with the following SQL.\nUSE tablestore.default;  Creating a Catalog with Hive Metastore #  By using Table Store Hive catalog, changes to the catalog will directly affect the corresponding Hive metastore. Tables created in such catalog can also be accessed directly from Hive.\nPreparing Table Store Hive Catalog Jar File #  You are using an unreleased version of Table Store so you need to manually build bundled jar from the source code. To build from source code, either download the source of a release or clone the git repository.\nBuild bundled jar with the following command. mvn clean install -Dmaven.test.skip=true\nYou can find Hive catalog jar in ./flink-table-store-hive/flink-table-store-hive-catalog/target/flink-table-store-hive-catalog-0.4-SNAPSHOT.jar.\nRegistering Hive Catalog #  Flink To enable Table Store Hive catalog support in Flink, you can pick one of the following two methods.\n Copy Table Store Hive catalog jar file into the lib directory of your Flink installation directory. Note that this must be done before starting your Flink cluster. If you\u0026rsquo;re using Flink\u0026rsquo;s SQL client, append --jar /path/to/flink-table-store-hive-catalog-0.4-SNAPSHOT.jar to the starting command of SQL client.  The following Flink SQL registers and uses a Table Store Hive catalog named my_hive. Metadata and table files are stored under hdfs://path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nCREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;table-store\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs://path/to/warehouse\u0026#39; ); USE CATALOG my_hive; Spark3 To enable Table Store Hive catalog support in Spark3, append the path of Table Store Hive catalog jar file to --jars argument when starting spark.\nThe following shell command registers a Table tore Hive catalog named tablestore. Metadata and table files are stored under hdfs://path/to/warehouse. In addition, metadata is also stored in Hive metastore.\nspark-sql ... \\  --conf spark.sql.catalog.tablestore=org.apache.flink.table.store.spark.SparkCatalog \\  --conf spark.sql.catalog.tablestore.warehouse=hdfs://path/to/warehouse \\  --conf spark.sql.catalog.tablestore.metastore=hive \\  --conf spark.sql.catalog.tablestore.uri=thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt; After spark-sql is started, you can switch to the default database of the tablestore catalog with the following SQL.\nUSE tablestore.default;  "});index.add({'id':2,'href':'/docs/master/docs/concepts/overview/','title':"Overview",'section':"Concepts",'content':"Overview #  Flink Table Store is a unified storage to build dynamic tables for both streaming and batch processing in Flink, supporting high-speed data ingestion and timely data query.\nArchitecture #  As shown in the architecture above:\nRead/Write: Table Store supports a versatile way to read/write data and perform OLAP queries.\n For reads, it supports consuming data  from historical snapshots (in batch mode), from the latest offset (in streaming mode), or reading incremental snapshots in a hybrid way.   For writes, it supports streaming synchronization from the changelog of databases (CDC) or batch insert/overwrite from offline data.  Ecosystem: In addition to Apache Flink, Table Store also supports read by other computation engines like Apache Hive, Apache Spark and Trino.\nInternal: Under the hood, Table Store uses a hybrid storage architecture with a lake format to store historical data and a queue system to store incremental data. The former stores the columnar files on the filesystem/object-store and uses the LSM tree structure to support a large volume of data updates and high-performance queries. The latter uses Apache Kafka to capture data in real-time.\nUnified Storage #  There are three types of connectors in Flink SQL.\n Message queue, such as Apache Kafka, it is used in both source and intermediate stages in this pipeline, to guarantee the latency stay within seconds. OLAP system, such as Clickhouse, it receives processed data in streaming fashion and serving user’s ad-hoc queries. Batch storage, such as Apache Hive, it supports various operations of the traditional batch processing, including INSERT OVERWRITE.  Flink Table Store provides table abstraction. It is used in a way that does not differ from the traditional database:\n In Flink batch execution mode, it acts like a Hive table and supports various operations of Batch SQL. Query it to see the latest snapshot. In Flink streaming execution mode, it acts like a message queue. Query it acts like querying a stream changelog from a message queue where historical data never expires.  "});index.add({'id':3,'href':'/docs/master/docs/engines/overview/','title':"Overview",'section':"Engines",'content':"Overview #  Table Store not only supports Flink SQL writes and queries natively, but also provides queries from other popular engines, such as Apache Spark and Apache Hive.\nCompatibility Matrix #     Engine Version Feature Read Pushdown     Flink 1.16/1.15/1.14 batch/streaming read, batch/streaming write, create/drop table, create/drop database Projection, Filter   Hive 3.1/2.3/2.2/2.1/2.1 CDH 6.3 batch read Projection, Filter   Spark 3.3/3.2/3.1 batch read, batch write, create/drop table, create/drop database Projection, Filter   Spark 2.4 batch read Projection, Filter   Trino 388/358 batch read Projection, Filter    "});index.add({'id':4,'href':'/docs/master/docs/filesystems/overview/','title':"Overview",'section':"Filesystems",'content':"Overview #  Apache Flink Table Store utilizes the same pluggable file systems as Apache Flink. Users can follow the standard plugin mechanism to configure the plugin structure if using Flink as compute engine. However, for other engines like Spark or Hive, the provided opt jars (by Flink) may get conflicts and cannot be used directly. It is not convenient for users to fix class conflicts, thus Flink Table Store provides the self-contained and engine-unified FileSystem pluggable jars for user to query tables from Spark/Hive side.\nSupported FileSystems #     FileSystem URI Scheme Pluggable Description     Local File System file:// N Built-in Support   HDFS hdfs:// N Built-in Support, ensure that the cluster is in the hadoop environment   Aliyun OSS oss:// Y    S3 s3:// Y     "});index.add({'id':5,'href':'/docs/master/docs/maintenance/write-performance/','title':"Write Performance",'section':"Maintenance",'content':"Write Performance #  Performance of Table Store writers are related with the following factors.\nParallelism #  It is recommended that the parallelism of sink should be less than or equal to the number of buckets, preferably equal. You can control the parallelism of the sink with the sink.parallelism table property.\n  Option Required Default Type Description     sink.parallelism No (none) Integer Defines the parallelism of the sink operator. By default, the parallelism is determined by the framework using the same parallelism of the upstream chained operator.    Compaction #  Number of Sorted Runs to Trigger Compaction #  Table Store uses LSM tree which supports a large number of updates. LSM organizes files in several sorted runs. When querying records from an LSM tree, all sorted runs must be combined to produce a complete view of all records.\nOne can easily see that too many sorted runs will result in poor query performance. To keep the number of sorted runs in a reasonable range, Table Store writers will automatically perform compactions. The following table property determines the minimum number of sorted runs to trigger a compaction.\n  Option Required Default Type Description     num-sorted-run.compaction-trigger No 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).    Compaction will become less frequent when num-sorted-run.compaction-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance.\nNumber of Sorted Runs to Pause Writing #  When number of sorted runs is small, Table Store writers will perform compaction asynchronously in separated threads, so records can be continuously written into the table. However to avoid unbounded growth of sorted runs, writers will have to pause writing when the number of sorted runs hits the threshold. The following table property determines the threshold.\n  Option Required Default Type Description     num-sorted-run.stop-trigger No 10 Integer The number of sorted-runs that trigger the stopping of writes.    Write stalls will become less frequent when num-sorted-run.stop-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance.\nDedicated Compaction Job #  By default, Table Store writers will perform compaction as needed when writing records. This is sufficient for most use cases, but there are two downsides:\n This may result in unstable write throughput because throughput might temporarily drop when performing a compaction. Compaction will mark some data files as \u0026ldquo;deleted\u0026rdquo; (not really deleted, see expiring snapshots for more info). If multiple writers mark the same file a conflict will occur when committing the changes. Table Store will automatically resolve the conflict, but this may result in job restarts.  To avoid these downsides, users can also choose to skip compactions in writers, and run a dedicated job only for compaction. As compactions are performed only by the dedicated job, writers can continuously write records without pausing and no conflicts will ever occur.\nTo skip compactions in writers, set the following table property to true.\n  Option Required Default Type Description     write-only No false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.    To run a dedicated job for compaction, follow these instructions.\nFlink Flink SQL currently does not support statements related to compactions, so we have to submit the compaction job through flink run.\nRun the following command to submit a compaction job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  compact \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; If you submit a batch job (set execution.runtime-mode: batch in Flink\u0026rsquo;s configuration), all current table files will be compacted. If you submit a streaming job (set execution.runtime-mode: streaming in Flink\u0026rsquo;s configuration), the job will continuously monitor new changes to the table and perform compactions as needed.\nIf you only want to submit the compaction job and don\u0026rsquo;t want to wait until the job is done, you should submit in detached mode.  For more usage of the compact action, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  compact --help  Memory #  There are three main places in Table Store writer that takes up memory:\n Writer\u0026rsquo;s memory buffer, shared and preempted by all writers of a single task. This memory value can be adjusted by the write-buffer-size table property. Memory consumed when merging several sorted runs for compaction. Can be adjusted by the num-sorted-run.compaction-trigger option to change the number of sorted runs to be merged. The memory consumed by writing columnar (ORC, Parquet, etc.) file, which is not adjustable.  "});index.add({'id':6,'href':'/docs/master/docs/concepts/basic-concepts/','title':"Basic Concepts",'section':"Concepts",'content':"Basic Concepts #  Snapshot #  A snapshot captures the state of a table at some point in time. Users can access the latest data of a table through the latest snapshot. By time traveling, users can also access the previous state of a table through an earlier snapshot.\nPartition #  Table Store adopts the same partitioning concept as Apache Hive to separate data.\nPartitioning is an optional way of dividing a table into related parts based on the values of particular columns like date, city, and department. Each table can have one or more partition keys to identify a particular partition.\nBy partitioning, users can efficiently operate on a slice of records in the table. See file layouts for how files are divided into multiple partitions.\nPartition keys must be a subset of primary keys if primary keys are defined.  Bucket #  Unpartitioned tables, or partitions in partitioned tables, are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying.\nThe range for a bucket is determined by the hash value of one or more columns in the records. Users can specify bucketing columns by providing the bucket-key option. If no bucket-key option is specified, the primary key (if defined) or the complete record will be used as the bucket key.\nA bucket is the smallest storage unit for reads and writes, so the number of buckets limits the maximum processing parallelism. This number should not be too big, though, as it will result in lots of small files and low read performance. In general, the recommended data size in each bucket is about 1GB.\nSee file layouts for how files are divided into buckets. Also, see rescale bucket if you want to adjust the number of buckets after a table is created.\nConsistency Guarantees #  Table Store writers uses two-phase commit protocol to atomically commit a batch of records to the table. Each commit produces at most two snapshots at commit time.\nFor any two writers modifying a table at the same time, as long as they do not modify the same bucket, their commits are serializable. If they modify the same bucket, only snapshot isolation is guaranteed. That is, the final table state may be a mix of the two commits, but no changes are lost.\n"});index.add({'id':7,'href':'/docs/master/docs/how-to/creating-tables/','title':"Creating Tables",'section':"How to",'content':"Creating Tables #  Creating Catalog Managed Tables #  Tables created in Table Store catalogs are managed by the catalog. When the table is dropped from catalog, its table files will also be deleted.\nThe following SQL assumes that you have registered and are using a Table Store catalog. It creates a managed table named MyTable with five columns in the catalog\u0026rsquo;s default database, where dt, hh and user_id are the primary keys.\nFlink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; );  Partitioned Tables #  The following SQL creates a table named MyTable with five columns partitioned by dt and hh, where dt, hh and user_id are the primary keys.\nFlink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; );  Partition keys must be a subset of primary keys if primary keys are defined.  By configuring partition.expiration-time, expired partitions can be automatically deleted.  Create Table As #  Table can be created and populated by the results of a query, for example, we have a sql like this: CREATE TABLE table_b AS SELECT id, name FORM table_a, The resulting table table_b will be equivalent to create the table and insert the data with the following statement: CREATE TABLE table_b (id INT, name STRING); INSERT INTO table_b SELECT id, name FROM table_a;\nWe can specify the primary key or partition when use CREATE TABLE AS SELECT, for syntax, please refer to the following sql.\nFlink /* For streaming mode, you need to enable the checkpoint. */ CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT ); CREATE TABLE MyTableAs AS SELECT * FROM MyTable; /* partitioned table */ CREATE TABLE MyTablePartition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE MyTablePartitionAs WITH (\u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM MyTablePartition; /* change options */ CREATE TABLE MyTableOptions ( user_id BIGINT, item_id BIGINT ) WITH (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE MyTableOptionsAs WITH (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM MyTableOptions; /* primary key */ CREATE TABLE MyTablePk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) ; CREATE TABLE MyTablePkAs WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM MyTablePk; /* primary key + partition */ CREATE TABLE MyTableAll ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh); CREATE TABLE MyTableAllAs WITH (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;, \u0026#39;partition\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM MyTableAll; Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT ); CREATE TABLE MyTableAs AS SELECT * FROM MyTable; /* partitioned table*/ CREATE TABLE MyTablePartition ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh); CREATE TABLE MyTablePartitionAs PARTITIONED BY (dt) AS SELECT * FROM MyTablePartition; /* change TBLPROPERTIES */ CREATE TABLE MyTableOptions ( user_id BIGINT, item_id BIGINT ) TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;orc\u0026#39;); CREATE TABLE MyTableOptionsAs TBLPROPERTIES (\u0026#39;file.format\u0026#39; = \u0026#39;parquet\u0026#39;) AS SELECT * FROM MyTableOptions; /* primary key */ CREATE TABLE MyTablePk ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE MyTablePkAs TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt\u0026#39;) AS SELECT * FROM MyTablePk; /* primary key + partition */ CREATE TABLE MyTableAll ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39; ); CREATE TABLE MyTableAllAs PARTITIONED BY (dt) TBLPROPERTIES (\u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh\u0026#39;) AS SELECT * FROM MyTableAll;  Create Table Like #  Flink To create a table with the same schema, partition, and table properties as another table, use CREATE TABLE LIKE.\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) ; CREATE TABLE MyTableLike LIKE MyTable;  Table Properties #  Users can specify table properties to enable features or improve performance of Table Store. For a complete list of such properties, see configurations.\nThe following SQL creates a table named MyTable with five columns partitioned by dt and hh, where dt, hh and user_id are the primary keys. This table has two properties: 'bucket' = '2' and 'bucket-key' = 'user_id'.\nFlink CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) PARTITIONED BY (dt, hh) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;user_id\u0026#39; ); Spark3 CREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING ) PARTITIONED BY (dt, hh) TBLPROPERTIES ( \u0026#39;primary-key\u0026#39; = \u0026#39;dt,hh,user_id\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;2\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;user_id\u0026#39; );  Creating External Tables #  External tables are recorded but not managed by catalogs. If an external table is dropped, its table files will not be deleted.\nTable Store external tables can be used in any catalog. If you do not want to create a Table Store catalog and just want to read / write a table, you can consider external tables.\nFlink Flink SQL supports reading and writing an external table. External Table Store tables are created by specifying the connector and path table properties. The following SQL creates an external table named MyTable with five columns, where the base path of table files is hdfs://path/to/table.\nCREATE TABLE MyTable ( user_id BIGINT, item_id BIGINT, behavior STRING, dt STRING, hh STRING, PRIMARY KEY (dt, hh, user_id) NOT ENFORCED ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;table-store\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;hdfs://path/to/table\u0026#39;, \u0026#39;auto-create\u0026#39; = \u0026#39;true\u0026#39; -- this table property creates table files for an empty table if table path does not exist  -- currently only supported by Flink ); Spark3 Spark3 only supports creating external tables through Scala API. The following Scala code loads the table located at hdfs://path/to/table into a DataSet.\nval dataset = spark.read.format(\u0026#34;tablestore\u0026#34;).load(\u0026#34;hdfs://path/to/table\u0026#34;) Spark2 Spark2 only supports creating external tables through Scala API. The following Scala code loads the table located at hdfs://path/to/table into a DataSet.\nval dataset = spark.read.format(\u0026#34;tablestore\u0026#34;).load(\u0026#34;hdfs://path/to/table\u0026#34;) Hive Hive SQL only supports reading from an external table. The following SQL creates an external table named my_table, where the base path of table files is hdfs://path/to/table. As schemas are stored in table files, users do not need to write column definitions.\nCREATE EXTERNAL TABLE my_table STORED BY \u0026#39;org.apache.flink.table.store.hive.TableStoreHiveStorageHandler\u0026#39; LOCATION \u0026#39;hdfs://path/to/table\u0026#39;;  Creating Temporary Tables #  Flink Temporary tables are only supported by Flink. Like external tables, temporary tables are just recorded but not managed by the current Flink SQL session. If the temporary table is dropped, its resources will not be deleted. Temporary tables are also dropped when Flink SQL session is closed.\nIf you want to use Table Store catalog along with other tables but do not want to store them in other catalogs, you can create a temporary table. The following Flink SQL creates a Table Store catalog and a temporary table and also illustrates how to use both tables together.\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;table-store\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;hdfs://path/to/warehouse\u0026#39; ); USE CATALOG my_catalog; -- Assume that there is already a table named my_table in my_catalog  CREATE TEMPORARY TABLE temp_table ( k INT, v STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;hdfs://path/to/temp_table.csv\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ); SELECT my_table.k, my_table.v, temp_table.v FROM my_table JOIN temp_table ON my_table.k = temp_table.k;  "});index.add({'id':8,'href':'/docs/master/docs/engines/','title':"Engines",'section':"Docs",'content':""});index.add({'id':9,'href':'/docs/master/docs/maintenance/expiring-snapshots/','title':"Expiring Snapshots",'section':"Maintenance",'content':"Expiring Snapshots #  Table Store writers generates one or two snapshots per commit. Each snapshot may add some new data files or mark some old data files as deleted. However, the marked data files are not truly deleted because Table Store also supports time traveling to an earlier snapshot. They are only deleted when the snapshot expires.\nCurrently, expiration is automatically performed by Table Store writers when committing new changes. By expiring old snapshots, old data files and metadata files that are no longer used can be deleted to release disk space.\nSnapshot expiration is controlled by the following table properties.\n  Option Required Default Type Description     snapshot.time-retained No 1 h Duration The maximum time of completed snapshots to retain.   snapshot.num-retained.min No 10 Integer The minimum number of completed snapshots to retain.   snapshot.num-retained.max No Integer.MAX_VALUE Integer The maximum number of completed snapshots to retain.    Please note that too short retain time or too small retain number may result in:\n Batch queries cannot find the file. For example, the table is relatively large and the batch query takes 10 minutes to read, but the snapshot from 10 minutes ago expires, at which point the batch query will read a deleted snapshot. Streaming reading jobs on table files (without the external log system) fail to restart. When the job restarts, the snapshot it recorded may have expired.  "});index.add({'id':10,'href':'/docs/master/docs/engines/flink/','title':"Flink",'section':"Engines",'content':"Flink #  This documentation is a guide for using Table Store in Flink.\nPreparing Table Store Jar File #  Table Store currently supports Flink 1.16, 1.15 and 1.14. We recommend the latest Flink version for a better experience.\nYou are using an unreleased version of Table Store so you need to manually build bundled jar from the source code. To build from source code, either download the source of a release or clone the git repository.\nBuild bundled jar with the following command.\n mvn clean install -DskipTests  You can find the bundled jar in ./flink-table-store-flink/flink-table-store-flink-\u0026lt;flink-version\u0026gt;/target/flink-table-store-flink-\u0026lt;flink-version\u0026gt;-0.4-SNAPSHOT.jar.\nQuick Start #  Step 1: Download Flink\nIf you haven\u0026rsquo;t downloaded Flink, you can download Flink 1.16, then extract the archive with the following command.\ntar -xzf flink-*.tgz Step 2: Copy Table Store Bundled Jar\nCopy table store bundled jar to the lib directory of your Flink home.\ncp flink-table-store-flink-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 3: Copy Hadoop Bundled Jar\nDownload Pre-bundled Hadoop jar and copy the jar file to the lib directory of your Flink home.\ncp flink-shaded-hadoop-2-uber-*.jar \u0026lt;FLINK_HOME\u0026gt;/lib/ Step 4: Start a Flink Local Cluster\nIn order to run multiple Flink jobs at the same time, you need to modify the cluster configuration in \u0026lt;FLINK_HOME\u0026gt;/conf/flink-conf.yaml.\ntaskmanager.numberOfTaskSlots:2To start a local cluster, run the bash script that comes with Flink:\n\u0026lt;FLINK_HOME\u0026gt;/bin/start-cluster.sh You should be able to navigate to the web UI at localhost:8081 to view the Flink dashboard and see that the cluster is up and running.\nYou can now start Flink SQL client to execute SQL scripts.\n\u0026lt;FLINK_HOME\u0026gt;/bin/sql-client.sh Step 5: Create a Catalog and a Table\n-- if you\u0026#39;re trying out Table Store in a distributed environment, -- warehouse path should be set to a shared file system, such as HDFS or OSS CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;table-store\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;file:/tmp/table_store\u0026#39; ); USE CATALOG my_catalog; -- create a word count table CREATE TABLE word_count ( word STRING PRIMARY KEY NOT ENFORCED, cnt BIGINT ); Step 6: Write Data\n-- create a word data generator table CREATE TEMPORARY TABLE word_table ( word STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;datagen\u0026#39;, \u0026#39;fields.word.length\u0026#39; = \u0026#39;1\u0026#39; ); -- table store requires checkpoint interval in streaming mode SET \u0026#39;execution.checkpointing.interval\u0026#39; = \u0026#39;10 s\u0026#39;; -- write streaming data to dynamic table INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; Step 7: OLAP Query\n-- use tableau result mode SET \u0026#39;sql-client.execution.result-mode\u0026#39; = \u0026#39;tableau\u0026#39;; -- switch to batch mode RESET \u0026#39;execution.checkpointing.interval\u0026#39;; SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- olap query the table SELECT * FROM word_count; You can execute the query multiple times and observe the changes in the results.\nStep 8: Streaming Query\n-- switch to streaming mode SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; -- track the changes of table and calculate the count interval statistics SELECT `interval`, COUNT(*) AS interval_cnt FROM (SELECT cnt / 10000 AS `interval` FROM word_count) GROUP BY `interval`; Step 9: Exit\nCancel streaming job in localhost:8081, then execute the following SQL script to exit Flink SQL client.\n-- uncomment the following line if you want to drop the dynamic table and clear the files -- DROP TABLE word_count;  -- exit sql-client EXIT; Stop the Flink local cluster.\n./bin/stop-cluster.sh Supported Flink Data Type #  See Flink Data Types.\nAll Flink data types are supported, except that\n MULTISET is not supported. MAP is not supported as primary keys.  "});index.add({'id':11,'href':'/docs/master/docs/filesystems/oss/','title':"OSS",'section':"Filesystems",'content':"OSS #  Build To build from source code, either download the source of a release or clone the git repository.\nBuild shaded jar with the following command.\nmvn clean install -DskipTests You can find the shaded jars under ./flink-table-store-filesystems/flink-table-store-oss/target/flink-table-store-oss-0.4-SNAPSHOT.jar.\nUsage #  Flink Put flink-table-store-oss-0.4-SNAPSHOT.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;table-store\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;oss://path/to/warehouse\u0026#39;, \u0026#39;fs.oss.endpoint\u0026#39; = \u0026#39;oss-cn-hangzhou.aliyuncs.com\u0026#39;, \u0026#39;fs.oss.accessKeyId\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;fs.oss.accessKeySecret\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark Place flink-table-store-oss-0.4-SNAPSHOT.jar together with flink-table-store-spark-0.4-SNAPSHOT.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.tablestore=org.apache.flink.table.store.spark.SparkCatalog \\  --conf spark.sql.catalog.tablestore.warehouse=oss://\u0026lt;bucket-name\u0026gt;/ \\  --conf spark.sql.catalog.tablestore.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com \\  --conf spark.sql.catalog.tablestore.fs.oss.accessKeyId=xxx \\  --conf spark.sql.catalog.tablestore.fs.oss.accessKeySecret=yyy Hive NOTE: You need to ensure that Hive metastore can access oss.\nPlace flink-table-store-oss-0.4-SNAPSHOT.jar together with flink-table-store-hive-connector-0.4-SNAPSHOT.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET tablestore.fs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com; SET tablestore.fs.oss.accessKeyId=xxx; SET tablestore.fs.oss.accessKeySecret=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino Place flink-table-store-oss-0.4-SNAPSHOT.jar together with flink-table-store-trino-0.4-SNAPSHOT.jar under plugin/tablestore directory.\nAdd options in etc/catalog/tablestore.properties.\nfs.oss.endpoint=oss-cn-hangzhou.aliyuncs.com fs.oss.accessKeyId=xxx fs.oss.accessKeySecret=yyy  "});index.add({'id':12,'href':'/docs/master/docs/how-to/altering-tables/','title':"Altering Tables",'section':"How to",'content':"Altering Tables #  Changing/Adding Table Properties #  The following SQL sets write-buffer-size table property to 256 MB.\nFlink ALTER TABLE my_table SET ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; ); Spark3 ALTER TABLE my_table SET TBLPROPERTIES ( \u0026#39;write-buffer-size\u0026#39; = \u0026#39;256 MB\u0026#39; );  Rename Table Name #  The following SQL rename the table name to new name.\nFlink ALTER TABLE my_table RENAME TO my_table_new; Spark3 ALTER TABLE my_table RENAME TO my_table_new;  If you use object storage, such as S3 or OSS, please use this syntax carefully, because the renaming of object storage is not atomic, and only partial files may be moved in case of failure.  Removing Table Properties #  The following SQL removes write-buffer-size table property.\nFlink ALTER TABLE my_table RESET (\u0026#39;write-buffer-size\u0026#39;); Spark3 ALTER TABLE my_table UNSET TBLPROPERTIES (\u0026#39;write-buffer-size\u0026#39;);  Adding New Columns #  The following SQL adds two columns c1 and c2 to table my_table.\nSpark3 ALTER TABLE my_table ADD COLUMNS ( c1 INT, c2 STRING );  Renaming Column Name #  The following SQL renames column c0 in table my_table to c1.\nSpark3 ALTER TABLE my_table RENAME COLUMN c0 TO c1;  Dropping Columns #  The syntax is:\nSpark3 ALTER TABLE table_identifier DROP { COLUMN | COLUMNS } [(] col_name [, ... ] [)]  The following SQL drops tow columns c1 and c2 from table my_table.\nSpark3 ALTER TABLE my_table DROP COLUMNS (c1, c2);  Changing Column Nullability #  The following SQL sets column coupon_info to be nullable.\nSpark3 ALTER TABLE my_table ALTER COLUMN coupon_info DROP NOT NULL;  Changing Column Comment #  The following SQL changes comment of column buy_count to buy count.\nSpark3 ALTER TABLE my_table ALTER COLUMN buy_count COMMENT \u0026#39;buy count\u0026#39;;  Changing Column Position #  To modify an existent column to a new position, use FIRST or AFTER col_name.\nSpark3 ALTER TABLE my_table ALTER COLUMN col_a FIRST; ALTER TABLE my_table ALTER COLUMN col_a AFTER col_b;  "});index.add({'id':13,'href':'/docs/master/docs/concepts/file-layouts/','title':"File Layouts",'section':"Concepts",'content':"File Layouts #  All files of a table are stored under one base directory. Table Store files are organized in a layered style. The following image illustrates the file layout. Starting from a snapshot file, Table Store readers can recursively access all records from the table.\nSnapshot Files #  All snapshot files are stored in the snapshot directory.\nA snapshot file is a JSON file containing information about this snapshot, including\n the schema file in use the manifest list containing all changes of this snapshot  Manifest Files #  All manifest lists and manifest files are stored in the manifest directory.\nA manifest list is a list of manifest file names.\nA manifest file is a file containing changes about LSM data files and changelog files. For example, which LSM data file is created and which file is deleted in the corresponding snapshot.\nData Files #  Data files are grouped by partitions and buckets. Each bucket directory contains an LSM tree and its changelog files.\nLSM Trees #  Table Store adapts the LSM tree (log-structured merge-tree) as the data structure for file storage. This documentation briefly introduces the concepts about LSM trees.\nSorted Runs #  LSM tree organizes files into several sorted runs. A sorted run consists of one or multiple data files and each data file belongs to exactly one sorted run.\nRecords within a data file are sorted by their primary keys. Within a sorted run, ranges of primary keys of data files never overlap.\nAs you can see, different sorted runs may have overlapping primary key ranges, and may even contain the same primary key. When querying the LSM tree, all sorted runs must be combined and all records with the same primary key must be merged according to the user-specified merge engine and the timestamp of each record.\nNew records written into the LSM tree will be first buffered in memory. When the memory buffer is full, all records in memory will be sorted and flushed to disk. A new sorted run is now created.\nCompaction #  When more and more records are written into the LSM tree, the number of sorted runs will increase. Because querying an LSM tree requires all sorted runs to be combined, too many sorted runs will result in a poor query performance, or even out of memory.\nTo limit the number of sorted runs, we have to merge several sorted runs into one big sorted run once in a while. This procedure is called compaction.\nHowever, compaction is a resource intensive procedure which consumes a certain amount of CPU time and disk IO, so too frequent compaction may in turn result in slower writes. It is a trade-off between query and write performance. Table Store currently adapts a compaction strategy similar to Rocksdb\u0026rsquo;s universal compaction.\nBy default, when Table Store writers append records to the LSM tree, they\u0026rsquo;ll also perform compactions as needed. Users can also choose to perform all compactions in a dedicated compaction job. See dedicated compaction job for more info.\n"});index.add({'id':14,'href':'/docs/master/docs/filesystems/','title':"Filesystems",'section':"Docs",'content':""});index.add({'id':15,'href':'/docs/master/docs/maintenance/rescale-bucket/','title':"Rescale Bucket",'section':"Maintenance",'content':"Rescale Bucket #  Since the number of total buckets dramatically influences the performance, Table Store allows users to tune bucket numbers by ALTER TABLE command and reorganize data layout by INSERT OVERWRITE without recreating the table/partition. When executing overwrite jobs, the framework will automatically scan the data with the old bucket number and hash the record according to the current bucket number.\nRescale Overwrite #  -- rescale number of total buckets ALTER TABLE table_identifier SET (\u0026#39;bucket\u0026#39; = \u0026#39;...\u0026#39;) -- reorganize data layout of table/partition INSERT OVERWRITE table_identifier [PARTITION (part_spec)] SELECT ... FROM table_identifier [WHERE part_spec] Please note that\n ALTER TABLE only modifies the table\u0026rsquo;s metadata and will NOT reorganize or reformat existing data. Reorganize existing data must be achieved by INSERT OVERWRITE. Rescale bucket number does not influence the read and running write jobs. Once the bucket number is changed, any newly scheduled INSERT INTO jobs which write to without-reorganized existing table/partition will throw a TableException with message like Try to write table/partition ... with a new bucket num ..., but the previous bucket num is ... Please switch to batch mode, and perform INSERT OVERWRITE to rescale current data layout first.  For partitioned table, it is possible to have different bucket number for different partitions. E.g. ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;4\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-01\u0026#39;) SELECT * FROM ...; ALTER TABLE my_table SET (\u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;); INSERT OVERWRITE my_table PARTITION (dt = \u0026#39;2022-01-02\u0026#39;) SELECT * FROM ...;  During overwrite period, make sure there are no other jobs writing the same table/partition.  Note: For the table which enables log system(e.g. Kafka), please rescale the topic\u0026rsquo;s partition as well to keep consistency.  Use Case #  Rescale bucket helps to handle sudden spikes in throughput. Suppose there is a daily streaming ETL task to sync transaction data. The table\u0026rsquo;s DDL and pipeline are listed as follows.\n-- table DDL CREATE TABLE verified_orders ( trade_order_id BIGINT, item_id BIGINT, item_price DOUBLE, dt STRING, PRIMARY KEY (dt, trade_order_id, item_id) NOT ENFORCED ) PARTITIONED BY (dt) WITH ( \u0026#39;bucket\u0026#39; = \u0026#39;16\u0026#39; ); -- like from a kafka table CREATE temporary TABLE raw_orders( trade_order_id BIGINT, item_id BIGINT, item_price BIGINT, gmt_create STRING, order_status STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); -- streaming insert as bucket num = 16 INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;; The pipeline has been running well for the past few weeks. However, the data volume has grown fast recently, and the job\u0026rsquo;s latency keeps increasing. To improve the data freshness, users can\n Suspend the streaming job with a savepoint ( see Suspended State and Stopping a Job Gracefully Creating a Final Savepoint ) $ ./bin/flink stop \\  --savepointPath /tmp/flink-savepoints \\  $JOB_ID  Increase the bucket number -- scaling out ALTER TABLE verified_orders SET (\u0026#39;bucket\u0026#39; = \u0026#39;32\u0026#39;);  Switch to the batch mode and overwrite the current partition(s) to which the streaming job is writing SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;batch\u0026#39;; -- suppose today is 2022-06-22 -- case 1: there is no late event which updates the historical partitions, thus overwrite today\u0026#39;s partition is enough INSERT OVERWRITE verified_orders PARTITION (dt = \u0026#39;2022-06-22\u0026#39;) SELECT trade_order_id, item_id, item_price FROM verified_orders WHERE dt = \u0026#39;2022-06-22\u0026#39;; -- case 2: there are late events updating the historical partitions, but the range does not exceed 3 days INSERT OVERWRITE verified_orders SELECT trade_order_id, item_id, item_price, dt FROM verified_orders WHERE dt IN (\u0026#39;2022-06-20\u0026#39;, \u0026#39;2022-06-21\u0026#39;, \u0026#39;2022-06-22\u0026#39;);  After overwrite job finished, switch back to streaming mode. And now, the parallelism can be increased alongside with bucket number to restore the streaming job from the savepoint ( see Start a SQL Job from a savepoint ) SET \u0026#39;execution.runtime-mode\u0026#39; = \u0026#39;streaming\u0026#39;; SET \u0026#39;execution.savepoint.path\u0026#39; = \u0026lt;savepointPath\u0026gt;; INSERT INTO verified_orders SELECT trade_order_id, item_id, item_price, DATE_FORMAT(gmt_create, \u0026#39;yyyy-MM-dd\u0026#39;) AS dt FROM raw_orders WHERE order_status = \u0026#39;verified\u0026#39;;   "});index.add({'id':16,'href':'/docs/master/docs/filesystems/s3/','title':"S3",'section':"Filesystems",'content':"S3 #  Build To build from source code, either download the source of a release or clone the git repository.\nBuild shaded jar with the following command.\nmvn clean install -DskipTests You can find the shaded jars under ./flink-table-store-filesystems/flink-table-store-s3/target/flink-table-store-s3-0.4-SNAPSHOT.jar.\nUsage #  Flink Put flink-table-store-s3-0.4-SNAPSHOT.jar into lib directory of your Flink home, and create catalog:\nCREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39; = \u0026#39;table-store\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;s3://path/to/warehouse\u0026#39;, \u0026#39;s3.endpoint\u0026#39; = \u0026#39;your-endpoint-hostname\u0026#39;, \u0026#39;s3.access-key\u0026#39; = \u0026#39;xxx\u0026#39;, \u0026#39;s3.secret-key\u0026#39; = \u0026#39;yyy\u0026#39; ); Spark Place flink-table-store-s3-0.4-SNAPSHOT.jar together with flink-table-store-spark-0.4-SNAPSHOT.jar under Spark\u0026rsquo;s jars directory, and start like\nspark-sql \\  --conf spark.sql.catalog.tablestore=org.apache.flink.table.store.spark.SparkCatalog \\  --conf spark.sql.catalog.tablestore.warehouse=s3://\u0026lt;bucket\u0026gt;/\u0026lt;endpoint\u0026gt; \\  --conf spark.sql.catalog.tablestore.s3.endpoint=your-endpoint-hostname \\  --conf spark.sql.catalog.tablestore.s3.access-key=xxx \\  --conf spark.sql.catalog.tablestore.s3.secret-key=yyy Hive NOTE: You need to ensure that Hive metastore can access s3.\nPlace flink-table-store-s3-0.4-SNAPSHOT.jar together with flink-table-store-hive-connector-0.4-SNAPSHOT.jar under Hive\u0026rsquo;s auxlib directory, and start like\nSET tablestore.s3.endpoint=your-endpoint-hostname; SET tablestore.s3.access-key=xxx; SET tablestore.s3.secret-key=yyy; And read table from hive metastore, table can be created by Flink or Spark, see Catalog with Hive Metastore\nSELECT * FROM test_table; SELECT COUNT(1) FROM test_table; Trino Place flink-table-store-s3-0.4-SNAPSHOT.jar together with flink-table-store-trino-0.4-SNAPSHOT.jar under plugin/tablestore directory.\nAdd options in etc/catalog/tablestore.properties.\ns3.endpoint=your-endpoint-hostname s3.access-key=xxx s3.secret-key=yyy  S3 Complaint Object Stores #  The S3 Filesystem also support using S3 compliant object stores such as IBM’s Cloud Object Storage and MinIO. Just configure your endpoint to the provider of the object store service.\ns3.endpoint:your-endpoint-hostname"});index.add({'id':17,'href':'/docs/master/docs/engines/spark3/','title':"Spark3",'section':"Engines",'content':"Spark3 #  This documentation is a guide for using Table Store in Spark3.\nPreparing Table Store Jar File #  Table Store currently supports Spark 3.3, 3.2 and 3.1. We recommend the latest Spark version for a better experience.\nYou are using an unreleased version of Table Store so you need to manually build bundled jar from the source code. To build from source code, either download the source of a release or clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests For Spark 3.3, you can find the bundled jar in ./flink-table-store-spark/flink-table-store-spark-3.3/target/flink-table-store-spark-3.3-0.4-SNAPSHOT.jar.\nQuick Start #  If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.  Step 1: Specify Table Store Jar File\nAppend path to table store jar file to the --jars argument when starting spark-sql.\nspark-sql ... --jars /path/to/flink-table-store-spark-3.3-0.4-SNAPSHOT.jar Alternatively, you can copy flink-table-store-spark-3.3-0.4-SNAPSHOT.jar under spark/jars in your Spark installation directory.\nStep 2: Specify Table Store Catalog\nWhen starting spark-sql, use the following command to register Table Store’s Spark catalog with the name tablestore. Table files of the warehouse is stored under /tmp/table_store.\nspark-sql ... \\  --conf spark.sql.catalog.tablestore=org.apache.flink.table.store.spark.SparkCatalog \\  --conf spark.sql.catalog.tablestore.warehouse=file:/tmp/table_store After spark-sql command line has started, run the following SQL to create and switch to database tablestore.default.\nCREATE DATABASE tablestore.default; USE tablestore.default; Step 3: Create a table and Write Some Records\ncreate table my_table ( k int, v string ) tblproperties ( \u0026#39;primary-key\u0026#39; = \u0026#39;k\u0026#39; ); INSERT INTO my_table VALUES (1, \u0026#39;Hi\u0026#39;), (2, \u0026#39;Hello\u0026#39;); Step 4: Query Table with SQL\nSELECT * FROM my_table; /* 1\tHi 2\tHello */ Step 5: Update the Records\nINSERT INTO my_table VALUES (1, \u0026#39;Hi Again\u0026#39;), (3, \u0026#39;Test\u0026#39;); SELECT * FROM my_table; /* 1\tHi Again 2\tHello 3\tTest */ Step 6: Query Table with Scala API\nIf you don\u0026rsquo;t want to use Table Store catalog, you can also run spark-shell and query the table with Scala API.\nspark-shell ... --jars /path/to/flink-table-store-spark-3.3-0.4-SNAPSHOT.jar val dataset = spark.read.format(\u0026#34;tablestore\u0026#34;).load(\u0026#34;file:/tmp/table_store/default.db/my_table\u0026#34;) dataset.createOrReplaceTempView(\u0026#34;my_table\u0026#34;) spark.sql(\u0026#34;SELECT * FROM my_table\u0026#34;).show() Spark Type Conversion #  This section lists all supported type conversion between Spark and Flink. All Spark\u0026rsquo;s data types are available in package org.apache.spark.sql.types.\n  Spark Data Type Flink Data Type Atomic Type     StructType RowType false   MapType MapType false   ArrayType ArrayType false   BooleanType BooleanType true   ByteType TinyIntType true   ShortType SmallIntType true   IntegerType IntType true   LongType BigIntType true   FloatType FloatType true   DoubleType DoubleType true   StringType VarCharType, CharType true   DateType DateType true   TimestampType TimestampType, LocalZonedTimestamp true   DecimalType(precision, scale) DecimalType(precision, scale) true   BinaryType VarBinaryType, BinaryType true     Currently, Spark\u0026rsquo;s field comment cannot be described under Flink CLI. Conversion between Spark\u0026rsquo;s UserDefinedType and Flink\u0026rsquo;s UserDefinedType is not supported.   "});index.add({'id':18,'href':'/docs/master/docs/how-to/','title':"How to",'section':"Docs",'content':""});index.add({'id':19,'href':'/docs/master/docs/maintenance/manage-partition/','title':"Manage Partition",'section':"Maintenance",'content':"Expiring Partitions #  You can set partition.expiration-time when creating a partitioned table. Table Store will periodically check the status of partitions and delete expired partitions according to time.\nHow to determine whether a partition has expired: compare the time extracted from the partition with the current time to see if survival time has exceeded the partition.expiration-time.\nAn example:\nCREATE TABLE T (...) PARTITIONED BY (dt) WITH ( \u0026#39;partition.expiration-time\u0026#39; = \u0026#39;7 d\u0026#39;, \u0026#39;partition.expiration-check-interval\u0026#39; = \u0026#39;1 d\u0026#39;, \u0026#39;partition.timestamp-formatter\u0026#39; = \u0026#39;yyyyMMdd\u0026#39; ); More options:\n  Option Default Type Description     partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.    "});index.add({'id':20,'href':'/docs/master/docs/engines/spark2/','title':"Spark2",'section':"Engines",'content':"Spark2 #  This documentation is a guide for using Table Store in Spark2.\nVersion #  Table Store supports Spark 2.4+. It is highly recommended to use Spark 2.4+ version with many improvements.\nPreparing Table Store Jar File #  You are using an unreleased version of Table Store so you need to manually build bundled jar from the source code. To build from source code, either download the source of a release or clone the git repository.\nBuild bundled jar with the following command.\nmvn clean install -DskipTests You can find the bundled jar in ./flink-table-store-spark/flink-table-store-spark-2/target/flink-table-store-spark-2-0.4-SNAPSHOT.jar.\nQuick Start #  If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.  Step 1: Prepare Test Data\nTable Store currently only supports reading tables through Spark2. To create a Table Store table with records, please follow our Flink quick start guide.\nAfter the guide, all table files should be stored under the path /tmp/table_store, or the warehouse path you\u0026rsquo;ve specified.\nStep 2: Specify Table Store Jar File\nYou can append path to table store jar file to the --jars argument when starting spark-shell.\nspark-shell ... --jars /path/to/flink-table-store-spark-2-0.4-SNAPSHOT.jar Alternatively, you can copy flink-table-store-spark-2-0.4-SNAPSHOT.jar under spark/jars in your Spark installation directory.\nStep 3: Query Table\nTable store with Spark 2.4 does not support DDL. You can use the Dataset reader and register the Dataset as a temporary table. In spark shell:\nval dataset = spark.read.format(\u0026#34;tablestore\u0026#34;).load(\u0026#34;file:/tmp/table_store/default.db/word_count\u0026#34;) dataset.createOrReplaceTempView(\u0026#34;word_count\u0026#34;) spark.sql(\u0026#34;SELECT * FROM word_count\u0026#34;).show() "});index.add({'id':21,'href':'/docs/master/docs/how-to/writing-tables/','title':"Writing Tables",'section':"How to",'content':"Writing Tables #  You can use the INSERT statement to inserts new rows into a table or overwrites the existing data in the table. The inserted rows can be specified by value expressions or result from a query.\nSyntax #  INSERT { INTO | OVERWRITE } table_identifier [ part_spec ] [ column_list ] { value_expr | query }   part_spec\nAn optional parameter that specifies a comma-separated list of key and value pairs for partitions. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.\nSyntax: PARTITION ( partition_col_name = partition_col_val [ , \u0026hellip; ] )\n  column_list\nAn optional parameter that specifies a comma-separated list of columns belonging to the table_identifier table.\nSyntax: (col_name1 [, column_name2, \u0026hellip;])\nAll specified columns should exist in the table and not be duplicated from each other. It includes all columns except the static partition columns. The size of the column list should be exactly the size of the data from VALUES clause or query.     value_expr\nSpecifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted. A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows.\nSyntax: VALUES ( { value | NULL } [ , … ] ) [ , ( … ) ]\nCurrently, Flink doesn't support use NULL directly, so the NULL should be cast to actual data type by `CAST (NULL AS data_type)`.     For more information, please check the syntax document:\nFlink INSERT Statement\nSpark INSERT Statement\nStreaming reading will ignore the commits generated by INSERT OVERWRITE by default. If you want to read the commits of OVERWRITE, you can configure streaming-read-overwrite.  Applying Records/Changes to Tables #  Flink Use INSERT INTO to apply records and changes to tables.\nINSERT INTO MyTable SELECT ... Table Store supports shuffle data by bucket in sink phase. To improve data skew, Table Store also supports shuffling data by partition fields. You can add option sink.partition-shuffle to the table.\nSpark3 Use INSERT INTO to apply records and changes to tables.\nINSERT INTO MyTable SELECT ...  Overwriting the Whole Table #  For unpartitioned tables, Table Store supports overwriting the whole table.\nFlink Use INSERT OVERWRITE to overwrite the whole unpartitioned table.\nINSERT OVERWRITE MyTable SELECT ...  Overwriting a Partition #  For partitioned tables, Table Store supports overwriting a partition.\nFlink Use INSERT OVERWRITE to overwrite a partition.\nINSERT OVERWRITE MyTable PARTITION (key1 = value1, key2 = value2, ...) SELECT ...  Purging tables #  You can use INSERT OVERWRITE to purge tables by inserting empty value.\nFlink INSERT OVERWRITE MyTable SELECT * FROM MyTable WHERE false  Purging Partitions #  Currently, Table Store supports two ways to purge partitions.\n  Like purging tables, you can use INSERT OVERWRITE to purge data of partitions by inserting empty value to them.\n  Method #1 does not support to drop multiple partitions. In case that you need to drop multiple partitions, you can submit the drop-partition job through flink run.\n  Flink -- Syntax INSERT OVERWRITE MyTable PARTITION (key1 = value1, key2 = value2, ...) SELECT selectSpec FROM MyTable WHERE false -- The following SQL is an example: -- table definition CREATE TABLE MyTable ( k0 INT, k1 INT, v STRING ) PARTITIONED BY (k0, k1); -- you can use INSERT OVERWRITE MyTable PARTITION (k0 = 0) SELECT k1, v FROM MyTable WHERE false -- or INSERT OVERWRITE MyTable PARTITION (k0 = 0, k1 = 0) SELECT v FROM MyTable WHERE false Flink Job Run the following command to submit a drop-partition job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  drop-partition \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; --partition \u0026lt;partition_spec\u0026gt; [--partition \u0026lt;partition_spec\u0026gt; ...] partition_spec: key1=value1,key2=value2... For more information of drop-partition, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  drop-partition --help  Deleting from table #  Currently, Table Store supports deleting records via submitting the \u0026lsquo;delete\u0026rsquo; job through flink run.\nFlink Job Run the following command to submit a \u0026lsquo;delete\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  delete \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;table-name\u0026gt; --where \u0026lt;filter_spec\u0026gt; filter_spec is equal to the \u0026#39;WHERE\u0026#39; clause in SQL DELETE statement. Examples: age \u0026gt;= 18 AND age \u0026lt;= 60 animal \u0026lt;\u0026gt; \u0026#39;cat\u0026#39; id \u0026gt; (SELECT count(*) FROM employee) For more information of \u0026lsquo;delete\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  delete --help  Merging into table #  Table Store supports \u0026ldquo;MERGE INTO\u0026rdquo; via submitting the \u0026lsquo;merge-into\u0026rsquo; job through flink run.\nImportant table properties setting:\n Only primary key table supports this feature. The action won\u0026rsquo;t produce UPDATE_BEFORE, so it\u0026rsquo;s not recommended to set \u0026lsquo;changelog-producer\u0026rsquo; = \u0026lsquo;input\u0026rsquo;.   The design referenced such syntax:\nMERGE INTO target-table USING source-table | source-expr AS source-alias ON merge-condition WHEN MATCHED [AND matched-condition] THEN UPDATE SET xxx WHEN MATCHED [AND matched-condition] THEN DELETE WHEN NOT MATCHED [AND not-matched-condition] THEN INSERT VALUES (xxx) WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN UPDATE SET xxx WHEN NOT MATCHED BY SOURCE [AND not-matched-by-source-condition] THEN DELETE The merge-into action use \u0026ldquo;upsert\u0026rdquo; semantics instead of \u0026ldquo;update\u0026rdquo;, which means if the row exists, then do update, else do insert. For example, for non-primary-key table, you can update every column, but for primary key table, if you want to update primary keys, you have to insert a new row which has different primary keys from rows in the table. In this scenario, \u0026ldquo;upsert\u0026rdquo; is useful.\nFlink Job Run the following command to submit a \u0026lsquo;merge-into\u0026rsquo; job for the table.\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table \u0026lt;target-table\u0026gt; \\  [--target-as \u0026lt;target-table-alias\u0026gt;] \\  --source-table \u0026lt;source-table\u0026gt; \\  [--source-as \u0026lt;source-table-alias\u0026gt;] \\  --on \u0026lt;merge-condition\u0026gt; \\  --merge-actions \u0026lt;matched-upsert,matched-delete,not-matched-insert,not-matched-by-source-upsert,not-matched-by-source-delete\u0026gt; \\  --matched-upsert-condition \u0026lt;matched-condition\u0026gt; \\  --matched-upsert-set \u0026lt;upsert-changes\u0026gt; \\  --matched-delete-condition \u0026lt;matched-condition\u0026gt; \\  --not-matched-insert-condition \u0026lt;not-matched-condition\u0026gt; \\  --not-matched-insert-values \u0026lt;insert-values\u0026gt; \\  --not-matched-by-source-upsert-condition \u0026lt;not-matched-by-source-condition\u0026gt; \\  --not-matched-by-source-upsert-set \u0026lt;not-matched-upsert-changes\u0026gt; \\  --not-matched-by-source-delete-condition \u0026lt;not-matched-by-source-condition\u0026gt; Alternatively, you can use \u0026#39;--source-sql \u0026lt;sql\u0026gt; [, --source-sql \u0026lt;sql\u0026gt; ...]\u0026#39; to create a new table as source table at runtime. -- Examples: -- Find all orders mentioned in the source table, then mark as important if the price is above 100 -- or delete if the price is under 10. ./flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions \\  matched-upsert,matched-delete \\  --matched-upsert-condition \u0026#34;T.price \u0026gt; 100\u0026#34; \\  --matched-upsert-set \u0026#34;mark = \u0026#39;important\u0026#39;\u0026#34; \\  --matched-delete-condition \u0026#34;T.price \u0026lt; 10\u0026#34; -- For matched order rows, increase the price, and if there is no match, insert the order from the -- source table: ./flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions \\  matched-upsert,not-matched-insert \\  --matched-upsert-set \u0026#34;price = T.price + 20\u0026#34; \\  --not-matched-insert-values * -- For not matched by source order rows (which are in the target table and does not match any row in the -- source table based on the merge-condition), decrease the price or if the mark is \u0026#39;trivial\u0026#39;, delete them: ./flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-table S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions \\  not-matched-by-source-upsert,not-matched-by-source-delete \\  --not-matched-by-source-upsert-condition \u0026#34;T.mark \u0026lt;\u0026gt; \u0026#39;trivial\u0026#39;\u0026#34; \\  --not-matched-by-source-upsert-set \u0026#34;price = T.price - 20\u0026#34; \\  --not-matched-by-source-delete-condition \u0026#34;T.mark = \u0026#39;trivial\u0026#39;\u0026#34; -- An source-sql example: -- Create a temporary view S in new catalog and use it as source table ./flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  merge-into \\  --warehouse \u0026lt;warehouse-path\u0026gt; \\  --database \u0026lt;database-name\u0026gt; \\  --table T \\  --source-sql \u0026#34;CREATE CATALOG test WITH (...)\u0026#34; \\  --source-sql \u0026#34;USE CATALOG test\u0026#34; \\  --source-sql \u0026#34;USE DATABASE default\u0026#34; \\  --source-sql \u0026#34;CREATE TEMPORARY VIEW S AS SELECT order_id, price, \u0026#39;important\u0026#39; FROM important_order\u0026#34; \\  --source-as test.default.S \\  --on \u0026#34;T.id = S.order_id\u0026#34; \\  --merge-actions not-matched-insert\\  --not-matched-insert-values * The term \u0026lsquo;matched\u0026rsquo; explanation:\n matched: changed rows are from target table and each can match a source table row based on merge-condition and optional matched-condition (source ∩ target). not-matched: changed rows are from source table and all rows cannot match any target table row based on merge-condition and optional not-matched-condition (source - target). not-matched-by-source: changed rows are from target table and all row cannot match any source table row based on merge-condition and optional not-matched-by-source-condition (target - source).  Parameters format:\nAll conditions, set changes and values should use Flink SQL syntax. Please quote them with \u0026quot; to escape special characters.\n matched-upsert-changes:\ncol = \u0026lt;source-table\u0026gt;.col | expression [, \u0026hellip;] (Means setting \u0026lt;target-table\u0026gt;.col with given value. Do not add \u0026lsquo;\u0026lt;target-table\u0026gt;.\u0026rsquo; before \u0026lsquo;col\u0026rsquo;.)\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to set columns with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not-matched-upsert-changes is similar to matched-upsert-changes, but you cannot reference source table\u0026rsquo;s column or use \u0026lsquo;*\u0026rsquo;. insert-values:\ncol1, col2, \u0026hellip;, col_end\nMust specify values of all columns. For each column, you can reference \u0026lt;source-table\u0026gt;.col or use an expression.\nEspecially, you can use \u0026lsquo;*\u0026rsquo; to insert with all source columns (require target table\u0026rsquo;s schema is equal to source\u0026rsquo;s). not-matched-condition cannot use target table\u0026rsquo;s columns to construct condition expression. not-matched-by-source-condition cannot use source table\u0026rsquo;s columns to construct condition expression.   source-alias cannot be duplicated with existed table name. If you use \u0026ndash;source-ddl, source-alias must be specified and equal to the table name in \u0026ldquo;CREATE\u0026rdquo; statement. If the source table is not in the same place as target table, the source-table-name or the source-alias should be qualified (database.table or catalog.database.table if in different catalog). At least one merge action must be specified. If both matched-upsert and matched-delete actions are present, their conditions must both be present too (same to not-matched-by-source-upsert and not-matched-by-source-delete). Otherwise, all conditions are optional.   For more information of \u0026lsquo;merge-into\u0026rsquo;, see\n\u0026lt;FLINK_HOME\u0026gt;/bin/flink run \\  -c org.apache.flink.table.store.connector.action.FlinkActions \\  /path/to/flink-table-store-flink-**-0.4-SNAPSHOT.jar \\  merge-into --help  "});index.add({'id':22,'href':'/docs/master/docs/engines/hive/','title':"Hive",'section':"Engines",'content':"Hive #  This documentation is a guide for using Table Store in Hive.\nVersion #  Table Store currently supports Hive 2.1, 2.1-cdh-6.3, 2.2, 2.3 and 3.1.\nExecution Engine #  Table Store currently supports MR and Tez execution engine for Hive.\nInstallation #  You are using an unreleased version of Table Store so you need to manually build bundled jar from the source code. To build from source code, either download the source of a release or clone the git repository.\nBuild bundled jar with the following command. mvn clean install -Dmaven.test.skip=true\nYou can find Hive connector jar in ./flink-table-store-hive/flink-table-store-hive-connector-\u0026lt;hive-version\u0026gt;/target/flink-table-store-hive-connector-\u0026lt;hive-version\u0026gt;-0.4-SNAPSHOT.jar.\nThere are several ways to add this jar to Hive.\n You can create an auxlib folder under the root directory of Hive, and copy flink-table-store-hive-connector-0.4-SNAPSHOT.jar into auxlib. You can also copy this jar to a path accessible by Hive, then use add jar /path/to/flink-table-store-hive-connector-0.4-SNAPSHOT.jar to enable table store support in Hive. Note that this method is not recommended. If you\u0026rsquo;re using the MR execution engine and running a join statement, you may be faced with the exception org.apache.hive.com.esotericsoftware.kryo.kryoexception: unable to find class.  NOTE: If you are using HDFS, make sure that the environment variable HADOOP_HOME or HADOOP_CONF_DIR is set.\nQuick Start with Table Store Hive Catalog #  By using table store Hive catalog, you can create, drop and insert into table store tables from Flink. These operations directly affect the corresponding Hive metastore. Tables created in this way can also be accessed directly from Hive.\nStep 1: Prepare Table Store Hive Catalog Jar File for Flink\nSee creating a catalog with Hive metastore.\nStep 2: Create Test Data with Flink SQL\nExecute the following Flink SQL script in Flink SQL client to define a Table Store Hive catalog and create a table.\n-- Flink SQL CLI -- Define table store Hive catalog  CREATE CATALOG my_hive WITH ( \u0026#39;type\u0026#39; = \u0026#39;table-store\u0026#39;, \u0026#39;metastore\u0026#39; = \u0026#39;hive\u0026#39;, \u0026#39;uri\u0026#39; = \u0026#39;thrift://\u0026lt;hive-metastore-host-name\u0026gt;:\u0026lt;port\u0026gt;\u0026#39;, \u0026#39;warehouse\u0026#39; = \u0026#39;/path/to/table/store/warehouse\u0026#39; ); -- Use table store Hive catalog  USE CATALOG my_hive; -- Create a table in table store Hive catalog (use \u0026#34;default\u0026#34; database by default)  CREATE TABLE test_table ( a int, b string ); -- Insert records into test table  INSERT INTO test_table VALUES (1, \u0026#39;Table\u0026#39;), (2, \u0026#39;Store\u0026#39;); -- Read records from test table  SELECT * FROM test_table; /* +---+-------+ | a | b | +---+-------+ | 1 | Table | | 2 | Store | +---+-------+ */ Step 3: Query the Table in Hive\nRun the following Hive SQL in Hive CLI to access the created table.\n-- Assume that flink-table-store-hive-connector-\u0026lt;hive-version\u0026gt;-0.4-SNAPSHOT.jar is already in auxlib directory. -- List tables in Hive -- (you might need to switch to \u0026#34;default\u0026#34; database if you\u0026#39;re not there by default)  SHOW TABLES; /* OK test_table */ -- Read records from test_table  SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore */ Quick Start with External Table #  To access existing table store table, you can also register them as external tables in Hive. Run the following Hive SQL in Hive CLI.\n-- Assume that flink-table-store-hive-connector-0.4-SNAPSHOT.jar is already in auxlib directory. -- Let\u0026#39;s use the test_table created in the above section. -- To create an external table, you don\u0026#39;t need to specify any column or table properties. -- Pointing the location to the path of table is enough.  CREATE EXTERNAL TABLE external_test_table STORED BY \u0026#39;org.apache.flink.table.store.hive.TableStoreHiveStorageHandler\u0026#39; LOCATION \u0026#39;/path/to/table/store/warehouse/default.db/test_table\u0026#39;; -- Read records from external_test_table  SELECT a, b FROM test_table ORDER BY a; /* OK 1\tTable 2\tStore */ Hive Type Conversion #  This section lists all supported type conversion between Hive and Flink. All Hive\u0026rsquo;s data types are available in package org.apache.hadoop.hive.serde2.typeinfo.\n  Hive Data Type Flink Data Type Atomic Type     StructTypeInfo RowType false   MapTypeInfo MapType false   ListTypeInfo ArrayType false   PrimitiveTypeInfo(\"boolean\") BooleanType true   PrimitiveTypeInfo(\"tinyint\") TinyIntType true   PrimitiveTypeInfo(\"smallint\") SmallIntType true   PrimitiveTypeInfo(\"int\") IntType true   PrimitiveTypeInfo(\"bigint\") BigIntType true   PrimitiveTypeInfo(\"float\") FloatType true   PrimitiveTypeInfo(\"double\") DoubleType true   BaseCharTypeInfo(\"char(%d)\") CharType(length) true   PrimitiveTypeInfo(\"string\") VarCharType(VarCharType.MAX_LENGTH) true   BaseCharTypeInfo(\"varchar(%d)\") VarCharType(length), length is less than VarCharType.MAX_LENGTH true   PrimitiveTypeInfo(\"date\") DateType true   TimestampType TimestampType true   DecimalTypeInfo(\"decimal(%d, %d)\") DecimalType(precision, scale) true   DecimalTypeInfo(\"binary\") VarBinaryType, BinaryType true    "});index.add({'id':23,'href':'/docs/master/docs/how-to/querying-tables/','title':"Querying Tables",'section':"How to",'content':"Querying Tables #  Just like all other tables, Table Store tables can be queried with SELECT statement.\nScan Mode #  By specifying the scan.mode table property, users can specify where and how Table Store sources should produce records.\n  Scan Mode Batch Source Behavior Streaming Source Behavior     default The default scan mode. Determines actual scan mode according to other table properties. If \"scan.timestamp-millis\" is set the actual scan mode will be \"from-timestamp\", and if \"scan.snapshot-id\" is set the actual startup mode will be \"from-snapshot\". Otherwise the actual scan mode will be \"latest-full\".    latest-full  Produces the latest snapshot of table.   Produces the latest snapshot on the table upon first startup, and continues to read the following changes.    compacted-full  Produces the snapshot after the latest compaction.   Produces the snapshot after the latest compaction on the table upon first startup, and continues to read the following changes.    latest Same as \"latest-full\" Continuously reads latest changes without producing a snapshot at the beginning.   from-timestamp Produces a snapshot earlier than or equals to the timestamp specified by \"scan.timestamp-millis\". Continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning.   from-snapshot Produces a snapshot specified by \"scan.snapshot-id\". Continuously reads changes starting from a snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning.    Users can also adjust changelog-producer table property to specify the pattern of produced changes. See changelog producer for details.\nStreaming Source can also be bounded, you can specify \u0026lsquo;scan.bounded.watermark\u0026rsquo; to define the end condition for bounded streaming mode, stream reading will end until a larger watermark snapshot is encountered.  System Tables #  System tables contain metadata and information about each table, such as the snapshots created and the options in use. Users can access system tables with batch queries.\nCurrently, Flink, Spark and Trino supports querying system tables.\nIn some cases, the table name needs to be enclosed with back quotes to avoid syntax parsing conflicts, for example triple access mode:\nSELECT * FROM my_catalog.my_db.`MyTable$snapshots`; Snapshots Table #  You can query the snapshot history information of the table through snapshots table, including the record count occurred in the snapshot.\nSELECT * FROM MyTable$snapshots; /* +--------------+------------+-----------------+-------------------+--------------+-------------------------+---------------------+---------------------+-------------------------+ | snapshot_id | schema_id | commit_user | commit_identifier | commit_kind | commit_time | total_record_count | delta_record_count | changelog_record_count | +--------------+------------+-----------------+-------------------+--------------+-------------------------+---------------------+---------------------+-------------------------+ | 2 | 0 | 7ca4cd28-98e... | 2 | APPEND | 2022-10-26 11:44:15.600 | 2 | 2 | 0 | | 1 | 0 | 870062aa-3e9... | 1 | APPEND | 2022-10-26 11:44:15.148 | 1 | 1 | 0 | +--------------+------------+-----------------+-------------------+--------------+-------------------------+---------------------+---------------------+-------------------------+ 2 rows in set */ By querying the snapshots table, you can know the commit and expiration information about that table and time travel through the data.\nSchemas Table #  You can query the historical schemas of the table through schemas table.\nSELECT * FROM MyTable$schemas; /* +-----------+--------------------------------+----------------+--------------+---------+---------+ | schema_id | fields | partition_keys | primary_keys | options | comment | +-----------+--------------------------------+----------------+--------------+---------+---------+ | 0 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | | 1 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | | 2 | [{\u0026#34;id\u0026#34;:0,\u0026#34;name\u0026#34;:\u0026#34;word\u0026#34;,\u0026#34;typ... | [] | [\u0026#34;word\u0026#34;] | {} | | +-----------+--------------------------------+----------------+--------------+---------+---------+ 3 rows in set */ You can join the snapshots table and schemas table to get the fields of given snapshots.\nSELECT s.snapshot_id, t.schema_id, t.fields FROM MyTable$snapshots s JOIN MyTable$schemas t ON s.schema_id=t.schema_id where s.snapshot_id=100; Options Table #  You can query the table\u0026rsquo;s option information which is specified from the DDL through options table. The options not shown will be the default value. You can take reference to [Configuration].\nSELECT * FROM MyTable$options; /* +------------------------+--------------------+ | key | value | +------------------------+--------------------+ | snapshot.time-retained | 5 h | +------------------------+--------------------+ 1 rows in set */ Audit log Table #  If you need to audit the changelog of the table, you can use the audit_log system table. Through audit_log table, you can get the rowkind column when you get the incremental data of the table. You can use this column for filtering and other operations to complete the audit.\nThere are four values for rowkind:\n +I: Insertion operation. -U: Update operation with the previous content of the updated row. +U: Update operation with new content of the updated row. -D: Deletion operation.  SELECT * FROM MyTable$audit_log; /* +------------------+-----------------+-----------------+ | rowkind | column_0 | column_1 | +------------------+-----------------+-----------------+ | +I | ... | ... | +------------------+-----------------+-----------------+ | -U | ... | ... | +------------------+-----------------+-----------------+ | +U | ... | ... | +------------------+-----------------+-----------------+ 3 rows in set */ Files Table #  You can query the files of the table with specific snapshot.\n-- Query the files of latest snapshot SELECT * FROM MyTable$files; +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ | [3] | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} |2023-02-24T16:06:21.166| | [2] | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} |2023-02-24T16:06:21.166| | [2] | 0 | data-83aa7973-060b-40b6-8c8... | orc | 0 | 0 | 1 | 605 | [d] | [d] | {cnt=0, val=0, word=0} | {cnt=2, val=32, word=d} | {cnt=2, val=32, word=d} |2023-02-24T16:06:21.166| | [5] | 0 | data-3d304f4a-bcea-44dc-a13... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=5, val=51, word=c} | {cnt=5, val=51, word=c} |2023-02-24T16:06:21.166| | [1] | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} |2023-02-24T16:06:21.166| | [4] | 0 | data-2c9b7095-65b7-4013-a7a... | orc | 0 | 0 | 1 | 593 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=4, val=12, word=a} | {cnt=4, val=12, word=a} |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ 6 rows in set -- You can also query the files with specific snapshot SELECT * FROM MyTable$files /*+ OPTIONS('scan.snapshot-id'='1') */; +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ | partition | bucket | file_path | file_format | schema_id | level | record_count | file_size_in_bytes | min_key | max_key | null_value_counts | min_value_stats | max_value_stats | creation_time | +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ | [3] | 0 | data-8f64af95-29cc-4342-adc... | orc | 0 | 0 | 1 | 593 | [c] | [c] | {cnt=0, val=0, word=0} | {cnt=3, val=33, word=c} | {cnt=3, val=33, word=c} |2023-02-24T16:06:21.166| | [2] | 0 | data-8b369068-0d37-4011-aa5... | orc | 0 | 0 | 1 | 593 | [b] | [b] | {cnt=0, val=0, word=0} | {cnt=2, val=22, word=b} | {cnt=2, val=22, word=b} |2023-02-24T16:06:21.166| | [1] | 0 | data-10abb5bc-0170-43ae-b6a... | orc | 0 | 0 | 1 | 595 | [a] | [a] | {cnt=0, val=0, word=0} | {cnt=1, val=11, word=a} | {cnt=1, val=11, word=a} |2023-02-24T16:06:21.166| +-----------+--------+--------------------------------+-------------+-----------+-------+--------------+--------------------+---------+---------+------------------------+-------------------------+-------------------------+-----------------------+ 3 rows in set "});index.add({'id':24,'href':'/docs/master/docs/how-to/lookup-joins/','title':"Lookup Joins",'section':"How to",'content':"Lookup Joins #  Lookup Joins are a type of join in streaming queries. It is used to enrich a table with data that is queried from Table Store. The join requires one table to have a processing time attribute and the other table to be backed by a lookup source connector.\nTable Store supports lookup joins on unpartitioned tables with primary keys in Flink. The following example illustrates this feature.\nFirst, let\u0026rsquo;s create a Table Store table and update it in real-time.\n-- Create a table store catalog CREATE CATALOG my_catalog WITH ( \u0026#39;type\u0026#39;=\u0026#39;table-store\u0026#39;, \u0026#39;warehouse\u0026#39;=\u0026#39;hdfs://nn:8020/warehouse/path\u0026#39; -- or \u0026#39;file://tmp/foo/bar\u0026#39; ); USE CATALOG my_catalog; -- Create a table in table-store catalog CREATE TABLE customers ( id INT PRIMARY KEY NOT ENFORCED, name STRING, country STRING, zip STRING ); -- Launch a streaming job to update customers table INSERT INTO customers ... -- Create a temporary left table, like from kafka CREATE TEMPORARY TABLE Orders ( order_id INT, total INT, customer_id INT, proc_time AS PROCTIME() ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;topic\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;properties.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39; ... ); You can now use customers in a lookup join query.\n-- enrich each order with customer information SELECT o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN customers FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id; The lookup join operator will maintain a RocksDB cache locally and pull the latest updates of the table in real time. Lookup join operator will only pull the necessary data, so your filter conditions are very important for performance.\nThis feature is only suitable for tables containing at most tens of millions of records to avoid excessive use of local disks.\nIf the records of Orders (main table) join missing because the corresponding data of customers (lookup table) is not ready. You can consider using Flink\u0026rsquo;s Delayed Retry Strategy For Lookup.  RocksDB Cache Options #  The following options allow users to finely adjust RocksDB for better performance. You can either specify them in table properties or in dynamic table hints.\n-- dynamic table hints example SELECT o.order_id, o.total, c.country, c.zip FROM Orders AS o JOIN customers /*+ OPTIONS(\u0026#39;lookup.cache-rows\u0026#39;=\u0026#39;20000\u0026#39;) */ FOR SYSTEM_TIME AS OF o.proc_time AS c ON o.customer_id = c.id;   Key Default Type Description     lookup.cache-rows 10000 Long The maximum number of rows to store in the cache.   rocksdb.block.blocksize 4 kb MemorySize The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'.   rocksdb.block.cache-size 8 mb MemorySize The amount of the cache for data blocks in RocksDB. The default block-cache size is '8MB'.   rocksdb.block.metadata-blocksize 4 kb MemorySize Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'.   rocksdb.bloom-filter.bits-per-key 10.0 Double Bits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0.   rocksdb.bloom-filter.block-based-mode false Boolean If true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'.   rocksdb.compaction.level.max-size-level-base 256 mb MemorySize The upper-bound of the total size of level base files in bytes. The default value is '256MB'.   rocksdb.compaction.level.target-file-size-base 64 mb MemorySize The target file size for compaction, which determines a level-1 file size. The default value is '64MB'.   rocksdb.compaction.level.use-dynamic-size false Boolean If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc.   rocksdb.compaction.style LEVEL Enum\n The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.\nPossible values:\"LEVEL\"\"UNIVERSAL\"\"FIFO\"\"NONE\"   rocksdb.compression.type LZ4_COMPRESSION Enum\n The compression type.\nPossible values:\"NO_COMPRESSION\"\"SNAPPY_COMPRESSION\"\"ZLIB_COMPRESSION\"\"BZLIB2_COMPRESSION\"\"LZ4_COMPRESSION\"\"LZ4HC_COMPRESSION\"\"XPRESS_COMPRESSION\"\"ZSTD_COMPRESSION\"\"DISABLE_COMPRESSION_OPTION\"   rocksdb.files.open -1 Integer The maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'.   rocksdb.thread.num 2 Integer The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'.   rocksdb.use-bloom-filter false Boolean If true, every newly created SST file will contain a Bloom filter. It is disabled by default.   rocksdb.writebuffer.count 2 Integer The maximum number of write buffers that are built up in memory. The default value is '2'.   rocksdb.writebuffer.number-to-merge 1 Integer The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'.   rocksdb.writebuffer.size 64 mb MemorySize The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'.    "});index.add({'id':25,'href':'/docs/master/docs/maintenance/','title':"Maintenance",'section':"Docs",'content':""});index.add({'id':26,'href':'/docs/master/docs/concepts/primary-key-table/','title':"Primary Key Table",'section':"Concepts",'content':"Primary Key Table #  Changelog table is the default table type when creating a table. Users can insert, update or delete records in the table.\nPrimary keys are a set of columns that are unique for each record. Table Store imposes an ordering of data, which means the system will sort the primary key within each bucket. Using this feature, users can achieve high performance by adding filter conditions on the primary key.\nBy defining primary keys on a changelog table, users can access the following features.\nMerge Engines #  When Table Store sink receives two or more records with the same primary keys, it will merge them into one record to keep primary keys unique. By specifying the merge-engine table property, users can choose how records are merged together.\nSet table.exec.sink.upsert-materialize to NONE always in Flink SQL TableConfig, sink upsert-materialize may result in strange behavior. When the input is out of order, we recommend that you use Sequence Field to correct disorder.  Deduplicate #  deduplicate merge engine is the default merge engine. Table Store will only keep the latest record and throw away other records with the same primary keys.\nSpecifically, if the latest record is a DELETE record, all records with the same primary keys will be deleted.\nPartial Update #  By specifying 'merge-engine' = 'partial-update', users can set columns of a record across multiple updates and finally get a complete record. Specifically, value fields are updated to the latest data one by one under the same primary key, but null values are not overwritten.\nFor example, let\u0026rsquo;s say Table Store receives three records:\n \u0026lt;1, 23.0, 10, NULL\u0026gt;- \u0026lt;1, NULL, NULL, 'This is a book'\u0026gt; \u0026lt;1, 25.2, NULL, NULL\u0026gt;  If the first column is the primary key. The final result will be \u0026lt;1, 25.2, 10, 'This is a book'\u0026gt;.\nFor streaming queries, partial-update merge engine must be used together with lookup or full-compaction changelog producer.  Partial cannot receive DELETE messages because the behavior cannot be defined. You can configure partial-update.ignore-delete to ignore DELETE messages.  Aggregation #  Sometimes users only care about aggregated results. The aggregation merge engine aggregates each value field with the latest data one by one under the same primary key according to the aggregate function.\nEach field not part of the primary keys can be given an aggregate function, specified by the fields.\u0026lt;field-name\u0026gt;.aggregate-function table property, otherwise it will use last_non_null_value aggregation as default. For example, consider the following table definition.\nFlink CREATE TABLE MyTable ( product_id BIGINT, price DOUBLE, sales BIGINT, PRIMARY KEY (product_id) NOT ENFORCED ) WITH ( \u0026#39;merge-engine\u0026#39; = \u0026#39;aggregation\u0026#39;, \u0026#39;fields.price.aggregate-function\u0026#39; = \u0026#39;max\u0026#39;, \u0026#39;fields.sales.aggregate-function\u0026#39; = \u0026#39;sum\u0026#39; );  Field price will be aggregated by the max function, and field sales will be aggregated by the sum function. Given two input records \u0026lt;1, 23.0, 15\u0026gt; and \u0026lt;1, 30.2, 20\u0026gt;, the final result will be \u0026lt;1, 30.2, 35\u0026gt;.\nCurrent supported aggregate functions and data types are:\n sum: supports DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT and DOUBLE. min/max: support DECIMAL, TINYINT, SMALLINT, INTEGER, BIGINT, FLOAT, DOUBLE, DATE, TIME, TIMESTAMP and TIMESTAMP_LTZ. last_value / last_non_null_value: support all data types. listagg: supports STRING data type. bool_and / bool_or: support BOOLEAN data type.  Only sum supports retraction (UPDATE_BEFORE and DELETE), others aggregate functions do not support retraction. If you allow some functions to ignore retraction messages, you can configure: 'fields.${field_name}.ignore-retract'='true'.\nFor streaming queries, aggregation merge engine must be used together with lookup or full-compaction changelog producer.  Changelog Producers #  Streaming queries will continuously produce latest changes. These changes can come from the underlying table files or from an external log system like Kafka. Compared to the external log system, changes from table files have lower cost but higher latency (depending on how often snapshots are created).\nBy specifying the changelog-producer table property when creating the table, users can choose the pattern of changes produced from files.\nThe changelog-producer table property only affects changelog from files. It does not affect the external log system.  None #  By default, no extra changelog producer will be applied to the writer of table. Table Store source can only see the merged changes across snapshots, like what keys are removed and what are the new values of some keys.\nHowever, these merged changes cannot form a complete changelog, because we can\u0026rsquo;t read the old values of the keys directly from them. Merged changes require the consumers to \u0026ldquo;remember\u0026rdquo; the values of each key and to rewrite the values without seeing the old ones. Some consumers, however, need the old values to ensure correctness or efficiency.\nConsider a consumer which calculates the sum on some grouping keys (might not be equal to the primary keys). If the consumer only sees a new value 5, it cannot determine what values should be added to the summing result. For example, if the old value is 4, it should add 1 to the result. But if the old value is 6, it should in turn subtract 1 from the result. Old values are important for these types of consumers.\nTo conclude, none changelog producers are best suited for consumers such as a database system. Flink also has a built-in \u0026ldquo;normalize\u0026rdquo; operator which persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided.\nInput #  By specifying 'changelog-producer' = 'input', Table Store writers rely on their inputs as a source of complete changelog. All input records will be saved in separated changelog files and will be given to the consumers by Table Store sources.\ninput changelog producer can be used when Table Store writers' inputs are complete changelog, such as from a database CDC, or generated by Flink stateful computation.\nLookup #  This is an experimental feature.  If your input can’t produce a complete changelog but you still want to get rid of the costly normalized operator, you may consider using the 'lookup' changelog producer.\nBy specifying 'changelog-producer' = 'lookup', Table Store will generate changelog through 'lookup' before committing the data writing.\nLookup will cache data on the memory and local disk, you can use the following options to tune performance:\n  Option Default Type Description     lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size unlimited MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.    Full Compaction #  If you think the resource consumption of \u0026lsquo;lookup\u0026rsquo; is too large, you can consider using \u0026lsquo;full-compaction\u0026rsquo; changelog producer, which can decouple data writing and changelog generation, and is more suitable for scenarios with high latency (For example, 10 minutes).\nBy specifying 'changelog-producer' = 'full-compaction', Table Store will compare the results between full compactions and produce the differences as changelog. The latency of changelog is affected by the frequency of full compactions.\nBy specifying changelog-producer.compaction-interval table property (default value 0s), users can define the maximum interval between two full compactions to ensure latency. This is set to 0 by default, so each checkpoint will have a full compression and generate a change log.\nFull compaction changelog producer can produce complete changelog for any type of source. However it is not as efficient as the input changelog producer and the latency to produce changelog might be high.  Sequence Field #  By default, the primary key table determines the merge order according to the input order (the last input record will be the last to merge). However, in distributed computing, there will be some cases that lead to data disorder. At this time, you can use a time field as sequence.field, for example:\nWhen the record is updated or deleted, the sequence.field must become larger and cannot remain unchanged. For example, you can use Mysql Binlog operation time as sequence.field.  Flink CREATE TABLE MyTable ( pk BIGINT PRIMARY KEY NOT ENFORCED, v1 DOUBLE, v2 BIGINT, dt TIMESTAMP ) WITH ( \u0026#39;sequence.field\u0026#39; = \u0026#39;dt\u0026#39; );  The record with the largest sequence.field value will be the last to merge, regardless of the input order.\n"});index.add({'id':27,'href':'/docs/master/docs/engines/trino/','title':"Trino",'section':"Engines",'content':"Trino #  Because Trino\u0026rsquo;s dependency is JDK 11, it is not possible to include the trino connector in flink-table-store.\nSee flink-table-store-trino.\n"});index.add({'id':28,'href':'/docs/master/docs/concepts/append-only-table/','title':"Append Only Table",'section':"Concepts",'content':"Append Only Table #  By specifying 'write-mode' = 'append-only' when creating the table, user creates an append-only table.\nYou can only insert a complete record into the table. No delete or update is supported and you cannot define primary keys. This type of table is suitable for use cases that do not require updates (such as log data synchronization).\nBucketing #  You can also define bucket number for Append-only table, see Bucket.\nIt is recommended that you set the bucket-key field. Otherwise, the data will be hashed according to the whole row, and the performance will be poor.\nStreaming Read Order #  For streaming reads, records are produced in the following order:\n For any two records from two different partitions  If scan.plan-sort-partition is set to true, the record with a smaller partition value will be produced first. Otherwise, the record with an earlier partition creation time will be produced first.   For any two records from the same partition and the same bucket, the first written record will be produced first. For any two records from the same partition but two different buckets, different buckets are processed by different tasks, there is no order guarantee between them.  Compaction #  By default, the sink node will automatically perform compaction to control the number of files. The following options control the strategy of compaction:\n  Key Default Type Description     write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number which satisfies sum(size(f_i)) \u0026gt;= targetFileSize to trigger a compaction for append-only table. This value avoids almost-full-file to be compacted, which is not cost-effective.   compaction.early-max.file-num 50 Integer For file set [f_0,...,f_N], the maximum file number to trigger a compaction for append-only table, even if sum(size(f_i)) \u0026lt; targetFileSize. This value avoids pending too much small files, which slows down the performance.    Example #  The following is an example of creating the Append-Only table and specifying the bucket key.\nFlink CREATE TABLE MyTable ( product_id BIGINT, price DOUBLE, sales BIGINT ) WITH ( \u0026#39;write-mode\u0026#39; = \u0026#39;append-only\u0026#39;, \u0026#39;bucket\u0026#39; = \u0026#39;8\u0026#39;, \u0026#39;bucket-key\u0026#39; = \u0026#39;product_id\u0026#39; );  "});index.add({'id':29,'href':'/docs/master/docs/concepts/external-log-systems/','title':"External Log Systems",'section':"Concepts",'content':"External Log Systems #  Aside from underlying table files, changelog of Table Store can also be stored into or consumed from an external log system, such as Kafka. By specifying log.system table property, users can choose which external log system to use.\nIf an external log system is used, all records written into table files will also be written into the log system. Changes produced by the streaming queries will thus come from the log system instead of table files.\nConsistency Guarantees #  By default, changes in the log systems are visible to consumers only after a snapshot, just like table files. This behavior guarantees the exactly-once semantics. That is, each record is seen by the consumers exactly once.\nHowever, users can also specify the table property 'log.consistency' = 'eventual' so that changelog written into the log system can be immediately consumed by the consumers, without waiting for the next snapshot. This behavior decreases the latency of changelog, but it can only guarantee the at-least-once semantics (that is, consumers might see duplicated records) due to possible failures.\nIf 'log.consistency' = 'eventual' is set, in order to achieve correct results, Table Store source in Flink will automatically adds a \u0026ldquo;normalize\u0026rdquo; operator for deduplication. This operator persists the values of each key in states. As one can easily tell, this operator will be very costly and should be avoided.\nSupported Log Systems #  Kafka #  By specifying 'log.system' = 'kafka', users can write changes into Kafka along with table files.\nFlink CREATE TABLE T (...) WITH ( \u0026#39;log.system\u0026#39; = \u0026#39;kafka\u0026#39;, \u0026#39;kafka.bootstrap.servers\u0026#39; = \u0026#39;...\u0026#39;, \u0026#39;kafka.topic\u0026#39; = \u0026#39;...\u0026#39; );  Table Properties for Kafka are listed as follows.\n  Key Default Type Description     kafka.bootstrap.servers (none) String Required Kafka server connection string.   kafka.topic (none) String Topic of this kafka table.    "});index.add({'id':30,'href':'/docs/master/docs/maintenance/configurations/','title':"Configurations",'section':"Maintenance",'content':"Configuration #  CoreOptions #  Core options for table store.\n  Key Default Type Description     auto-create false Boolean Whether to create underlying storage when reading and writing the table.   bucket 1 Integer Bucket number for file store.   bucket-key (none) String Specify the table store distribution policy. Data is assigned to each bucket according to the hash value of bucket-key.\nIf you specify multiple fields, delimiter is ','.\nIf not specified, the primary key will be used; if there is no primary key, the full row will be used.   changelog-producer none Enum\n Whether to double write to a changelog file. This changelog file keeps the details of data changes, it can be read directly during stream reads.\nPossible values:\"none\": No changelog file.\"input\": Double write to a changelog file when flushing memory table, the changelog is from input.\"full-compaction\": Generate changelog files with each full compaction.\"lookup\": Generate changelog files through 'lookup' before committing the data writing.   commit.force-compact false Boolean Whether to force a compaction before commit.   compaction.early-max.file-num 50 Integer For file set [f_0,...,f_N], the maximum file number to trigger a compaction for append-only table, even if sum(size(f_i)) \u0026lt; targetFileSize. This value avoids pending too much small files, which slows down the performance.   compaction.max-size-amplification-percent 200 Integer The size amplification is defined as the amount (in percentage) of additional storage needed to store a single byte of data in the merge tree for changelog mode table.   compaction.max-sorted-run-num 2147483647 Integer The maximum sorted run number to pick for compaction. This value avoids merging too much sorted runs at the same time during compaction, which may lead to OutOfMemoryError.   compaction.min.file-num 5 Integer For file set [f_0,...,f_N], the minimum file number which satisfies sum(size(f_i)) \u0026gt;= targetFileSize to trigger a compaction for append-only table. This value avoids almost-full-file to be compacted, which is not cost-effective.   compaction.size-ratio 1 Integer Percentage flexibility while comparing sorted run size for changelog mode table. If the candidate sorted run(s) size is 1% smaller than the next sorted run's size, then include next sorted run into this candidate set.   continuous.discovery-interval 1 s Duration The discovery interval of continuous reading.   file.compression.per.level  Map Define different compression policies for different level, you can add the conf like this: 'file.compression.per.level' = '0:lz4,1:zlib', for orc file format, the compression value could be NONE, ZLIB, SNAPPY, LZO, LZ4, for parquet file format, the compression value could be UNCOMPRESSED, SNAPPY, GZIP, LZO, BROTLI, LZ4, ZSTD.   file.format \"orc\" String Specify the message format of data files.   local-sort.max-num-file-handles 128 Integer The maximal fan-in for external merge sort. It limits the number of file handles. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading.   log.changelog-mode auto Enum\n Specify the log changelog mode for table.\nPossible values:\"auto\": Upsert for table with primary key, all for table without primary key.\"all\": The log system stores all changes including UPDATE_BEFORE.\"upsert\": The log system does not store the UPDATE_BEFORE changes, the log consumed job will automatically add the normalized node, relying on the state to generate the required update_before.   log.consistency transactional Enum\n Specify the log consistency mode for table.\nPossible values:\"transactional\": Only the data after the checkpoint can be seen by readers, the latency depends on checkpoint interval.\"eventual\": Immediate data visibility, you may see some intermediate states, but eventually the right results will be produced, only works for table with primary key.   log.format \"debezium-json\" String Specify the message format of log system.   log.key.format \"json\" String Specify the key message format of log system with primary key.   log.scan.remove-normalize false Boolean Whether to force the removal of the normalize node when streaming read. Note: This is dangerous and is likely to cause data errors if downstream is used to calculate aggregation and the input is not complete changelog.   lookup.cache-file-retention 1 h Duration The cached files retention time for lookup. After the file expires, if there is a need for access, it will be re-read from the DFS to build an index on the local disk.   lookup.cache-max-disk-size 9223372036854775807 bytes MemorySize Max disk size for lookup cache, you can use this option to limit the use of local disks.   lookup.cache-max-memory-size 256 mb MemorySize Max memory size for lookup cache.   lookup.hash-load-factor 0.75 Float The index load factor for lookup.   manifest.format \"avro\" String Specify the message format of manifest files.   manifest.merge-min-count 30 Integer To avoid frequent manifest merges, this parameter specifies the minimum number of ManifestFileMeta to merge.   manifest.target-file-size 8 mb MemorySize Suggested file size of a manifest file.   merge-engine deduplicate Enum\n Specify the merge engine for table with primary key.\nPossible values:\"deduplicate\": De-duplicate and keep the last row.\"partial-update\": Partial update non-null fields.\"aggregation\": Aggregate fields with same primary key.   num-levels (none) Integer Total level number, for example, there are 3 levels, including 0,1,2 levels.   num-sorted-run.compaction-trigger 5 Integer The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).   num-sorted-run.stop-trigger (none) Integer The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 1.   orc.bloom.filter.columns (none) String A comma-separated list of columns for which to create a bloon filter when writing.   orc.bloom.filter.fpp 0.05 Double Define the default false positive probability for bloom filters.   page-size 64 kb MemorySize Memory page size.   partial-update.ignore-delete false Boolean Whether to ignore delete records in partial-update mode.   partition (none) String Define partition by table options, cannot define partition on DDL and table options at the same time.   partition.default-name \"__DEFAULT_PARTITION__\" String The default partition name in case the dynamic partition column value is null/empty string.   partition.expiration-check-interval 1 h Duration The check interval of partition expiration.   partition.expiration-time (none) Duration The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.   partition.timestamp-formatter (none) String The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.Supports multiple partition fields like '$year-$month-$day $hour:00:00'.The timestamp-formatter is compatible with Java's DateTimeFormatter.   partition.timestamp-pattern (none) String You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.By default, read from the first field.If the timestamp in the partition is a single field called 'dt', you can use '$dt'.If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.   primary-key (none) String Define primary key by table options, cannot define primary key on DDL and table options at the same time.   scan.bounded.watermark (none) Long End condition \"watermark\" for bounded streaming mode. Stream reading will end when a larger watermark snapshot is encountered.   scan.mode default Enum\n Specify the scanning behavior of the source.\nPossible values:\"default\": Determines actual startup mode according to other table properties. If \"scan.timestamp-millis\" is set the actual startup mode will be \"from-timestamp\", and if \"scan.snapshot-id\" is set the actual startup mode will be \"from-snapshot\". Otherwise the actual startup mode will be \"latest-full\".\"latest-full\": For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes. For batch sources, just produce the latest snapshot but does not read new changes.\"full\": Deprecated. Same as \"latest-full\".\"latest\": For streaming sources, continuously reads latest changes without producing a snapshot at the beginning. For batch sources, behaves the same as the \"latest-full\" startup mode.\"compacted-full\": For streaming sources, produces a snapshot after the latest compaction on the table upon first startup, and continue to read the latest changes. For batch sources, just produce a snapshot after the latest compaction but does not read new changes.\"from-timestamp\": For streaming sources, continuously reads changes starting from timestamp specified by \"scan.timestamp-millis\", without producing a snapshot at the beginning. For batch sources, produces a snapshot at timestamp specified by \"scan.timestamp-millis\" but does not read new changes.\"from-snapshot\": For streaming sources, continuously reads changes starting from snapshot specified by \"scan.snapshot-id\", without producing a snapshot at the beginning. For batch sources, produces a snapshot specified by \"scan.snapshot-id\" but does not read new changes.   scan.plan-sort-partition false Boolean Whether to sort plan files by partition fields, this allows you to read according to the partition order, even if your partition writes are out of order.\nIt is recommended that you use this for streaming read of the 'append-only' table. By default, streaming read will read the full snapshot first. In order to avoid the disorder reading for partitions, you can open this option.   scan.snapshot-id (none) Long Optional snapshot id used in case of \"from-snapshot\" scan mode   scan.timestamp-millis (none) Long Optional timestamp used in case of \"from-timestamp\" scan mode.   sequence.field (none) String The field that generates the sequence number for primary key table, the sequence number determines which data is the most recent.   snapshot.num-retained.max 2147483647 Integer The maximum number of completed snapshots to retain.   snapshot.num-retained.min 10 Integer The minimum number of completed snapshots to retain.   snapshot.time-retained 1 h Duration The maximum time of completed snapshots to retain.   source.split.open-file-cost 4 mb MemorySize Open file cost of a source file. It is used to avoid reading too many files with a source split, which can be very slow.   source.split.target-size 128 mb MemorySize Target size of a source split when scanning a bucket.   streaming-read-overwrite false Boolean Whether to read the changes from overwrite in streaming mode.   target-file-size 128 mb MemorySize Target size of a file.   write-buffer-size 256 mb MemorySize Amount of data to build up in memory before converting to a sorted on-disk file.   write-buffer-spillable (none) Boolean Whether the write buffer can be spillable. Enabled by default when using object storage.   write-mode change-log Enum\n Specify the write mode for table.\nPossible values:\"append-only\": The table can only accept append-only insert operations. Neither data deduplication nor any primary key constraints will be done when inserting rows into table store.\"change-log\": The table can accept insert/delete/update operations.   write-only false Boolean If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.    CatalogOptions #  Options for table store catalog.\n  Key Default Type Description     fs.allow-hadoop-fallback true Boolean Allow to fallback to hadoop File IO when no file io found for the scheme.   lock-acquire-timeout 8 min Duration The maximum time to wait for acquiring the lock.   lock-check-max-sleep 8 s Duration The maximum sleep time when retrying to check the lock.   lock.enabled false Boolean Enable Catalog Lock.   metastore \"filesystem\" String Metastore of table store catalog, supports filesystem and hive.   table.type managed Enum\n Type of table.\nPossible values:\"managed\": Table Store owned table where the entire lifecycle of the table data is managed.\"external\": The table where Table Store has loose coupling with the data stored in external locations.   uri (none) String Uri of metastore server.   warehouse (none) String The warehouse root path of catalog.    FlinkConnectorOptions #  Flink connector options for table store.\n  Key Default Type Description     changelog-producer.compaction-interval 0 ms Duration When changelog-producer is set to FULL_COMPACTION, full compaction will be constantly triggered after this interval.   changelog-producer.lookup-wait true Boolean When changelog-producer is set to LOOKUP, commit will wait for changelog generation by lookup.   log.system \"none\" String The log system used to keep changes of the table.\nPossible values:\n\"none\": No log system, the data is written only to file store, and the streaming read will be directly read from the file store.\"kafka\": Kafka log system, the data is double written to file store and kafka, and the streaming read will be read from kafka.   scan.parallelism (none) Integer Define a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.   sink.parallelism (none) Integer Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.   sink.partition-shuffle false Boolean The option to enable shuffle data by dynamic partition fields in sink phase for table store.   streaming-read-atomic false Boolean The option to enable return per iterator instead of per record in streaming read.This can ensure that there will be no checkpoint segmentation in iterator consumption.\nBy default, streaming source checkpoint will be performed in any time, this means 'UPDATE_BEFORE' and 'UPDATE_AFTER' can be split into two checkpoint. Downstream can see intermediate state.    "});index.add({'id':31,'href':'/docs/master/versions/','title':"Versions",'section':"Apache Flink Table Store",'content':"Versions #  An appendix of hosted documentation for all versions of Apache Flink Table Store.\n master    latest    0.3    "});})();