'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/downloads/','title':"Downloads",'section':"Apache Paimon",'content':"Downloads #  Paimon is released as a source artifact, and also through Maven.\nSource Releases #     RELEASE DATE DOWNLOAD     0.7.0-incubating 2024-02-06 tar (digest, pgp)   0.6.1-incubating (recommended) 2024-02-06 tar (digest, pgp)    To download a source distribution for a particular release, click on the tar link.\nFor security, hash and signature files are always hosted at Apache.\nAll Paimon releases are available via https://archive.apache.org/dist/paimon/ including checksums and signatures.\nVerify the Integrity of the Files #  You must verify the integrity of the downloaded file using the PGP signature (.asc file) or a hash (.sha256; .md5 for older releases). For more information why this must be done, please read Verifying Apache Software Foundation Releases.\nVerify the hash digest #  We use SHA-512 to verify the hash digest of the file.\nTo check a hash, you can first compute the SHA-512 checksum for the file you just downloaded, and then download the digest file for comparison, they should be equal.\nCompute the checksum of your file:\n Windows: certUtil -hashfile file SHA512 Linux: sha512sum file Mac: shasum -a 512 file  Verify the PGP signature #  To verify the signature using GPG or PGP, please do the following:\n Download the release artifact and the corresponding PGP signature from the table above. Download the Apache Paimon KEYS file. Import the KEYS file and verify the downloaded artifact using one of the following methods:  gpg --import KEYS gpg --verify downloaded_file.asc downloaded_file or\npgpk -a KEYS pgpv downloaded_file.asc or\npgp -ka KEYS pgp downloaded_file.asc Maven Artifacts #  Add the following to the dependencies section of your pom.xml file:\nFlink Please replace ${flink.version} in the following xml file to the version of Flink you\u0026rsquo;re using. For example, 1.17 or 1.18.\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-flink-${flink.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.0-incubating\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Also include \u0026lt;dependency\u0026gt; elements for any extension modules you need: paimon-flink-cdc, paimon-flink-action, and so forth.\nSpark3 Please replace ${spark.version} in the following xml file to the version of Flink you\u0026rsquo;re using. For example, 3.4 or 3.5.\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.paimon\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;paimon-spark-${spark.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.0-incubating\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;  "});index.add({'id':1,'href':'/releases/','title':"Releases",'section':"Apache Paimon",'content':""});index.add({'id':2,'href':'/releases/release-0.7/','title':"Release 0.7",'section':"Releases",'content':"Apache Paimon 0.7 Available #  February 29, 2024 - Paimon Community (dev@paimon.apache.org)\nApache Paimon PPMC has officially released Apache Paimon 0.7.0-incubating version. A total of 34 people contributed to this version and completed over 300 Commits. Thank you to all contributors for their support!\nIn this version, we mainly focus on enhancement and optimization for the existing features. For more details, please refer to the following content below.\nFlink #  Lookup Join #   Fix bug that lookup join cannot handle sequence field of dim table. Introduced primary key partial lookup based on Paimon hash lookup for lookup join. Use parallel reading and bulk loading to speed up initial data loading of dim table.  Paimon CDC #  In 0.7, we continued to improve the Paimon CDC:\n Support Postgres table synchronization. Paimon data commit relies on checkpoint but many users forget to enable it when submitting CDC job. In this case, the job will set checkpoint interval to 180 seconds. Support UNAWARE Bucket mode table as CDC sink. Support extract time attribute from CDC input as watermark. Now, you can set tag.automatic-creation to watermark in CDC jobs.  Spark #   Merge-into supports WHEN NOT MATCHED BY SOURCE semantics. Sort compact supports Hilbert Curve sorter. Multiple optimization for improving query performance.  Hive #  In 0.7, we mainly focus on improving compatibility with Hive.\n Support timestamp with local time zone type. Support create database with location, comment and properties of HiveSQL. Support table comment.  Tag Management #   Support a new tag.automatic-creation mode batch. In this mode, a tag will be created after a batch job completed. The tag auto creation relies on commit, so if no commit when it is time to auto-create tag, the tag won\u0026rsquo;t be created. In this case, we introduce an option snapshot.watermark-idle-timeout. If the flink source idles over the specified timeout duration, the job will force to create a snapshot and thus trigger tag creation.  New Aggregation Functions #   count: counts the values across multiple rows. product: computes product values across multiple rows. nested-update: collects multiple rows into one ARRAY (so-called \u0026lsquo;nested table\u0026rsquo;). You can use fields.\u0026lt;field-name\u0026gt;.nested-key=pk0,pk1,... to define the primary keys of the nested table. If no keys defined, the rows will be appended to the array. collect: collects elements into an ARRAY. You can set fields.\u0026lt;field-name\u0026gt;.distinct=true to deduplicate elements. merge_map: merges input maps into single map.  New Metrics #   Support Flink standard connector metric currentEmitEventTimeLag. Support level0FileCount to show the compaction progress.  Other Improvements #  Besides above, there are some useful improvements for existed features:\n New time travel option scan.file-creation-time-millis: By specifying this option, only the data files created after this time will be read. It is more convenient than scan.timestamp-millis and scan.tag-name, but is imprecise (depending on whether compaction occurs). For primary key table, now the row kind can be determined by field which is specified by option rowkind.field. Support ignoring delete records in deduplicate mode by option deduplicate.ignore-delete. Support ignoring consumer id when starting streaming reading job by option consumer.ignore-progress. Support new procedure expire_snapshots to manually trigger snapshot expiration. Support new system table aggregation_fields to show the aggregation fields information for aggregate or partial-update table. Introduce bloom filter to speed up the local file lookup, which can benefit both lookup changelog-producer and flink lookup join.  "});index.add({'id':3,'href':'/releases/release-0.6/','title':"Release 0.6",'section':"Releases",'content':"Apache Paimon 0.6 Available #  December 13, 2023 - Paimon Community (dev@paimon.apache.org)\nApache Paimon PPMC has officially released Apache Paimon 0.6.0-incubating version. A total of 58 people contributed to this version and completed over 400 Commits. Thank you to all contributors for their support!\nSome outstanding developments are:\n Flink Paimon CDC almost supports all mainstream data ingestion currently available. Flink 1.18 and Paimon supports CALL procedure, this will make table management easier. Cross partition update is available for production! Read-optimized table is introduced to enhance query performance. Append scalable mode is available for production! Paimon Presto module is available for production! Metrics system is integrated to Flink Metrics. Spark Paimon has made tremendous progress.  For details, please refer to the following text.\nFlink #  Paimon CDC #  Paimon CDC integrates Flink CDC, Kafka, Pulsar, etc., and provides comprehensive support in version 0.6:\n Kafka CDC supports formats: Canal Json, Debezium Json, Maxwell and OGG. Pulsar CDC is added, both Table Sync and Database Sync. Mongo CDC is available for production!  Flink Batch Source #  By default, the parallelism of batch reads is the same as the number of splits, while the parallelism of stream reads is the same as the number of buckets, but not greater than scan.infer-parallelism.max (Default is 1024).\nFlink Streaming Source #  Consumer-id is available for production!\nYou can specify the consumer-id when streaming read table record consuming snapshot id in Paimon, the newly started job can continue to consume from the previous progress without resuming from the state. You can also set consumer.mode to at-least-once to get better checkpoint time.\nFlink Time Travel #  Flink 1.18 SQL supports Time Travel Query (You can also use dynamic option):\nSELECT * FROM t FOR SYSTEM_TIME AS OF TIMESTAMP \u0026#39;2023-01-01 00:00:00\u0026#39;; Flink Call Procedures #  Flink 1.18 SQL supports Call Procedures:\n   Procedure Name Example     compact CALL sys.compact(\u0026lsquo;default.T\u0026rsquo;, \u0026lsquo;p=0\u0026rsquo;, \u0026lsquo;zorder\u0026rsquo;, \u0026lsquo;a,b\u0026rsquo;, \u0026lsquo;sink.parallelism=4\u0026rsquo;)   compact_database CALL sys.compact_database(\u0026lsquo;db1   create_tag CALL sys.create_tag(\u0026lsquo;default.T\u0026rsquo;, \u0026lsquo;my_tag\u0026rsquo;, 10)   delete_tag CALL sys.delete_tag(\u0026lsquo;default.T\u0026rsquo;, \u0026lsquo;my_tag\u0026rsquo;)   merge_into CALL sys.merge_into(\u0026lsquo;default.T\u0026rsquo;, \u0026lsquo;\u0026rsquo;, \u0026lsquo;\u0026rsquo;, \u0026lsquo;default.S\u0026rsquo;, \u0026lsquo;T.id=S.order_id\u0026rsquo;, \u0026lsquo;\u0026rsquo;, \u0026lsquo;price=T.price+20\u0026rsquo;, \u0026lsquo;\u0026rsquo;, \u0026lsquo;*')   remove_orphan_files CALL remove_orphan_files(\u0026lsquo;default.T\u0026rsquo;, \u0026lsquo;2023-10-31 12:00:00\u0026rsquo;)   reset_consumer CALL sys.reset_consumer(\u0026lsquo;default.T\u0026rsquo;, \u0026lsquo;myid\u0026rsquo;, 10)   rollback_to CALL sys.rollback_to(\u0026lsquo;default.T\u0026rsquo;, 10)    Flink 1.19 will support Named Arguments which will make it easier to use when there are multiple arguments.\nCommitter Improvement #  The Committee is responsible for submitting metadata, and sometimes it may have bottlenecks that can lead to backpressure operations. In 0.6, we have the following optimizations:\n By default, paimon will delete expired snapshots synchronously. Users can use asynchronous expiration mode by setting snapshot.expire.execution-mode to async to improve performance. You can use fine-grained-resource-management of Flink to increase committer heap memory and cpu only.  Primary Key Table #  Cross Partition Update #  Cross partition update is available for production!\nCurrently Flink batch \u0026amp; streaming writes are supported and has been applied by enterprises to production environments! How to use Cross partition update:\n Primary keys not contain all partition fields. Use dynamic bucket mode, which means bucket is -1.  This mode directly maintains the mapping of keys to partition and bucket, uses local disks, and initializes indexes by reading all existing keys in the table when starting write job. Although maintaining the index is necessary, this mode also maintains high throughput performance. Please try it out.\nRead Optimized #  For Primary Key Table, it\u0026rsquo;s a \u0026lsquo;MergeOnRead\u0026rsquo; technology. When reading data, multiple layers of LSM data are merged, and the number of parallelism will be limited by the number of buckets. If you want to query fast enough in certain scenarios, but can only find older data, you can query from read-optimized table: SELECT * FROM T$ro.\nBut the freshness of the data cannot be guaranteed, you can configure \u0026lsquo;full-compaction.delta-commits\u0026rsquo; when writing data to ensure that data with a determined latency is read.\nStarRocks and other OLAP systems will release a version to greatly enhance query performance for read-optimized tables based on Paimon 0.6.\nPartial Update #  In 0.6, you can define aggregation functions for the partial-update merge engine with sequence group. This allows you to perform special aggregations on certain fields under certain conditions, such as count, sum, etc.\nCompaction #  We have introduced some asynchronous techniques to further improve the performance of Compaction! 20%+\nAnd 0.6 introduces the database compaction, you can run the following command to submit a compaction job for multiple database. If you submit a streaming job, the job will continuously monitor new changes to the table and perform compactions as needed.\nAppend Table #  Append scalable mode is available for production!\nBy defining \u0026lsquo;bucket\u0026rsquo; = \u0026lsquo;-1\u0026rsquo; to non-primary table, you can assign an append scalable mode for the table. This type of table is an upgrade to Hive format. You can use it:\n Spark, Flink Batch Read \u0026amp; Write, including INSERT OVERWRITE support. Flink, Spark Streaming Read \u0026amp; Write, Flink will do small files compaction. You can sort (z-order) this table, which will greatly accelerate query performance, especially when there are filtering conditions related to sorting keys.  You can set write-buffer-for-append option for append-only table, to apply situations where a large number of partitions are streaming written simultaneously.\n0.6 also introduce Hive Table Migration, Apache Hive supports ORC, Parquet file formats that could be migrated to Paimon. When migrating data to a paimon table, the origin table will be permanently disappeared. So please back up your data if you still need the original table. The migrated table will be append table. You can use Flink Spark CALL procedure to migrate Hive table.\nStarRocks and other OLAP systems will release a version to greatly enhance query performance for append tables based on Paimon 0.6.\nTag Management #  Upsert To Partitioned #  The Tag will maintain the manifests and data files of the snapshot. Offline data warehouses require an immutable view every day to ensure the idempotence of calculations. So we created a Tag mechanism to output these views.\nHowever, the traditional use of Hive data warehouses is more accustomed to using partitions to specify the query\u0026rsquo;s Tag, and is more accustomed to using Hive computing engines.\nSo, we introduce metastore.tag-to-partition and metastore.tag-to-partition.preview to mapping a non-partitioned primary key table to the partition table in Hive metastore, and mapping the partition field to the name of the Tag to be fully compatible with Hive.\nTag with Flink Savepoint #  You cannot recover a write job from an old Flink savepoint, which may cause issues with the Paimon table. In 0.6, we avoided this situation where an exception is thrown when data anomalies occur, causing the job to fail to start.\nIf you want to recover from the old savepoint, we recommend setting sink.savepoint.auto-tag to true to enable the feature of automatically creating tags for Flink savepoint.\nFormats #  0.6 upgrates ORC version to 1.8.3, and Parquet version to 1.13.1. ORC natively supports ZSTD in this version, which is a compression algorithm with a higher compression rate. We recommend using it when high compression rates are needed.\nMetrics System #  In 0.6, Paimon has built a metrics system to measure the behaviours of reading and writing, Paimon has supported built-in metrics to measure operations of commits, scans, writes and compactions, which can be bridged to computing engine like Flink. The most important for streaming read is currentFetchEventTimeLag.\nPaimon Spark #   Support Spark 3.5 Structured Streaming: Supports serving as a Streaming Source, supports source side traffic control through custom read triggers, and supports stream read changelog Row Level Operation: DELETE optimization, supporting UPDATE and MERGE INTO Call Procedure: Add compact and migrate_table, migrate_file, remove_orphan_files, create_tag, delete_tag, rollback Query optimization: Push down filter optimization, support for Push down limit, and runtime filter (DPP) Other: Truncate Table optimization, support for CTAS, support for Truncate Partition  Paimon Trino #  The Paimon Trino module mainly performs the following tasks to accelerate queries:\n Optimize the issue of converting pages to avoid memory overflow caused by large pages Implemented Limit Pushdown and can combine partition pruning  Paimon Presto #  The Paimon Presto module is available for production! The following capabilities have been added:\n Implement Filter Pushdown, which allows Paimon Presto to be available for production Use the Inject mode, which allows Paimon Catalog to reside in the process and improve query speed  What\u0026rsquo;s next? #  Report your requirements!\n"});index.add({'id':4,'href':'/releases/release-0.5/','title':"Release 0.5",'section':"Releases",'content':"Apache Paimon 0.5 Available #  September 06, 2023 - Jingsong Lee (jingsonglee0@gmail.com)\nWe are happy to announce the availability of Paimon 0.5.0-incubating.\nNearly 100 contributors have come to contribute release-0.5, we created 500+ commits together, bringing many exciting new features and improvements to the community. Thank you all for your joint efforts!\nHighlight:\n CDC Data Ingestion into Lake has reached maturity. Introduce Tags to provide immutable view to Offline data warehouse. Dynamic Bucket mode for Primary Key Table is available in production. Introduce Append Only Scalable Table to replace Hive table.  CDC Ingestion #  Paimon supports a variety of ways to ingest data into Paimon tables with schema evolution. In release 0.5, a large number of new features have been added:\n MySQL Synchronizing Table  support synchronizing shards into one Paimon table support type-mapping to make all fields to string   MySQL Synchronizing Database  support merge multiple shards from multiple database support --mode combined to a unified sink to sync all tables, and sync newly added tables without restarting job   Kafka Synchronizing Table  synchronize one Kafka topic’s table into one Paimon table. support Canal and OGG   Kafka Synchronizing Database  synchronize one Kafka topic containing multiple tables or multiple topics containing one table each into one Paimon database. support Canal and OGG   MongoDB Synchronizing Collection  synchronize one Collection from MongoDB into one Paimon table.   MongoDB Synchronizing Database  synchronize the whole MongoDB database into one Paimon database.    Primary Key Table #  By specific Primary Key in creating table DDL, you can get a Primary Key Table, it accepts insert, update or delete records.\nDynamic Bucket #  Configure 'bucket' = '-1', Paimon dynamically maintains the index, automatic expansion of the number of buckets.\n Option1: 'dynamic-bucket.target-row-num': controls the target row number for one bucket. Option2: 'dynamic-bucket.assigner-parallelism': Parallelism of assigner operator, controls the number of initialized bucket.  Dynamic Bucket mode uses HASH index to maintain mapping from key to bucket, it requires more memory than fixed bucket mode. For performance:\n Generally speaking, there is no performance loss, but there will be some additional memory consumption, 100 million entries in a partition takes up 1 GB more memory, partitions that are no longer active do not take up memory. For tables with low update rates, this mode is recommended to significantly improve performance.  Partial-Update: Sequence Group #  A sequence-field may not solve the disorder problem of partial-update tables with multiple stream updates, because the sequence-field may be overwritten by the latest data of another stream during multi-stream update. So we introduce sequence group mechanism for partial-update tables. It can solve:\n Disorder during multi-stream update. Each stream defines its own sequence-groups. A true partial-update, not just a non-null update. Accept delete records to retract partial columns.  First Row Merge Engine #  By specifying 'merge-engine' = 'first-row', users can keep the first row of the same primary key. It differs from the deduplicate merge engine that in the first-row merge engine, it will generate insert only changelog.\nThis is of great help in replacing log deduplication in streaming computation.\nLookup Changelog-Producer #  Lookup Changelog-Producer is available in production, this can greatly reduce the delay for tables that need to generate changelogs.\n(Note: Please increase 'execution.checkpointing.max-concurrent-checkpoints' Flink configuration, this is very important for performance).\nSequence Auto Padding #  When the record is updated or deleted, the sequence.field must become larger and cannot remain unchanged. For -U and +U, their sequence-fields must be different. If you cannot meet this requirement, Paimon provides option to automatically pad the sequence field for you.\nConfigure 'sequence.auto-padding' = 'row-kind-flag': If you are using same value for -U and +U, just like \u0026ldquo;op_ts\u0026rdquo; (the time that the change was made in the database) in Mysql Binlog. It is recommended to use the automatic padding for row kind flag, which will automatically distinguish between -U (-D) and +U (+I).\nAsynchronous Compaction #  Compaction is inherently asynchronous, but if you want it to be completely asynchronous and not blocking writing, expect a mode to have maximum writing throughput, the compaction can be done slowly and not in a hurry. You can use the following strategies for your table:\nnum-sorted-run.stop-trigger = 2147483647 sort-spill-threshold = 10 This configuration will generate more files during peak write periods and gradually merge into optimal read performance during low write periods.\nAvro File Format #  If you want to achieve ultimate compaction performance, you can consider using row storage file format AVRO.\n The advantage is that you can achieve high write throughput and compaction performance. The disadvantage is that your analysis queries will be slow, and the biggest problem with row storage is that it does not have the query projection. For example, if the table have 100 columns but only query a few columns, the IO of row storage cannot be ignored. Additionally, compression efficiency will decrease and storage costs will increase.  file.format = avro metadata.stats-mode = none If you don\u0026rsquo;t want to modify all files to Avro format, at least you can consider modifying the files in the previous layers to Avro format. You can use 'file.format.per.level' = '0:avro,1:avro' to specify the files in the first two layers to be in Avro format.\nAppend Only Table #  Append Only Scalable Table #  By defining 'bucket' = '-1' to a non-pk table, you can assign an Append Only Scalable Table. In this mode, the table doesn\u0026rsquo;t have the concept of bucket anymore, read and write are concurrent. We regard this table as a batch off-line table(although we can stream read and write still).\nUsing this mode, you can replace your Hive table to lake table.\nWe have auto small file compaction for this mode by default. And you can use Sort Compact action to sort whole partition, using zorder sorter, this can greatly speed up data skipping when querying.\nManage Tags #  Paimon\u0026rsquo;s snapshots can provide an easy way to query historical data. But in most scenarios, a job will generate too many snapshots and table will expire old snapshots according to table configuration. Snapshot expiration will also delete old data files, and the historical data of expired snapshots cannot be queried anymore.\nTo solve this problem, you can create a Tag based on a snapshot. The tag will maintain the manifests and data files of the snapshot. A typical usage is creating tags daily, then you can maintain the historical data of each day for batch reading.\nPaimon supports automatic creation of tags in writing job. You can use 'tag.automatic-creation'to create tags automatically.\nAnd you can query the incremental data of Tags (or snapshots) too, both Flink and Spark support incremental queries.\nEngines #  Flink #  After Flink released 1.17, Paimon underwent very in-depth integration.\n ALTER TABLE syntax is enhanced by including the ability to ADD/MODIFY/DROP columns, making it easier for users to maintain their table schema. FlinkGenericCatalog, you need to use Hive metastore. Then, you can use all the tables from Paimon, Hive, and Flink Generic Tables (Kafka and other tables)! Dynamic Partition Overwrite Flink’s default overwrite mode is dynamic partition overwrite (that means Paimon only deletes the partitions appear in the overwritten data). You can configure dynamic-partition-overwrite to change it to static overwritten. Sync Partitions into Hive Metastore By default, Paimon does not synchronize newly created partitions into Hive metastore. If you want to see a partitioned table in Hive and also synchronize newly created partitions into Hive metastore, please set the table property metastore.partitioned-table to true. Retry Lookup Join support Retry Lookup and Async Retry Lookup.  Spark #  Spark is another computing engine that Paimon has in-depth integration and has taken a big step forward at 0.5, including the following features:\n INSERT OVERWRITE insert ovewrite partition, Spark’s default overwrite mode is static partition overwrite, you can enable dynamic overwritten too. Partition Management: Support DROP PARTITION, SHOW PARTITIONS. Supports saving a DataFrame to a paimon location. Schema merging write: You can set write.merge-schema to true to write with schema merging. Streaming sink: You can use Spark streaming foreachBatch API to streaming sink to Paimon.  Download #  Download the release here.\nWhat\u0026rsquo;s next? #  Paimon will be committed to solving the following scenarios for a long time:\n Acceleration of CDC data into the lake: real-time writing, real-time query, and offline immutable partition view by using Tags. Enrich Merge Engines to improve streaming computation: Partial-Update table, Aggregation table, First Row table. Changelog Streaming read, build incremental stream processing based on lake storage. Append mode accelerates Hive offline tables, writes in real time and brings query acceleration after sorting. Append mode replaces some message queue scenarios, stream reads in input order, and without data TTL.  "});index.add({'id':5,'href':'/releases/release-0.4/','title':"Release 0.4",'section':"Releases",'content':"Apache Paimon 0.4 Available #  June 07, 2023\nWe are happy to announce the availability of Paimon 0.4. This is the first release of the system inside the Apache Incubator and under the name Paimon. Releases up to 0.3 were under the name Flink Table Store, a sub-project of Flink where Paimon originates from.\nWhat is Paimon? #  Apache Paimon(incubating) is a streaming data lake platform that supports high-speed data ingestion, change data tracking and efficient real-time analytics.\nPaimon offers the following core capabilities:\n Unified Batch \u0026amp; Streaming: Paimon supports batch write and batch read, as well as streaming write changes and streaming read table changelogs. Data Lake: As a data lake storage, Paimon has the following advantages: low cost, high reliability, and scalable metadata. Merge Engines: Paimon supports rich Merge Engines. By default, the last entry of the primary key is reserved. You can also use the \u0026ldquo;partial-update\u0026rdquo; or \u0026ldquo;aggregation\u0026rdquo; engine. Changelog producer: Paimon supports rich Changelog producers, such as \u0026ldquo;lookup\u0026rdquo; and \u0026ldquo;full-compaction\u0026rdquo;. The correct changelog can simplify the construction of a streaming pipeline. Append Only Tables: Paimon supports Append Only tables, automatically compact small files, and provides orderly stream reading. You can use this to replace message queues.  Release 0.4 #  Paimon 0.4 includes many bug fixes and improvements that make the system more stable and robust.\nDownload the release here.\n"});})();